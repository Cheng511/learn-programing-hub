# Python é‚Šç·£AIæ‡‰ç”¨ ğŸ¤–

æœ¬ç« å°‡ä»‹ç´¹å¦‚ä½•åœ¨é‚Šç·£è¨­å‚™ä¸Šéƒ¨ç½²å’Œé‹è¡Œ AI æ¨¡å‹ï¼Œå¯¦ç¾æœ¬åœ°æ™ºèƒ½åŒ–è™•ç†ã€‚

## å­¸ç¿’ç›®æ¨™
- æŒæ¡é‚Šç·£ AI éƒ¨ç½²æ–¹æ³•
- å­¸ç¿’æ¨¡å‹å„ªåŒ–æŠ€è¡“
- å¯¦ç¾å¯¦æ™‚æ¨ç†ç³»çµ±
- é–‹ç™¼é‚Šç·£ AI æ‡‰ç”¨

## æ¨¡å‹å„ªåŒ–
```python
import tensorflow as tf
import numpy as np

class ModelOptimizer:
    def __init__(self, model_path):
        self.model = tf.keras.models.load_model(model_path)
    
    def quantize_model(self):
        """é‡åŒ–æ¨¡å‹"""
        converter = tf.lite.TFLiteConverter.from_keras_model(self.model)
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_types = [tf.float16]
        tflite_model = converter.convert()
        return tflite_model
    
    def prune_model(self, target_sparsity=0.5):
        """å‰ªææ¨¡å‹"""
        pruning_params = {
            'pruning_schedule': tf.keras.optimizers.schedules.PolynomialDecay(
                initial_sparsity=0.0,
                final_sparsity=target_sparsity,
                begin_step=0,
                end_step=1000
            )
        }
        return tf.keras.models.clone_model(self.model, pruning_params)
```

## é‚Šç·£æ¨ç†
```python
class EdgeInference:
    def __init__(self, model_path):
        self.interpreter = tf.lite.Interpreter(model_path=model_path)
        self.interpreter.allocate_tensors()
        
        self.input_details = self.interpreter.get_input_details()
        self.output_details = self.interpreter.get_output_details()
    
    def preprocess_input(self, data):
        """é è™•ç†è¼¸å…¥æ•¸æ“š"""
        return np.array(data, dtype=np.float32)
    
    def inference(self, input_data):
        """åŸ·è¡Œæ¨ç†"""
        processed_data = self.preprocess_input(input_data)
        self.interpreter.set_tensor(
            self.input_details[0]['index'], 
            processed_data
        )
        
        self.interpreter.invoke()
        
        return self.interpreter.get_tensor(
            self.output_details[0]['index']
        )
```

## å¯¦æ™‚è¦–è¦ºè™•ç†
```python
import cv2

class RealTimeVision:
    def __init__(self, model_path, camera_id=0):
        self.inference = EdgeInference(model_path)
        self.camera = cv2.VideoCapture(camera_id)
        self.running = False
    
    def start_processing(self):
        """é–‹å§‹å¯¦æ™‚è™•ç†"""
        self.running = True
        while self.running:
            ret, frame = self.camera.read()
            if not ret:
                break
            
            # é è™•ç†åœ–åƒ
            processed_frame = self.preprocess_frame(frame)
            
            # åŸ·è¡Œæ¨ç†
            results = self.inference.inference(processed_frame)
            
            # å¾Œè™•ç†ä¸¦é¡¯ç¤ºçµæœ
            self.display_results(frame, results)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        self.camera.release()
        cv2.destroyAllWindows()
    
    def preprocess_frame(self, frame):
        """é è™•ç†åœ–åƒå¹€"""
        resized = cv2.resize(frame, (224, 224))
        return resized / 255.0
    
    def display_results(self, frame, results):
        """é¡¯ç¤ºè™•ç†çµæœ"""
        # åœ¨åœ–åƒä¸Šç¹ªè£½çµæœ
        cv2.imshow('Edge AI Processing', frame)
```

## è²éŸ³è™•ç†
```python
import pyaudio
import numpy as np
from queue import Queue

class AudioProcessor:
    def __init__(self, model_path, chunk_size=1024):
        self.inference = EdgeInference(model_path)
        self.chunk_size = chunk_size
        self.audio_queue = Queue()
        
        self.p = pyaudio.PyAudio()
        self.stream = self.p.open(
            format=pyaudio.paFloat32,
            channels=1,
            rate=16000,
            input=True,
            frames_per_buffer=chunk_size,
            stream_callback=self._audio_callback
        )
    
    def _audio_callback(self, in_data, frame_count, time_info, status):
        """éŸ³é »å›èª¿"""
        audio_data = np.frombuffer(in_data, dtype=np.float32)
        self.audio_queue.put(audio_data)
        return (in_data, pyaudio.paContinue)
    
    def process_audio(self):
        """è™•ç†éŸ³é »æ•¸æ“š"""
        while not self.audio_queue.empty():
            audio_chunk = self.audio_queue.get()
            results = self.inference.inference(audio_chunk)
            self.handle_results(results)
    
    def handle_results(self, results):
        """è™•ç†æ¨ç†çµæœ"""
        # å¯¦ç¾å…·é«”çš„çµæœè™•ç†é‚è¼¯
        pass
```

## å¯¦æˆ°ç·´ç¿’
1. é–‹ç™¼é‚Šç·£è¦–è¦ºæ‡‰ç”¨
   - å¯¦ç¾ç‰©é«”æª¢æ¸¬ç³»çµ±
   - é–‹ç™¼äººè‡‰è­˜åˆ¥æ‡‰ç”¨
   - å»ºç«‹å‹•ä½œè­˜åˆ¥ç³»çµ±

2. å¯¦ç¾é‚Šç·£èªéŸ³è™•ç†
   - é–‹ç™¼èªéŸ³å‘½ä»¤è­˜åˆ¥
   - å¯¦ç¾è²éŸ³äº‹ä»¶æª¢æ¸¬
   - å»ºç«‹å¯¦æ™‚èªéŸ³è½‰å¯«

3. å„ªåŒ–é‚Šç·£ AI æ¨¡å‹
   - æ‡‰ç”¨æ¨¡å‹é‡åŒ–æŠ€è¡“
   - å¯¦ç¾æ¨¡å‹å‰ªæ
   - å„ªåŒ–æ¨ç†æ€§èƒ½

## ç¸½çµ
- é‚Šç·£ AI å¯ä»¥æ¸›å°‘ç¶²çµ¡ä¾è³´
- æ¨¡å‹å„ªåŒ–å°é‚Šç·£éƒ¨ç½²è‡³é—œé‡è¦
- å¯¦æ™‚è™•ç†éœ€è¦è€ƒæ…®æ€§èƒ½å¹³è¡¡
- é‚Šç·£ AI æ‡‰ç”¨å ´æ™¯å»£æ³›

## å»¶ä¼¸é–±è®€
- TensorFlow Lite æ–‡æª”
- é‚Šç·£ AI æœ€ä½³å¯¦è¸
- æ¨¡å‹å„ªåŒ–æŠ€è¡“
- å¯¦æ™‚ç³»çµ±è¨­è¨ˆ

---
ä¸‹ä¸€ç« ï¼š[å¯¦æ™‚æ•¸æ“šè™•ç†](./151_å¯¦æ™‚æ•¸æ“šè™•ç†.md) 