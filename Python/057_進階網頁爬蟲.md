[上一章：網頁爬蟲基礎](056_網頁爬蟲基礎.md) | [下一章：自動化運維](058_自動化運維.md)

# Python 進階網頁爬蟲 🚀

## Selenium自動化

### 1. 基本操作

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
import time

# 初始化瀏覽器
def init_browser():
    # 設置Chrome選項
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')  # 無頭模式
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    
    # 創建瀏覽器實例
    driver = webdriver.Chrome(options=options)
    return driver

# 基本頁面操作
def basic_operations():
    driver = init_browser()
    try:
        # 訪問頁面
        driver.get('https://www.example.com')
        
        # 查找元素
        element = driver.find_element(By.ID, 'search')
        element.send_keys('Python')
        element.send_keys(Keys.RETURN)
        
        # 等待元素加載
        wait = WebDriverWait(driver, 10)
        results = wait.until(
            EC.presence_of_all_elements_located((By.CLASS_NAME, 'result'))
        )
        
        # 獲取結果
        for result in results:
            print(result.text)
            
    finally:
        driver.quit()

# 處理JavaScript
def handle_javascript():
    driver = init_browser()
    try:
        driver.get('https://example.com')
        
        # 執行JavaScript
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        
        # 等待動態加載的內容
        time.sleep(2)
        
        # 獲取JavaScript生成的內容
        content = driver.execute_script(
            "return document.getElementById('dynamic-content').innerHTML;"
        )
        print(content)
        
    finally:
        driver.quit()
```

### 2. 高級功能

```python
from selenium.webdriver.common.action_chains import ActionChains

# 處理彈窗
def handle_alerts():
    driver = init_browser()
    try:
        driver.get('https://example.com')
        
        # 點擊觸發彈窗的按鈕
        button = driver.find_element(By.ID, 'alert-button')
        button.click()
        
        # 切換到彈窗並接受
        alert = driver.switch_to.alert
        print('彈窗文本:', alert.text)
        alert.accept()
        
    finally:
        driver.quit()

# 處理iframe
def handle_iframes():
    driver = init_browser()
    try:
        driver.get('https://example.com')
        
        # 切換到iframe
        iframe = driver.find_element(By.TAG_NAME, 'iframe')
        driver.switch_to.frame(iframe)
        
        # 在iframe中操作
        element = driver.find_element(By.ID, 'frame-content')
        print(element.text)
        
        # 切回主文檔
        driver.switch_to.default_content()
        
    finally:
        driver.quit()

# 高級鼠標操作
def advanced_mouse_actions():
    driver = init_browser()
    try:
        driver.get('https://example.com')
        
        # 創建ActionChains對象
        actions = ActionChains(driver)
        
        # 找到目標元素
        menu = driver.find_element(By.ID, 'menu')
        submenu = driver.find_element(By.ID, 'submenu')
        
        # 執行鼠標懸停
        actions.move_to_element(menu)
        actions.move_to_element(submenu)
        actions.click()
        actions.perform()
        
    finally:
        driver.quit()
```

## 異步爬蟲

### 1. aiohttp基礎

```python
import aiohttp
import asyncio
from typing import List, Dict
import time

async def fetch_page(session: aiohttp.ClientSession, url: str) -> str:
    """異步獲取頁面內容"""
    try:
        async with session.get(url) as response:
            return await response.text()
    except Exception as e:
        print(f'獲取頁面失敗: {url}, 錯誤: {e}')
        return None

async def process_urls(urls: List[str]) -> List[Dict]:
    """並發處理多個URL"""
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_page(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
        return [{'url': url, 'content': content}
                for url, content in zip(urls, results)
                if content is not None]

# 使用示例
async def main():
    urls = [
        'http://example.com/page1',
        'http://example.com/page2',
        'http://example.com/page3'
    ]
    
    start_time = time.time()
    results = await process_urls(urls)
    end_time = time.time()
    
    print(f'處理 {len(urls)} 個URL用時: {end_time - start_time:.2f}秒')
    return results

# 運行異步程序
if __name__ == '__main__':
    asyncio.run(main())
```

### 2. 異步爬蟲框架

```python
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from typing import List, Dict, Set
import logging

class AsyncCrawler:
    def __init__(self, start_url: str, max_pages: int = 10):
        self.start_url = start_url
        self.max_pages = max_pages
        self.visited_urls: Set[str] = set()
        self.results: List[Dict] = []
        
        # 設置日誌
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    async def fetch_page(self, session: aiohttp.ClientSession,
                        url: str) -> str:
        """異步獲取頁面內容"""
        try:
            async with session.get(url) as response:
                return await response.text()
        except Exception as e:
            self.logger.error(f'獲取頁面失敗: {url}, 錯誤: {e}')
            return None
    
    def parse_links(self, html: str, base_url: str) -> List[str]:
        """解析頁面中的鏈接"""
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        for a in soup.find_all('a', href=True):
            url = a['href']
            if url.startswith('/'):
                url = base_url + url
            if url.startswith(base_url) and url not in self.visited_urls:
                links.append(url)
        return links
    
    def parse_content(self, html: str, url: str) -> Dict:
        """解析頁面內容"""
        soup = BeautifulSoup(html, 'html.parser')
        return {
            'url': url,
            'title': soup.title.string if soup.title else '',
            'text': soup.get_text()[:200]  # 前200個字符
        }
    
    async def crawl(self):
        """開始爬取"""
        async with aiohttp.ClientSession() as session:
            await self._crawl_page(session, self.start_url)
    
    async def _crawl_page(self, session: aiohttp.ClientSession,
                         url: str):
        """遞歸爬取頁面"""
        if len(self.visited_urls) >= self.max_pages:
            return
        
        if url in self.visited_urls:
            return
        
        self.visited_urls.add(url)
        self.logger.info(f'正在爬取: {url}')
        
        html = await self.fetch_page(session, url)
        if not html:
            return
        
        # 解析內容
        content = self.parse_content(html, url)
        self.results.append(content)
        
        # 獲取新鏈接
        links = self.parse_links(html, self.start_url)
        
        # 創建新的爬取任務
        tasks = [self._crawl_page(session, link) for link in links]
        await asyncio.gather(*tasks)

# 使用示例
async def main():
    crawler = AsyncCrawler('https://example.com', max_pages=5)
    await crawler.crawl()
    print(f'爬取完成，共獲取 {len(crawler.results)} 個頁面')
    return crawler.results

if __name__ == '__main__':
    asyncio.run(main())
```

## 反爬處理

### 1. 代理池

```python
import requests
import random
from typing import List, Dict
import time

class ProxyPool:
    def __init__(self):
        self.proxies: List[Dict] = []
        self.current_proxy = None
        self.fail_count = 0
        self.max_fails = 3
    
    def add_proxy(self, proxy: Dict):
        """添加代理"""
        self.proxies.append(proxy)
    
    def get_proxy(self) -> Dict:
        """獲取代理"""
        if not self.current_proxy or self.fail_count >= self.max_fails:
            self.current_proxy = random.choice(self.proxies)
            self.fail_count = 0
        return self.current_proxy
    
    def mark_fail(self):
        """標記當前代理失敗"""
        self.fail_count += 1
    
    def remove_proxy(self, proxy: Dict):
        """移除失效代理"""
        if proxy in self.proxies:
            self.proxies.remove(proxy)

class ProxyRequests:
    def __init__(self, proxy_pool: ProxyPool):
        self.proxy_pool = proxy_pool
        self.session = requests.Session()
    
    def request(self, url: str, method: str = 'GET', **kwargs) -> requests.Response:
        """發送代理請求"""
        max_retries = 3
        for i in range(max_retries):
            proxy = self.proxy_pool.get_proxy()
            try:
                response = self.session.request(
                    method,
                    url,
                    proxies=proxy,
                    timeout=10,
                    **kwargs
                )
                response.raise_for_status()
                return response
            except Exception as e:
                print(f'代理請求失敗: {e}')
                self.proxy_pool.mark_fail()
                if i == max_retries - 1:
                    raise

# 使用示例
def main():
    # 初始化代理池
    pool = ProxyPool()
    pool.add_proxy({'http': 'http://proxy1.example.com:8080'})
    pool.add_proxy({'http': 'http://proxy2.example.com:8080'})
    
    # 創建請求對象
    requester = ProxyRequests(pool)
    
    # 發送請求
    try:
        response = requester.request('https://example.com')
        print('請求成功:', response.status_code)
    except Exception as e:
        print('請求失敗:', e)

if __name__ == '__main__':
    main()
```

### 2. 請求頻率控制

```python
import time
from typing import Dict
from collections import defaultdict

class RateLimit:
    def __init__(self, requests_per_second: float = 1.0):
        self.requests_per_second = requests_per_second
        self.last_request_time = defaultdict(float)
        self.minimum_interval = 1.0 / requests_per_second
    
    def wait(self, domain: str):
        """等待適當的時間以滿足頻率限制"""
        current_time = time.time()
        time_passed = current_time - self.last_request_time[domain]
        
        if time_passed < self.minimum_interval:
            sleep_time = self.minimum_interval - time_passed
            time.sleep(sleep_time)
        
        self.last_request_time[domain] = time.time()

class SmartCrawler:
    def __init__(self, rate_limit: RateLimit):
        self.rate_limit = rate_limit
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def get(self, url: str) -> requests.Response:
        """發送GET請求"""
        domain = url.split('/')[2]
        self.rate_limit.wait(domain)
        return self.session.get(url)
    
    def post(self, url: str, data: Dict) -> requests.Response:
        """發送POST請求"""
        domain = url.split('/')[2]
        self.rate_limit.wait(domain)
        return self.session.post(url, data=data)

# 使用示例
def main():
    # 創建限速器（每秒2個請求）
    rate_limit = RateLimit(2.0)
    crawler = SmartCrawler(rate_limit)
    
    # 發送請求
    urls = [
        'https://example.com/page1',
        'https://example.com/page2',
        'https://example.com/page3'
    ]
    
    for url in urls:
        try:
            response = crawler.get(url)
            print(f'請求成功: {url}, 狀態碼: {response.status_code}')
        except Exception as e:
            print(f'請求失敗: {url}, 錯誤: {e}')

if __name__ == '__main__':
    main()
```

## 練習題

1. **動態網站爬蟲**
   實現一個動態加載網站的爬蟲：
   - 處理AJAX請求
   - 等待頁面加載
   - 處理無限滾動
   - 提取動態內容

2. **分布式爬蟲**
   開發一個分布式爬蟲系統：
   - 任務分發
   - 結果匯總
   - 錯誤處理
   - 狀態監控

3. **反爬系統**
   實現一個反爬蟲系統：
   - IP限制
   - 用戶驗證
   - 行為分析
   - 日誌記錄

## 小提醒 💡

1. 性能優化
   - 使用異步編程
   - 實現並發控制
   - 優化資源使用
   - 處理內存管理

2. 穩定性保證
   - 完善錯誤處理
   - 實現重試機制
   - 添加日誌記錄
   - 監控系統狀態

3. 合規性
   - 遵守robots.txt
   - 設置合理延遲
   - 避免過度請求
   - 保護用戶隱私

4. 代碼質量
   - 模塊化設計
   - 代碼復用
   - 完善文檔
   - 單元測試

[上一章：網頁爬蟲基礎](056_網頁爬蟲基礎.md) | [下一章：自動化運維](058_自動化運維.md) 