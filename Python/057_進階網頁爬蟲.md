[ä¸Šä¸€ç« ï¼šç¶²é çˆ¬èŸ²åŸºç¤](056_ç¶²é çˆ¬èŸ²åŸºç¤.md) | [ä¸‹ä¸€ç« ï¼šè‡ªå‹•åŒ–é‹ç¶­](058_è‡ªå‹•åŒ–é‹ç¶­.md)

# Python é€²éšç¶²é çˆ¬èŸ² ğŸš€

## Seleniumè‡ªå‹•åŒ–

### 1. åŸºæœ¬æ“ä½œ

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
import time

# åˆå§‹åŒ–ç€è¦½å™¨
def init_browser():
    # è¨­ç½®Chromeé¸é …
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')  # ç„¡é ­æ¨¡å¼
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    
    # å‰µå»ºç€è¦½å™¨å¯¦ä¾‹
    driver = webdriver.Chrome(options=options)
    return driver

# åŸºæœ¬é é¢æ“ä½œ
def basic_operations():
    driver = init_browser()
    try:
        # è¨ªå•é é¢
        driver.get('https://www.example.com')
        
        # æŸ¥æ‰¾å…ƒç´ 
        element = driver.find_element(By.ID, 'search')
        element.send_keys('Python')
        element.send_keys(Keys.RETURN)
        
        # ç­‰å¾…å…ƒç´ åŠ è¼‰
        wait = WebDriverWait(driver, 10)
        results = wait.until(
            EC.presence_of_all_elements_located((By.CLASS_NAME, 'result'))
        )
        
        # ç²å–çµæœ
        for result in results:
            print(result.text)
            
    finally:
        driver.quit()

# è™•ç†JavaScript
def handle_javascript():
    driver = init_browser()
    try:
        driver.get('https://example.com')
        
        # åŸ·è¡ŒJavaScript
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        
        # ç­‰å¾…å‹•æ…‹åŠ è¼‰çš„å…§å®¹
        time.sleep(2)
        
        # ç²å–JavaScriptç”Ÿæˆçš„å…§å®¹
        content = driver.execute_script(
            "return document.getElementById('dynamic-content').innerHTML;"
        )
        print(content)
        
    finally:
        driver.quit()
```

### 2. é«˜ç´šåŠŸèƒ½

```python
from selenium.webdriver.common.action_chains import ActionChains

# è™•ç†å½ˆçª—
def handle_alerts():
    driver = init_browser()
    try:
        driver.get('https://example.com')
        
        # é»æ“Šè§¸ç™¼å½ˆçª—çš„æŒ‰éˆ•
        button = driver.find_element(By.ID, 'alert-button')
        button.click()
        
        # åˆ‡æ›åˆ°å½ˆçª—ä¸¦æ¥å—
        alert = driver.switch_to.alert
        print('å½ˆçª—æ–‡æœ¬:', alert.text)
        alert.accept()
        
    finally:
        driver.quit()

# è™•ç†iframe
def handle_iframes():
    driver = init_browser()
    try:
        driver.get('https://example.com')
        
        # åˆ‡æ›åˆ°iframe
        iframe = driver.find_element(By.TAG_NAME, 'iframe')
        driver.switch_to.frame(iframe)
        
        # åœ¨iframeä¸­æ“ä½œ
        element = driver.find_element(By.ID, 'frame-content')
        print(element.text)
        
        # åˆ‡å›ä¸»æ–‡æª”
        driver.switch_to.default_content()
        
    finally:
        driver.quit()

# é«˜ç´šé¼ æ¨™æ“ä½œ
def advanced_mouse_actions():
    driver = init_browser()
    try:
        driver.get('https://example.com')
        
        # å‰µå»ºActionChainså°è±¡
        actions = ActionChains(driver)
        
        # æ‰¾åˆ°ç›®æ¨™å…ƒç´ 
        menu = driver.find_element(By.ID, 'menu')
        submenu = driver.find_element(By.ID, 'submenu')
        
        # åŸ·è¡Œé¼ æ¨™æ‡¸åœ
        actions.move_to_element(menu)
        actions.move_to_element(submenu)
        actions.click()
        actions.perform()
        
    finally:
        driver.quit()
```

## ç•°æ­¥çˆ¬èŸ²

### 1. aiohttpåŸºç¤

```python
import aiohttp
import asyncio
from typing import List, Dict
import time

async def fetch_page(session: aiohttp.ClientSession, url: str) -> str:
    """ç•°æ­¥ç²å–é é¢å…§å®¹"""
    try:
        async with session.get(url) as response:
            return await response.text()
    except Exception as e:
        print(f'ç²å–é é¢å¤±æ•—: {url}, éŒ¯èª¤: {e}')
        return None

async def process_urls(urls: List[str]) -> List[Dict]:
    """ä¸¦ç™¼è™•ç†å¤šå€‹URL"""
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_page(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
        return [{'url': url, 'content': content}
                for url, content in zip(urls, results)
                if content is not None]

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    urls = [
        'http://example.com/page1',
        'http://example.com/page2',
        'http://example.com/page3'
    ]
    
    start_time = time.time()
    results = await process_urls(urls)
    end_time = time.time()
    
    print(f'è™•ç† {len(urls)} å€‹URLç”¨æ™‚: {end_time - start_time:.2f}ç§’')
    return results

# é‹è¡Œç•°æ­¥ç¨‹åº
if __name__ == '__main__':
    asyncio.run(main())
```

### 2. ç•°æ­¥çˆ¬èŸ²æ¡†æ¶

```python
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from typing import List, Dict, Set
import logging

class AsyncCrawler:
    def __init__(self, start_url: str, max_pages: int = 10):
        self.start_url = start_url
        self.max_pages = max_pages
        self.visited_urls: Set[str] = set()
        self.results: List[Dict] = []
        
        # è¨­ç½®æ—¥èªŒ
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    async def fetch_page(self, session: aiohttp.ClientSession,
                        url: str) -> str:
        """ç•°æ­¥ç²å–é é¢å…§å®¹"""
        try:
            async with session.get(url) as response:
                return await response.text()
        except Exception as e:
            self.logger.error(f'ç²å–é é¢å¤±æ•—: {url}, éŒ¯èª¤: {e}')
            return None
    
    def parse_links(self, html: str, base_url: str) -> List[str]:
        """è§£æé é¢ä¸­çš„éˆæ¥"""
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        for a in soup.find_all('a', href=True):
            url = a['href']
            if url.startswith('/'):
                url = base_url + url
            if url.startswith(base_url) and url not in self.visited_urls:
                links.append(url)
        return links
    
    def parse_content(self, html: str, url: str) -> Dict:
        """è§£æé é¢å…§å®¹"""
        soup = BeautifulSoup(html, 'html.parser')
        return {
            'url': url,
            'title': soup.title.string if soup.title else '',
            'text': soup.get_text()[:200]  # å‰200å€‹å­—ç¬¦
        }
    
    async def crawl(self):
        """é–‹å§‹çˆ¬å–"""
        async with aiohttp.ClientSession() as session:
            await self._crawl_page(session, self.start_url)
    
    async def _crawl_page(self, session: aiohttp.ClientSession,
                         url: str):
        """éæ­¸çˆ¬å–é é¢"""
        if len(self.visited_urls) >= self.max_pages:
            return
        
        if url in self.visited_urls:
            return
        
        self.visited_urls.add(url)
        self.logger.info(f'æ­£åœ¨çˆ¬å–: {url}')
        
        html = await self.fetch_page(session, url)
        if not html:
            return
        
        # è§£æå…§å®¹
        content = self.parse_content(html, url)
        self.results.append(content)
        
        # ç²å–æ–°éˆæ¥
        links = self.parse_links(html, self.start_url)
        
        # å‰µå»ºæ–°çš„çˆ¬å–ä»»å‹™
        tasks = [self._crawl_page(session, link) for link in links]
        await asyncio.gather(*tasks)

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    crawler = AsyncCrawler('https://example.com', max_pages=5)
    await crawler.crawl()
    print(f'çˆ¬å–å®Œæˆï¼Œå…±ç²å– {len(crawler.results)} å€‹é é¢')
    return crawler.results

if __name__ == '__main__':
    asyncio.run(main())
```

## åçˆ¬è™•ç†

### 1. ä»£ç†æ± 

```python
import requests
import random
from typing import List, Dict
import time

class ProxyPool:
    def __init__(self):
        self.proxies: List[Dict] = []
        self.current_proxy = None
        self.fail_count = 0
        self.max_fails = 3
    
    def add_proxy(self, proxy: Dict):
        """æ·»åŠ ä»£ç†"""
        self.proxies.append(proxy)
    
    def get_proxy(self) -> Dict:
        """ç²å–ä»£ç†"""
        if not self.current_proxy or self.fail_count >= self.max_fails:
            self.current_proxy = random.choice(self.proxies)
            self.fail_count = 0
        return self.current_proxy
    
    def mark_fail(self):
        """æ¨™è¨˜ç•¶å‰ä»£ç†å¤±æ•—"""
        self.fail_count += 1
    
    def remove_proxy(self, proxy: Dict):
        """ç§»é™¤å¤±æ•ˆä»£ç†"""
        if proxy in self.proxies:
            self.proxies.remove(proxy)

class ProxyRequests:
    def __init__(self, proxy_pool: ProxyPool):
        self.proxy_pool = proxy_pool
        self.session = requests.Session()
    
    def request(self, url: str, method: str = 'GET', **kwargs) -> requests.Response:
        """ç™¼é€ä»£ç†è«‹æ±‚"""
        max_retries = 3
        for i in range(max_retries):
            proxy = self.proxy_pool.get_proxy()
            try:
                response = self.session.request(
                    method,
                    url,
                    proxies=proxy,
                    timeout=10,
                    **kwargs
                )
                response.raise_for_status()
                return response
            except Exception as e:
                print(f'ä»£ç†è«‹æ±‚å¤±æ•—: {e}')
                self.proxy_pool.mark_fail()
                if i == max_retries - 1:
                    raise

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # åˆå§‹åŒ–ä»£ç†æ± 
    pool = ProxyPool()
    pool.add_proxy({'http': 'http://proxy1.example.com:8080'})
    pool.add_proxy({'http': 'http://proxy2.example.com:8080'})
    
    # å‰µå»ºè«‹æ±‚å°è±¡
    requester = ProxyRequests(pool)
    
    # ç™¼é€è«‹æ±‚
    try:
        response = requester.request('https://example.com')
        print('è«‹æ±‚æˆåŠŸ:', response.status_code)
    except Exception as e:
        print('è«‹æ±‚å¤±æ•—:', e)

if __name__ == '__main__':
    main()
```

### 2. è«‹æ±‚é »ç‡æ§åˆ¶

```python
import time
from typing import Dict
from collections import defaultdict

class RateLimit:
    def __init__(self, requests_per_second: float = 1.0):
        self.requests_per_second = requests_per_second
        self.last_request_time = defaultdict(float)
        self.minimum_interval = 1.0 / requests_per_second
    
    def wait(self, domain: str):
        """ç­‰å¾…é©ç•¶çš„æ™‚é–“ä»¥æ»¿è¶³é »ç‡é™åˆ¶"""
        current_time = time.time()
        time_passed = current_time - self.last_request_time[domain]
        
        if time_passed < self.minimum_interval:
            sleep_time = self.minimum_interval - time_passed
            time.sleep(sleep_time)
        
        self.last_request_time[domain] = time.time()

class SmartCrawler:
    def __init__(self, rate_limit: RateLimit):
        self.rate_limit = rate_limit
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def get(self, url: str) -> requests.Response:
        """ç™¼é€GETè«‹æ±‚"""
        domain = url.split('/')[2]
        self.rate_limit.wait(domain)
        return self.session.get(url)
    
    def post(self, url: str, data: Dict) -> requests.Response:
        """ç™¼é€POSTè«‹æ±‚"""
        domain = url.split('/')[2]
        self.rate_limit.wait(domain)
        return self.session.post(url, data=data)

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # å‰µå»ºé™é€Ÿå™¨ï¼ˆæ¯ç§’2å€‹è«‹æ±‚ï¼‰
    rate_limit = RateLimit(2.0)
    crawler = SmartCrawler(rate_limit)
    
    # ç™¼é€è«‹æ±‚
    urls = [
        'https://example.com/page1',
        'https://example.com/page2',
        'https://example.com/page3'
    ]
    
    for url in urls:
        try:
            response = crawler.get(url)
            print(f'è«‹æ±‚æˆåŠŸ: {url}, ç‹€æ…‹ç¢¼: {response.status_code}')
        except Exception as e:
            print(f'è«‹æ±‚å¤±æ•—: {url}, éŒ¯èª¤: {e}')

if __name__ == '__main__':
    main()
```

## ç·´ç¿’é¡Œ

1. **å‹•æ…‹ç¶²ç«™çˆ¬èŸ²**
   å¯¦ç¾ä¸€å€‹å‹•æ…‹åŠ è¼‰ç¶²ç«™çš„çˆ¬èŸ²ï¼š
   - è™•ç†AJAXè«‹æ±‚
   - ç­‰å¾…é é¢åŠ è¼‰
   - è™•ç†ç„¡é™æ»¾å‹•
   - æå–å‹•æ…‹å…§å®¹

2. **åˆ†å¸ƒå¼çˆ¬èŸ²**
   é–‹ç™¼ä¸€å€‹åˆ†å¸ƒå¼çˆ¬èŸ²ç³»çµ±ï¼š
   - ä»»å‹™åˆ†ç™¼
   - çµæœåŒ¯ç¸½
   - éŒ¯èª¤è™•ç†
   - ç‹€æ…‹ç›£æ§

3. **åçˆ¬ç³»çµ±**
   å¯¦ç¾ä¸€å€‹åçˆ¬èŸ²ç³»çµ±ï¼š
   - IPé™åˆ¶
   - ç”¨æˆ¶é©—è­‰
   - è¡Œç‚ºåˆ†æ
   - æ—¥èªŒè¨˜éŒ„

## å°æé†’ ğŸ’¡

1. æ€§èƒ½å„ªåŒ–
   - ä½¿ç”¨ç•°æ­¥ç·¨ç¨‹
   - å¯¦ç¾ä¸¦ç™¼æ§åˆ¶
   - å„ªåŒ–è³‡æºä½¿ç”¨
   - è™•ç†å…§å­˜ç®¡ç†

2. ç©©å®šæ€§ä¿è­‰
   - å®Œå–„éŒ¯èª¤è™•ç†
   - å¯¦ç¾é‡è©¦æ©Ÿåˆ¶
   - æ·»åŠ æ—¥èªŒè¨˜éŒ„
   - ç›£æ§ç³»çµ±ç‹€æ…‹

3. åˆè¦æ€§
   - éµå®ˆrobots.txt
   - è¨­ç½®åˆç†å»¶é²
   - é¿å…éåº¦è«‹æ±‚
   - ä¿è­·ç”¨æˆ¶éš±ç§

4. ä»£ç¢¼è³ªé‡
   - æ¨¡å¡ŠåŒ–è¨­è¨ˆ
   - ä»£ç¢¼å¾©ç”¨
   - å®Œå–„æ–‡æª”
   - å–®å…ƒæ¸¬è©¦

[ä¸Šä¸€ç« ï¼šç¶²é çˆ¬èŸ²åŸºç¤](056_ç¶²é çˆ¬èŸ²åŸºç¤.md) | [ä¸‹ä¸€ç« ï¼šè‡ªå‹•åŒ–é‹ç¶­](058_è‡ªå‹•åŒ–é‹ç¶­.md) 