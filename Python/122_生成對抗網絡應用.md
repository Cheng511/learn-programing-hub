[上一章：強化學習實踐](121_強化學習實踐.md) | [下一章：圖神經網絡](123_圖神經網絡.md)

# Python 生成對抗網絡應用 🎨

## 1. GAN基礎架構

### 1.1 基本組件

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from typing import Tuple

class Generator(nn.Module):
    def __init__(self, latent_dim: int, img_shape: Tuple[int, int, int]):
        super().__init__()
        self.img_shape = img_shape
        
        def block(in_feat: int, out_feat: int, normalize: bool = True):
            layers = [nn.Linear(in_feat, out_feat)]
            if normalize:
                layers.append(nn.BatchNorm1d(out_feat, 0.8))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers
        
        self.model = nn.Sequential(
            *block(latent_dim, 128, normalize=False),
            *block(128, 256),
            *block(256, 512),
            *block(512, 1024),
            nn.Linear(1024, int(torch.prod(torch.tensor(img_shape)))),
            nn.Tanh()
        )
    
    def forward(self, z: torch.Tensor) -> torch.Tensor:
        img = self.model(z)
        img = img.view(img.size(0), *self.img_shape)
        return img

class Discriminator(nn.Module):
    def __init__(self, img_shape: Tuple[int, int, int]):
        super().__init__()
        
        self.model = nn.Sequential(
            nn.Linear(int(torch.prod(torch.tensor(img_shape))), 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
    
    def forward(self, img: torch.Tensor) -> torch.Tensor:
        img_flat = img.view(img.size(0), -1)
        validity = self.model(img_flat)
        return validity
```

### 1.2 訓練流程

```python
class GANTrainer:
    def __init__(self, latent_dim: int, img_shape: Tuple[int, int, int],
                 device: str = 'cuda'):
        self.latent_dim = latent_dim
        self.device = device
        
        # 初始化網絡
        self.generator = Generator(latent_dim, img_shape).to(device)
        self.discriminator = Discriminator(img_shape).to(device)
        
        # 優化器
        self.g_optimizer = optim.Adam(self.generator.parameters(),
                                    lr=0.0002, betas=(0.5, 0.999))
        self.d_optimizer = optim.Adam(self.discriminator.parameters(),
                                    lr=0.0002, betas=(0.5, 0.999))
        
        self.criterion = nn.BCELoss()
    
    def train_step(self, real_imgs: torch.Tensor) -> Tuple[float, float]:
        """執行一步訓練"""
        batch_size = real_imgs.size(0)
        
        # 真實和虛假標籤
        valid = torch.ones(batch_size, 1).to(self.device)
        fake = torch.zeros(batch_size, 1).to(self.device)
        
        # 訓練判別器
        self.d_optimizer.zero_grad()
        real_loss = self.criterion(self.discriminator(real_imgs), valid)
        
        z = torch.randn(batch_size, self.latent_dim).to(self.device)
        fake_imgs = self.generator(z)
        fake_loss = self.criterion(self.discriminator(fake_imgs.detach()), fake)
        
        d_loss = (real_loss + fake_loss) / 2
        d_loss.backward()
        self.d_optimizer.step()
        
        # 訓練生成器
        self.g_optimizer.zero_grad()
        g_loss = self.criterion(self.discriminator(fake_imgs), valid)
        g_loss.backward()
        self.g_optimizer.step()
        
        return d_loss.item(), g_loss.item()
```

## 2. 條件GAN

### 2.1 CGAN實現

```python
class ConditionalGenerator(nn.Module):
    def __init__(self, latent_dim: int, n_classes: int,
                 img_shape: Tuple[int, int, int]):
        super().__init__()
        self.img_shape = img_shape
        self.label_emb = nn.Embedding(n_classes, n_classes)
        
        def block(in_feat: int, out_feat: int, normalize: bool = True):
            layers = [nn.Linear(in_feat, out_feat)]
            if normalize:
                layers.append(nn.BatchNorm1d(out_feat, 0.8))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers
        
        self.model = nn.Sequential(
            *block(latent_dim + n_classes, 128, normalize=False),
            *block(128, 256),
            *block(256, 512),
            *block(512, 1024),
            nn.Linear(1024, int(torch.prod(torch.tensor(img_shape)))),
            nn.Tanh()
        )
    
    def forward(self, noise: torch.Tensor,
                labels: torch.Tensor) -> torch.Tensor:
        gen_input = torch.cat((self.label_emb(labels), noise), -1)
        img = self.model(gen_input)
        img = img.view(img.size(0), *self.img_shape)
        return img

class ConditionalDiscriminator(nn.Module):
    def __init__(self, n_classes: int, img_shape: Tuple[int, int, int]):
        super().__init__()
        self.label_embedding = nn.Embedding(n_classes, n_classes)
        
        self.model = nn.Sequential(
            nn.Linear(n_classes + int(torch.prod(torch.tensor(img_shape))), 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 512),
            nn.Dropout(0.4),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 512),
            nn.Dropout(0.4),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 1),
            nn.Sigmoid()
        )
    
    def forward(self, img: torch.Tensor,
                labels: torch.Tensor) -> torch.Tensor:
        d_in = torch.cat((img.view(img.size(0), -1),
                         self.label_embedding(labels)), -1)
        validity = self.model(d_in)
        return validity
```

## 3. 高級GAN架構

### 3.1 WGAN-GP實現

```python
class WGANGenerator(nn.Module):
    def __init__(self, latent_dim: int, img_shape: Tuple[int, int, int]):
        super().__init__()
        
        self.model = nn.Sequential(
            *self._block(latent_dim, 128, normalize=False),
            *self._block(128, 256),
            *self._block(256, 512),
            *self._block(512, 1024),
            nn.Linear(1024, int(torch.prod(torch.tensor(img_shape)))),
            nn.Tanh()
        )
    
    def _block(self, in_feat: int, out_feat: int,
               normalize: bool = True) -> List[nn.Module]:
        layers = [nn.Linear(in_feat, out_feat)]
        if normalize:
            layers.append(nn.BatchNorm1d(out_feat, 0.8))
        layers.append(nn.LeakyReLU(0.2, inplace=True))
        return layers
    
    def forward(self, z: torch.Tensor) -> torch.Tensor:
        return self.model(z)

class WGANCritic(nn.Module):
    def __init__(self, img_shape: Tuple[int, int, int]):
        super().__init__()
        
        self.model = nn.Sequential(
            nn.Linear(int(torch.prod(torch.tensor(img_shape))), 512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Linear(256, 1)
        )
    
    def forward(self, img: torch.Tensor) -> torch.Tensor:
        img_flat = img.view(img.size(0), -1)
        return self.model(img_flat)
```

### 3.2 StyleGAN特性

```python
class AdaIN(nn.Module):
    def __init__(self, in_channel: int, style_dim: int):
        super().__init__()
        
        self.norm = nn.InstanceNorm2d(in_channel)
        self.style = nn.Linear(style_dim, in_channel * 2)
    
    def forward(self, input: torch.Tensor,
                style: torch.Tensor) -> torch.Tensor:
        style = self.style(style).unsqueeze(2).unsqueeze(3)
        gamma, beta = style.chunk(2, 1)
        
        out = self.norm(input)
        return gamma * out + beta

class StyleBlock(nn.Module):
    def __init__(self, in_channel: int, out_channel: int,
                 style_dim: int):
        super().__init__()
        
        self.conv = nn.Conv2d(in_channel, out_channel, 3, padding=1)
        self.adain = AdaIN(out_channel, style_dim)
        self.activation = nn.LeakyReLU(0.2, inplace=True)
    
    def forward(self, input: torch.Tensor,
                style: torch.Tensor) -> torch.Tensor:
        out = self.conv(input)
        out = self.adain(out, style)
        return self.activation(out)
```

## 練習題 🏃

1. 實現一個基本的DCGAN來生成手寫數字。
2. 使用CGAN生成特定類別的圖像。
3. 實現WGAN-GP並比較其與原始GAN的差異。
4. 開發一個基於StyleGAN的人臉生成系統。
5. 設計一個圖像風格遷移應用。

## 小結 📝

- 學習了GAN的基本架構
- 掌握了條件GAN的實現
- 理解了WGAN-GP的改進
- 學會了StyleGAN的特性
- 了解了GAN的實際應用

## 延伸閱讀 📚

1. GAN: Generative Adversarial Networks
2. Progressive Growing of GANs
3. StyleGAN and StyleGAN2
4. Conditional Image Synthesis
5. Advanced GAN Techniques

[上一章：強化學習實踐](121_強化學習實踐.md) | [下一章：圖神經網絡](123_圖神經網絡.md) 