[ä¸Šä¸€ç« ï¼šåœ–ç¥ç¶“ç¶²çµ¡](123_åœ–ç¥ç¶“ç¶²çµ¡.md) | [ä¸‹ä¸€ç« ï¼šAIå®‰å…¨èˆ‡éš±ç§](125_AIå®‰å…¨èˆ‡éš±ç§.md)

# Python è¯é‚¦å­¸ç¿’åŸºç¤ ğŸŒ

## 1. è¯é‚¦å­¸ç¿’æ¶æ§‹

### 1.1 åŸºæœ¬çµ„ä»¶

```python
import torch
import torch.nn as nn
import torch.optim as optim
from typing import List, Dict, Tuple
import numpy as np
from dataclasses import dataclass
import copy

@dataclass
class ClientConfig:
    """å®¢æˆ¶ç«¯é…ç½®"""
    client_id: str
    local_epochs: int
    batch_size: int
    learning_rate: float

class FederatedClient:
    def __init__(self, config: ClientConfig,
                 model: nn.Module, data: Tuple[torch.Tensor, torch.Tensor]):
        self.config = config
        self.model = copy.deepcopy(model)
        self.data = data
        self.optimizer = optim.SGD(self.model.parameters(),
                                 lr=config.learning_rate)
    
    def train(self) -> nn.Module:
        """æœ¬åœ°è¨“ç·´"""
        self.model.train()
        x, y = self.data
        
        for _ in range(self.config.local_epochs):
            self.optimizer.zero_grad()
            output = self.model(x)
            loss = nn.CrossEntropyLoss()(output, y)
            loss.backward()
            self.optimizer.step()
        
        return self.model
```

### 1.2 æœå‹™å™¨ç«¯å¯¦ç¾

```python
class FederatedServer:
    def __init__(self, model: nn.Module,
                 num_clients: int):
        self.global_model = model
        self.num_clients = num_clients
        self.selected_clients: List[FederatedClient] = []
    
    def select_clients(self, clients: List[FederatedClient],
                      num_selected: int) -> List[FederatedClient]:
        """é¸æ“‡åƒèˆ‡è¨“ç·´çš„å®¢æˆ¶ç«¯"""
        return np.random.choice(clients,
                              size=num_selected,
                              replace=False).tolist()
    
    def aggregate_models(self,
                        client_models: List[nn.Module]) -> nn.Module:
        """èšåˆå®¢æˆ¶ç«¯æ¨¡å‹"""
        global_dict = self.global_model.state_dict()
        
        # è¨ˆç®—å¹³å‡å€¼
        for key in global_dict.keys():
            global_dict[key] = torch.stack([
                client_model.state_dict()[key]
                for client_model in client_models
            ]).mean(0)
        
        self.global_model.load_state_dict(global_dict)
        return self.global_model
```

## 2. é«˜ç´šè¯é‚¦å­¸ç¿’ç®—æ³•

### 2.1 FedAvgå¯¦ç¾

```python
class FedAvg:
    def __init__(self, server: FederatedServer,
                 clients: List[FederatedClient]):
        self.server = server
        self.clients = clients
    
    def train_round(self, num_selected: int) -> nn.Module:
        """åŸ·è¡Œä¸€è¼ªè¯é‚¦å­¸ç¿’"""
        # é¸æ“‡å®¢æˆ¶ç«¯
        selected_clients = self.server.select_clients(
            self.clients,
            num_selected
        )
        
        # æœ¬åœ°è¨“ç·´
        client_models = []
        for client in selected_clients:
            client.model.load_state_dict(
                self.server.global_model.state_dict()
            )
            client_models.append(client.train())
        
        # æ¨¡å‹èšåˆ
        return self.server.aggregate_models(client_models)
```

### 2.2 FedProxå¯¦ç¾

```python
class FedProxClient(FederatedClient):
    def __init__(self, config: ClientConfig,
                 model: nn.Module,
                 data: Tuple[torch.Tensor, torch.Tensor],
                 mu: float = 0.01):
        super().__init__(config, model, data)
        self.mu = mu
        self.global_model = copy.deepcopy(model)
    
    def proximal_term(self) -> torch.Tensor:
        """è¨ˆç®—è¿‘ç«¯é …"""
        proximal_term = 0
        for w, w_t in zip(self.model.parameters(),
                         self.global_model.parameters()):
            proximal_term += (w - w_t).norm(2)
        return (self.mu / 2) * proximal_term
    
    def train(self) -> nn.Module:
        """æœ¬åœ°è¨“ç·´ï¼ˆå¸¶è¿‘ç«¯é …ï¼‰"""
        self.model.train()
        x, y = self.data
        
        for _ in range(self.config.local_epochs):
            self.optimizer.zero_grad()
            output = self.model(x)
            loss = nn.CrossEntropyLoss()(output, y)
            
            # æ·»åŠ è¿‘ç«¯é …
            proximal_term = self.proximal_term()
            total_loss = loss + proximal_term
            
            total_loss.backward()
            self.optimizer.step()
        
        return self.model
```

## 3. éš±ç§ä¿è­·æ©Ÿåˆ¶

### 3.1 å·®åˆ†éš±ç§

```python
class DPFederatedClient(FederatedClient):
    def __init__(self, config: ClientConfig,
                 model: nn.Module,
                 data: Tuple[torch.Tensor, torch.Tensor],
                 epsilon: float,
                 delta: float):
        super().__init__(config, model, data)
        self.epsilon = epsilon
        self.delta = delta
    
    def add_noise(self, gradients: torch.Tensor) -> torch.Tensor:
        """æ·»åŠ é«˜æ–¯å™ªè²"""
        sensitivity = self.compute_sensitivity(gradients)
        noise_scale = np.sqrt(2 * np.log(1.25/self.delta)) / self.epsilon
        noise = torch.normal(0, sensitivity * noise_scale,
                           size=gradients.shape)
        return gradients + noise
    
    def train(self) -> nn.Module:
        """å·®åˆ†éš±ç§è¨“ç·´"""
        self.model.train()
        x, y = self.data
        
        for _ in range(self.config.local_epochs):
            self.optimizer.zero_grad()
            output = self.model(x)
            loss = nn.CrossEntropyLoss()(output, y)
            loss.backward()
            
            # æ·»åŠ å™ªè²åˆ°æ¢¯åº¦
            for param in self.model.parameters():
                if param.grad is not None:
                    param.grad = self.add_noise(param.grad)
            
            self.optimizer.step()
        
        return self.model
```

### 3.2 å®‰å…¨èšåˆ

```python
class SecureAggregation:
    def __init__(self, num_clients: int, key_size: int = 256):
        self.num_clients = num_clients
        self.key_size = key_size
        self.keys = self.generate_keys()
    
    def generate_keys(self) -> Dict[Tuple[int, int], bytes]:
        """ç”Ÿæˆå¯†é‘°å°"""
        keys = {}
        for i in range(self.num_clients):
            for j in range(i + 1, self.num_clients):
                keys[(i, j)] = np.random.bytes(self.key_size)
                keys[(j, i)] = keys[(i, j)]
        return keys
    
    def mask_model(self, client_id: int,
                   model_params: torch.Tensor) -> torch.Tensor:
        """æ·»åŠ æ©ç¢¼"""
        mask = torch.zeros_like(model_params)
        for other_id in range(self.num_clients):
            if other_id != client_id:
                key = self.keys[(client_id, other_id)]
                np.random.seed(int.from_bytes(key, byteorder='big'))
                if client_id < other_id:
                    mask += torch.tensor(np.random.normal(size=model_params.shape))
                else:
                    mask -= torch.tensor(np.random.normal(size=model_params.shape))
        return model_params + mask
    
    def aggregate(self, masked_models: List[torch.Tensor]) -> torch.Tensor:
        """å®‰å…¨èšåˆ"""
        return torch.stack(masked_models).mean(0)
```

## ç·´ç¿’é¡Œ ğŸƒ

1. å¯¦ç¾ä¸€å€‹åŸºæœ¬çš„è¯é‚¦å­¸ç¿’ç³»çµ±ã€‚
2. ä½¿ç”¨FedProxç®—æ³•è™•ç†éIIDæ•¸æ“šã€‚
3. é–‹ç™¼ä¸€å€‹å¸¶å·®åˆ†éš±ç§çš„è¯é‚¦å­¸ç¿’æ¡†æ¶ã€‚
4. å¯¦ç¾å®‰å…¨èšåˆå”è­°ã€‚
5. è¨­è¨ˆä¸€å€‹è¯é‚¦å­¸ç¿’æ€§èƒ½è©•ä¼°ç³»çµ±ã€‚

## å°çµ ğŸ“

- å­¸ç¿’äº†è¯é‚¦å­¸ç¿’çš„åŸºæœ¬æ¶æ§‹
- æŒæ¡äº†FedAvgå’ŒFedProxç®—æ³•
- ç†è§£äº†å·®åˆ†éš±ç§çš„æ‡‰ç”¨
- å­¸æœƒäº†å®‰å…¨èšåˆçš„å¯¦ç¾
- äº†è§£äº†è¯é‚¦å­¸ç¿’çš„éš±ç§ä¿è­·æ©Ÿåˆ¶

## å»¶ä¼¸é–±è®€ ğŸ“š

1. Federated Learning: Concepts and Applications
2. Privacy-Preserving Machine Learning
3. Secure Aggregation Protocols
4. Differential Privacy in Deep Learning
5. Non-IID Data in Federated Learning

[ä¸Šä¸€ç« ï¼šåœ–ç¥ç¶“ç¶²çµ¡](123_åœ–ç¥ç¶“ç¶²çµ¡.md) | [ä¸‹ä¸€ç« ï¼šAIå®‰å…¨èˆ‡éš±ç§](125_AIå®‰å…¨èˆ‡éš±ç§.md) 