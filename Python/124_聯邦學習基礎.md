[上一章：圖神經網絡](123_圖神經網絡.md) | [下一章：AI安全與隱私](125_AI安全與隱私.md)

# Python 聯邦學習基礎 🌐

## 1. 聯邦學習架構

### 1.1 基本組件

```python
import torch
import torch.nn as nn
import torch.optim as optim
from typing import List, Dict, Tuple
import numpy as np
from dataclasses import dataclass
import copy

@dataclass
class ClientConfig:
    """客戶端配置"""
    client_id: str
    local_epochs: int
    batch_size: int
    learning_rate: float

class FederatedClient:
    def __init__(self, config: ClientConfig,
                 model: nn.Module, data: Tuple[torch.Tensor, torch.Tensor]):
        self.config = config
        self.model = copy.deepcopy(model)
        self.data = data
        self.optimizer = optim.SGD(self.model.parameters(),
                                 lr=config.learning_rate)
    
    def train(self) -> nn.Module:
        """本地訓練"""
        self.model.train()
        x, y = self.data
        
        for _ in range(self.config.local_epochs):
            self.optimizer.zero_grad()
            output = self.model(x)
            loss = nn.CrossEntropyLoss()(output, y)
            loss.backward()
            self.optimizer.step()
        
        return self.model
```

### 1.2 服務器端實現

```python
class FederatedServer:
    def __init__(self, model: nn.Module,
                 num_clients: int):
        self.global_model = model
        self.num_clients = num_clients
        self.selected_clients: List[FederatedClient] = []
    
    def select_clients(self, clients: List[FederatedClient],
                      num_selected: int) -> List[FederatedClient]:
        """選擇參與訓練的客戶端"""
        return np.random.choice(clients,
                              size=num_selected,
                              replace=False).tolist()
    
    def aggregate_models(self,
                        client_models: List[nn.Module]) -> nn.Module:
        """聚合客戶端模型"""
        global_dict = self.global_model.state_dict()
        
        # 計算平均值
        for key in global_dict.keys():
            global_dict[key] = torch.stack([
                client_model.state_dict()[key]
                for client_model in client_models
            ]).mean(0)
        
        self.global_model.load_state_dict(global_dict)
        return self.global_model
```

## 2. 高級聯邦學習算法

### 2.1 FedAvg實現

```python
class FedAvg:
    def __init__(self, server: FederatedServer,
                 clients: List[FederatedClient]):
        self.server = server
        self.clients = clients
    
    def train_round(self, num_selected: int) -> nn.Module:
        """執行一輪聯邦學習"""
        # 選擇客戶端
        selected_clients = self.server.select_clients(
            self.clients,
            num_selected
        )
        
        # 本地訓練
        client_models = []
        for client in selected_clients:
            client.model.load_state_dict(
                self.server.global_model.state_dict()
            )
            client_models.append(client.train())
        
        # 模型聚合
        return self.server.aggregate_models(client_models)
```

### 2.2 FedProx實現

```python
class FedProxClient(FederatedClient):
    def __init__(self, config: ClientConfig,
                 model: nn.Module,
                 data: Tuple[torch.Tensor, torch.Tensor],
                 mu: float = 0.01):
        super().__init__(config, model, data)
        self.mu = mu
        self.global_model = copy.deepcopy(model)
    
    def proximal_term(self) -> torch.Tensor:
        """計算近端項"""
        proximal_term = 0
        for w, w_t in zip(self.model.parameters(),
                         self.global_model.parameters()):
            proximal_term += (w - w_t).norm(2)
        return (self.mu / 2) * proximal_term
    
    def train(self) -> nn.Module:
        """本地訓練（帶近端項）"""
        self.model.train()
        x, y = self.data
        
        for _ in range(self.config.local_epochs):
            self.optimizer.zero_grad()
            output = self.model(x)
            loss = nn.CrossEntropyLoss()(output, y)
            
            # 添加近端項
            proximal_term = self.proximal_term()
            total_loss = loss + proximal_term
            
            total_loss.backward()
            self.optimizer.step()
        
        return self.model
```

## 3. 隱私保護機制

### 3.1 差分隱私

```python
class DPFederatedClient(FederatedClient):
    def __init__(self, config: ClientConfig,
                 model: nn.Module,
                 data: Tuple[torch.Tensor, torch.Tensor],
                 epsilon: float,
                 delta: float):
        super().__init__(config, model, data)
        self.epsilon = epsilon
        self.delta = delta
    
    def add_noise(self, gradients: torch.Tensor) -> torch.Tensor:
        """添加高斯噪聲"""
        sensitivity = self.compute_sensitivity(gradients)
        noise_scale = np.sqrt(2 * np.log(1.25/self.delta)) / self.epsilon
        noise = torch.normal(0, sensitivity * noise_scale,
                           size=gradients.shape)
        return gradients + noise
    
    def train(self) -> nn.Module:
        """差分隱私訓練"""
        self.model.train()
        x, y = self.data
        
        for _ in range(self.config.local_epochs):
            self.optimizer.zero_grad()
            output = self.model(x)
            loss = nn.CrossEntropyLoss()(output, y)
            loss.backward()
            
            # 添加噪聲到梯度
            for param in self.model.parameters():
                if param.grad is not None:
                    param.grad = self.add_noise(param.grad)
            
            self.optimizer.step()
        
        return self.model
```

### 3.2 安全聚合

```python
class SecureAggregation:
    def __init__(self, num_clients: int, key_size: int = 256):
        self.num_clients = num_clients
        self.key_size = key_size
        self.keys = self.generate_keys()
    
    def generate_keys(self) -> Dict[Tuple[int, int], bytes]:
        """生成密鑰對"""
        keys = {}
        for i in range(self.num_clients):
            for j in range(i + 1, self.num_clients):
                keys[(i, j)] = np.random.bytes(self.key_size)
                keys[(j, i)] = keys[(i, j)]
        return keys
    
    def mask_model(self, client_id: int,
                   model_params: torch.Tensor) -> torch.Tensor:
        """添加掩碼"""
        mask = torch.zeros_like(model_params)
        for other_id in range(self.num_clients):
            if other_id != client_id:
                key = self.keys[(client_id, other_id)]
                np.random.seed(int.from_bytes(key, byteorder='big'))
                if client_id < other_id:
                    mask += torch.tensor(np.random.normal(size=model_params.shape))
                else:
                    mask -= torch.tensor(np.random.normal(size=model_params.shape))
        return model_params + mask
    
    def aggregate(self, masked_models: List[torch.Tensor]) -> torch.Tensor:
        """安全聚合"""
        return torch.stack(masked_models).mean(0)
```

## 練習題 🏃

1. 實現一個基本的聯邦學習系統。
2. 使用FedProx算法處理非IID數據。
3. 開發一個帶差分隱私的聯邦學習框架。
4. 實現安全聚合協議。
5. 設計一個聯邦學習性能評估系統。

## 小結 📝

- 學習了聯邦學習的基本架構
- 掌握了FedAvg和FedProx算法
- 理解了差分隱私的應用
- 學會了安全聚合的實現
- 了解了聯邦學習的隱私保護機制

## 延伸閱讀 📚

1. Federated Learning: Concepts and Applications
2. Privacy-Preserving Machine Learning
3. Secure Aggregation Protocols
4. Differential Privacy in Deep Learning
5. Non-IID Data in Federated Learning

[上一章：圖神經網絡](123_圖神經網絡.md) | [下一章：AI安全與隱私](125_AI安全與隱私.md) 