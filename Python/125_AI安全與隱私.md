[ä¸Šä¸€ç« ï¼šè¯é‚¦å­¸ç¿’åŸºç¤](124_è¯é‚¦å­¸ç¿’åŸºç¤.md) | [ä¸‹ä¸€ç« ï¼šå¾®æœå‹™æ¶æ§‹è¨­è¨ˆ](126_å¾®æœå‹™æ¶æ§‹è¨­è¨ˆ.md)

# Python AIå®‰å…¨èˆ‡éš±ç§ ğŸ”’

## 1. æ¨¡å‹å®‰å…¨

### 1.1 å°æŠ—æ¨£æœ¬é˜²ç¦¦

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional
import numpy as np

class AdversarialDefense:
    def __init__(self, model: nn.Module, epsilon: float = 0.3):
        self.model = model
        self.epsilon = epsilon
    
    def fgsm_attack(self, data: torch.Tensor,
                    target: torch.Tensor) -> torch.Tensor:
        """å¿«é€Ÿæ¢¯åº¦ç¬¦è™Ÿæ”»æ“Š"""
        data.requires_grad = True
        output = self.model(data)
        loss = F.cross_entropy(output, target)
        loss.backward()
        
        # ç”Ÿæˆå°æŠ—æ¨£æœ¬
        perturbed_data = data + self.epsilon * data.grad.sign()
        perturbed_data = torch.clamp(perturbed_data, 0, 1)
        
        return perturbed_data
    
    def adversarial_training(self, data: torch.Tensor,
                           target: torch.Tensor,
                           optimizer: torch.optim.Optimizer):
        """å°æŠ—è¨“ç·´"""
        # ç”Ÿæˆå°æŠ—æ¨£æœ¬
        perturbed_data = self.fgsm_attack(data, target)
        
        # æ­£å¸¸æ¨£æœ¬è¨“ç·´
        optimizer.zero_grad()
        output = self.model(data)
        clean_loss = F.cross_entropy(output, target)
        
        # å°æŠ—æ¨£æœ¬è¨“ç·´
        adv_output = self.model(perturbed_data)
        adv_loss = F.cross_entropy(adv_output, target)
        
        # çµ„åˆæå¤±
        total_loss = 0.5 * (clean_loss + adv_loss)
        total_loss.backward()
        optimizer.step()
        
        return total_loss.item()
```

### 1.2 æ¨¡å‹åŠ å›º

```python
class ModelHardening:
    def __init__(self, model: nn.Module):
        self.model = model
    
    def add_noise_layer(self, std: float = 0.1):
        """æ·»åŠ å™ªè²å±¤"""
        class NoiseLayer(nn.Module):
            def __init__(self, std):
                super().__init__()
                self.std = std
            
            def forward(self, x):
                if self.training:
                    return x + torch.randn_like(x) * self.std
                return x
        
        # åœ¨æ¯å€‹ç·šæ€§å±¤å¾Œæ·»åŠ å™ªè²å±¤
        for name, module in self.model.named_children():
            if isinstance(module, nn.Linear):
                setattr(self.model, name, nn.Sequential(
                    module,
                    NoiseLayer(std)
                ))
    
    def gradient_clipping(self, max_norm: float = 1.0):
        """æ¢¯åº¦è£å‰ª"""
        torch.nn.utils.clip_grad_norm_(
            self.model.parameters(),
            max_norm
        )
```

## 2. éš±ç§ä¿è­·

### 2.1 æ•¸æ“šåŠ å¯†

```python
from cryptography.fernet import Fernet
import base64
import json

class DataEncryption:
    def __init__(self):
        self.key = Fernet.generate_key()
        self.cipher_suite = Fernet(self.key)
    
    def encrypt_tensor(self, tensor: torch.Tensor) -> bytes:
        """åŠ å¯†å¼µé‡æ•¸æ“š"""
        # å°‡å¼µé‡è½‰æ›ç‚ºbytes
        tensor_bytes = tensor.numpy().tobytes()
        
        # åŠ å¯†
        encrypted_data = self.cipher_suite.encrypt(tensor_bytes)
        return encrypted_data
    
    def decrypt_tensor(self, encrypted_data: bytes,
                      shape: Tuple[int, ...],
                      dtype: torch.dtype) -> torch.Tensor:
        """è§£å¯†å¼µé‡æ•¸æ“š"""
        # è§£å¯†
        decrypted_data = self.cipher_suite.decrypt(encrypted_data)
        
        # è½‰æ›å›å¼µé‡
        numpy_array = np.frombuffer(decrypted_data).reshape(shape)
        return torch.from_numpy(numpy_array).to(dtype)
    
    def encrypt_model_weights(self, model: nn.Module) -> Dict[str, bytes]:
        """åŠ å¯†æ¨¡å‹æ¬Šé‡"""
        encrypted_weights = {}
        for name, param in model.state_dict().items():
            encrypted_weights[name] = self.encrypt_tensor(param.data)
        return encrypted_weights
```

### 2.2 åŒæ…‹åŠ å¯†

```python
from tenseal import Context, TenSEALContext
import tenseal as ts

class HomomorphicEncryption:
    def __init__(self):
        # å‰µå»ºåŠ å¯†ä¸Šä¸‹æ–‡
        self.context = ts.context(
            ts.SCHEME_TYPE.CKKS,
            poly_modulus_degree=8192,
            coeff_mod_bit_sizes=[60, 40, 40, 60]
        )
        self.context.global_scale = 2**40
    
    def encrypt_vector(self, vector: List[float]) -> ts.CKKSVector:
        """åŠ å¯†å‘é‡"""
        return ts.ckks_vector(self.context, vector)
    
    def decrypt_vector(self, encrypted_vector: ts.CKKSVector) -> List[float]:
        """è§£å¯†å‘é‡"""
        return encrypted_vector.decrypt()
    
    def secure_dot_product(self, enc_vec1: ts.CKKSVector,
                          enc_vec2: ts.CKKSVector) -> ts.CKKSVector:
        """åŠ å¯†ç‹€æ…‹ä¸‹çš„é»ç©é‹ç®—"""
        return enc_vec1.dot(enc_vec2)
```

## 3. å®‰å…¨è©•ä¼°

### 3.1 æ¼æ´æƒæ

```python
class SecurityScanner:
    def __init__(self, model: nn.Module):
        self.model = model
    
    def check_gradient_masking(self, data: torch.Tensor,
                             target: torch.Tensor) -> bool:
        """æª¢æŸ¥æ¢¯åº¦æ©è”½"""
        data.requires_grad = True
        output = self.model(data)
        loss = F.cross_entropy(output, target)
        loss.backward()
        
        gradients = data.grad.view(data.size(0), -1)
        gradient_norms = torch.norm(gradients, dim=1)
        
        # æª¢æŸ¥æ¢¯åº¦æ˜¯å¦ç•°å¸¸å°
        return bool(torch.any(gradient_norms < 1e-5))
    
    def check_model_robustness(self, data: torch.Tensor,
                              target: torch.Tensor,
                              num_attacks: int = 100) -> float:
        """è©•ä¼°æ¨¡å‹é­¯æ£’æ€§"""
        success_count = 0
        defense = AdversarialDefense(self.model)
        
        for _ in range(num_attacks):
            perturbed_data = defense.fgsm_attack(data.clone(), target)
            output = self.model(perturbed_data)
            pred = output.argmax(dim=1)
            success_count += (pred == target).sum().item()
        
        return success_count / (num_attacks * len(target))
```

### 3.2 å®‰å…¨ç›£æ§

```python
class SecurityMonitor:
    def __init__(self, threshold: float = 0.95):
        self.threshold = threshold
        self.alerts = []
    
    def monitor_predictions(self, predictions: torch.Tensor,
                          confidence: torch.Tensor) -> bool:
        """ç›£æ§é æ¸¬è¡Œç‚º"""
        # æª¢æŸ¥é«˜ç½®ä¿¡åº¦éŒ¯èª¤é æ¸¬
        high_conf_mask = confidence > self.threshold
        if torch.any(high_conf_mask):
            self.alerts.append({
                'type': 'high_confidence_error',
                'timestamp': time.time(),
                'details': {
                    'predictions': predictions[high_conf_mask].tolist(),
                    'confidence': confidence[high_conf_mask].tolist()
                }
            })
            return True
        return False
    
    def monitor_gradients(self, gradients: List[torch.Tensor]) -> bool:
        """ç›£æ§æ¢¯åº¦è¡Œç‚º"""
        for grad in gradients:
            if torch.isnan(grad).any() or torch.isinf(grad).any():
                self.alerts.append({
                    'type': 'gradient_anomaly',
                    'timestamp': time.time(),
                    'details': {
                        'has_nan': torch.isnan(grad).any().item(),
                        'has_inf': torch.isinf(grad).any().item()
                    }
                })
                return True
        return False
```

## ç·´ç¿’é¡Œ ğŸƒ

1. å¯¦ç¾ä¸€å€‹å®Œæ•´çš„å°æŠ—æ¨£æœ¬é˜²ç¦¦ç³»çµ±ã€‚
2. é–‹ç™¼ä¸€å€‹åŸºæ–¼åŒæ…‹åŠ å¯†çš„å®‰å…¨æ¨¡å‹æ¨ç†æœå‹™ã€‚
3. è¨­è¨ˆä¸€å€‹æ¨¡å‹å®‰å…¨è©•ä¼°æ¡†æ¶ã€‚
4. å¯¦ç¾ä¸€å€‹éš±ç§ä¿è­·çš„æ•¸æ“šé è™•ç†ç®¡é“ã€‚
5. å‰µå»ºä¸€å€‹å¯¦æ™‚å®‰å…¨ç›£æ§ç³»çµ±ã€‚

## å°çµ ğŸ“

- å­¸ç¿’äº†å°æŠ—æ¨£æœ¬é˜²ç¦¦æŠ€è¡“
- æŒæ¡äº†æ¨¡å‹åŠ å›ºæ–¹æ³•
- ç†è§£äº†æ•¸æ“šåŠ å¯†åŸç†
- å­¸æœƒäº†åŒæ…‹åŠ å¯†æ‡‰ç”¨
- äº†è§£äº†å®‰å…¨è©•ä¼°å’Œç›£æ§

## å»¶ä¼¸é–±è®€ ğŸ“š

1. Adversarial Machine Learning
2. Privacy-Preserving Deep Learning
3. Homomorphic Encryption for ML
4. AI Security Best Practices
5. Model Protection Techniques

[ä¸Šä¸€ç« ï¼šè¯é‚¦å­¸ç¿’åŸºç¤](124_è¯é‚¦å­¸ç¿’åŸºç¤.md) | [ä¸‹ä¸€ç« ï¼šå¾®æœå‹™æ¶æ§‹è¨­è¨ˆ](126_å¾®æœå‹™æ¶æ§‹è¨­è¨ˆ.md) 