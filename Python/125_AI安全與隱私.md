[上一章：聯邦學習基礎](124_聯邦學習基礎.md) | [下一章：微服務架構設計](126_微服務架構設計.md)

# Python AI安全與隱私 🔒

## 1. 模型安全

### 1.1 對抗樣本防禦

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional
import numpy as np

class AdversarialDefense:
    def __init__(self, model: nn.Module, epsilon: float = 0.3):
        self.model = model
        self.epsilon = epsilon
    
    def fgsm_attack(self, data: torch.Tensor,
                    target: torch.Tensor) -> torch.Tensor:
        """快速梯度符號攻擊"""
        data.requires_grad = True
        output = self.model(data)
        loss = F.cross_entropy(output, target)
        loss.backward()
        
        # 生成對抗樣本
        perturbed_data = data + self.epsilon * data.grad.sign()
        perturbed_data = torch.clamp(perturbed_data, 0, 1)
        
        return perturbed_data
    
    def adversarial_training(self, data: torch.Tensor,
                           target: torch.Tensor,
                           optimizer: torch.optim.Optimizer):
        """對抗訓練"""
        # 生成對抗樣本
        perturbed_data = self.fgsm_attack(data, target)
        
        # 正常樣本訓練
        optimizer.zero_grad()
        output = self.model(data)
        clean_loss = F.cross_entropy(output, target)
        
        # 對抗樣本訓練
        adv_output = self.model(perturbed_data)
        adv_loss = F.cross_entropy(adv_output, target)
        
        # 組合損失
        total_loss = 0.5 * (clean_loss + adv_loss)
        total_loss.backward()
        optimizer.step()
        
        return total_loss.item()
```

### 1.2 模型加固

```python
class ModelHardening:
    def __init__(self, model: nn.Module):
        self.model = model
    
    def add_noise_layer(self, std: float = 0.1):
        """添加噪聲層"""
        class NoiseLayer(nn.Module):
            def __init__(self, std):
                super().__init__()
                self.std = std
            
            def forward(self, x):
                if self.training:
                    return x + torch.randn_like(x) * self.std
                return x
        
        # 在每個線性層後添加噪聲層
        for name, module in self.model.named_children():
            if isinstance(module, nn.Linear):
                setattr(self.model, name, nn.Sequential(
                    module,
                    NoiseLayer(std)
                ))
    
    def gradient_clipping(self, max_norm: float = 1.0):
        """梯度裁剪"""
        torch.nn.utils.clip_grad_norm_(
            self.model.parameters(),
            max_norm
        )
```

## 2. 隱私保護

### 2.1 數據加密

```python
from cryptography.fernet import Fernet
import base64
import json

class DataEncryption:
    def __init__(self):
        self.key = Fernet.generate_key()
        self.cipher_suite = Fernet(self.key)
    
    def encrypt_tensor(self, tensor: torch.Tensor) -> bytes:
        """加密張量數據"""
        # 將張量轉換為bytes
        tensor_bytes = tensor.numpy().tobytes()
        
        # 加密
        encrypted_data = self.cipher_suite.encrypt(tensor_bytes)
        return encrypted_data
    
    def decrypt_tensor(self, encrypted_data: bytes,
                      shape: Tuple[int, ...],
                      dtype: torch.dtype) -> torch.Tensor:
        """解密張量數據"""
        # 解密
        decrypted_data = self.cipher_suite.decrypt(encrypted_data)
        
        # 轉換回張量
        numpy_array = np.frombuffer(decrypted_data).reshape(shape)
        return torch.from_numpy(numpy_array).to(dtype)
    
    def encrypt_model_weights(self, model: nn.Module) -> Dict[str, bytes]:
        """加密模型權重"""
        encrypted_weights = {}
        for name, param in model.state_dict().items():
            encrypted_weights[name] = self.encrypt_tensor(param.data)
        return encrypted_weights
```

### 2.2 同態加密

```python
from tenseal import Context, TenSEALContext
import tenseal as ts

class HomomorphicEncryption:
    def __init__(self):
        # 創建加密上下文
        self.context = ts.context(
            ts.SCHEME_TYPE.CKKS,
            poly_modulus_degree=8192,
            coeff_mod_bit_sizes=[60, 40, 40, 60]
        )
        self.context.global_scale = 2**40
    
    def encrypt_vector(self, vector: List[float]) -> ts.CKKSVector:
        """加密向量"""
        return ts.ckks_vector(self.context, vector)
    
    def decrypt_vector(self, encrypted_vector: ts.CKKSVector) -> List[float]:
        """解密向量"""
        return encrypted_vector.decrypt()
    
    def secure_dot_product(self, enc_vec1: ts.CKKSVector,
                          enc_vec2: ts.CKKSVector) -> ts.CKKSVector:
        """加密狀態下的點積運算"""
        return enc_vec1.dot(enc_vec2)
```

## 3. 安全評估

### 3.1 漏洞掃描

```python
class SecurityScanner:
    def __init__(self, model: nn.Module):
        self.model = model
    
    def check_gradient_masking(self, data: torch.Tensor,
                             target: torch.Tensor) -> bool:
        """檢查梯度掩蔽"""
        data.requires_grad = True
        output = self.model(data)
        loss = F.cross_entropy(output, target)
        loss.backward()
        
        gradients = data.grad.view(data.size(0), -1)
        gradient_norms = torch.norm(gradients, dim=1)
        
        # 檢查梯度是否異常小
        return bool(torch.any(gradient_norms < 1e-5))
    
    def check_model_robustness(self, data: torch.Tensor,
                              target: torch.Tensor,
                              num_attacks: int = 100) -> float:
        """評估模型魯棒性"""
        success_count = 0
        defense = AdversarialDefense(self.model)
        
        for _ in range(num_attacks):
            perturbed_data = defense.fgsm_attack(data.clone(), target)
            output = self.model(perturbed_data)
            pred = output.argmax(dim=1)
            success_count += (pred == target).sum().item()
        
        return success_count / (num_attacks * len(target))
```

### 3.2 安全監控

```python
class SecurityMonitor:
    def __init__(self, threshold: float = 0.95):
        self.threshold = threshold
        self.alerts = []
    
    def monitor_predictions(self, predictions: torch.Tensor,
                          confidence: torch.Tensor) -> bool:
        """監控預測行為"""
        # 檢查高置信度錯誤預測
        high_conf_mask = confidence > self.threshold
        if torch.any(high_conf_mask):
            self.alerts.append({
                'type': 'high_confidence_error',
                'timestamp': time.time(),
                'details': {
                    'predictions': predictions[high_conf_mask].tolist(),
                    'confidence': confidence[high_conf_mask].tolist()
                }
            })
            return True
        return False
    
    def monitor_gradients(self, gradients: List[torch.Tensor]) -> bool:
        """監控梯度行為"""
        for grad in gradients:
            if torch.isnan(grad).any() or torch.isinf(grad).any():
                self.alerts.append({
                    'type': 'gradient_anomaly',
                    'timestamp': time.time(),
                    'details': {
                        'has_nan': torch.isnan(grad).any().item(),
                        'has_inf': torch.isinf(grad).any().item()
                    }
                })
                return True
        return False
```

## 練習題 🏃

1. 實現一個完整的對抗樣本防禦系統。
2. 開發一個基於同態加密的安全模型推理服務。
3. 設計一個模型安全評估框架。
4. 實現一個隱私保護的數據預處理管道。
5. 創建一個實時安全監控系統。

## 小結 📝

- 學習了對抗樣本防禦技術
- 掌握了模型加固方法
- 理解了數據加密原理
- 學會了同態加密應用
- 了解了安全評估和監控

## 延伸閱讀 📚

1. Adversarial Machine Learning
2. Privacy-Preserving Deep Learning
3. Homomorphic Encryption for ML
4. AI Security Best Practices
5. Model Protection Techniques

[上一章：聯邦學習基礎](124_聯邦學習基礎.md) | [下一章：微服務架構設計](126_微服務架構設計.md) 