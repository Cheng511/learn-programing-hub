[上一章：機器學習進階](097_機器學習進階.md) | [下一章：深度學習進階](099_深度學習進階.md)

# Python 深度學習基礎 🧠

## 神經網絡基礎

### 1. 前饋神經網絡

```python
import numpy as np
from typing import List, Tuple, Optional
import time
import sys
import os

class FeedForwardNeuralNetwork:
    def __init__(self, layer_sizes: List[int]):
        """初始化前饋神經網絡"""
        self.layer_sizes = layer_sizes
        self.weights = []
        self.biases = []
        self.activations = []
        self.initialize_parameters()
    
    def initialize_parameters(self):
        """初始化參數"""
        try:
            for i in range(len(self.layer_sizes) - 1):
                # 初始化權重
                w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * 0.01
                self.weights.append(w)
                
                # 初始化偏置
                b = np.zeros((1, self.layer_sizes[i + 1]))
                self.biases.append(b)
            
            print("Parameters initialized")
            
        except Exception as e:
            print(f"Error initializing parameters: {e}")
    
    def sigmoid(self, z: np.ndarray) -> np.ndarray:
        """sigmoid激活函數"""
        return 1 / (1 + np.exp(-z))
    
    def sigmoid_derivative(self, z: np.ndarray) -> np.ndarray:
        """sigmoid導數"""
        return z * (1 - z)
    
    def forward_propagation(self, X: np.ndarray) -> Tuple[List[np.ndarray], List[np.ndarray]]:
        """前向傳播"""
        try:
            activations = [X]
            z_values = []
            
            for i in range(len(self.weights)):
                # 計算線性組合
                z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
                z_values.append(z)
                
                # 應用激活函數
                a = self.sigmoid(z)
                activations.append(a)
            
            return activations, z_values
            
        except Exception as e:
            print(f"Error in forward propagation: {e}")
            return [], []
    
    def backward_propagation(self, X: np.ndarray, y: np.ndarray, activations: List[np.ndarray], z_values: List[np.ndarray]) -> Tuple[List[np.ndarray], List[np.ndarray]]:
        """反向傳播"""
        try:
            m = X.shape[0]
            delta_weights = []
            delta_biases = []
            
            # 計算輸出層誤差
            delta = activations[-1] - y
            
            for i in range(len(self.weights) - 1, -1, -1):
                # 計算權重梯度
                dw = np.dot(activations[i].T, delta)
                delta_weights.insert(0, dw)
                
                # 計算偏置梯度
                db = np.sum(delta, axis=0, keepdims=True)
                delta_biases.insert(0, db)
                
                if i > 0:
                    # 計算隱藏層誤差
                    delta = np.dot(delta, self.weights[i].T) * self.sigmoid_derivative(activations[i])
            
            return delta_weights, delta_biases
            
        except Exception as e:
            print(f"Error in backward propagation: {e}")
            return [], []
    
    def update_parameters(self, delta_weights: List[np.ndarray], delta_biases: List[np.ndarray], learning_rate: float):
        """更新參數"""
        try:
            for i in range(len(self.weights)):
                self.weights[i] -= learning_rate * delta_weights[i]
                self.biases[i] -= learning_rate * delta_biases[i]
            
            print("Parameters updated")
            
        except Exception as e:
            print(f"Error updating parameters: {e}")
    
    def train(self, X: np.ndarray, y: np.ndarray, epochs: int, learning_rate: float):
        """訓練模型"""
        try:
            for epoch in range(epochs):
                # 前向傳播
                activations, z_values = self.forward_propagation(X)
                
                # 反向傳播
                delta_weights, delta_biases = self.backward_propagation(X, y, activations, z_values)
                
                # 更新參數
                self.update_parameters(delta_weights, delta_biases, learning_rate)
                
                if (epoch + 1) % 100 == 0:
                    loss = np.mean((activations[-1] - y) ** 2)
                    print(f"Epoch {epoch + 1}, Loss: {loss:.4f}")
            
            print("Training completed")
            
        except Exception as e:
            print(f"Error in training: {e}")
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """預測"""
        try:
            activations, _ = self.forward_propagation(X)
            return activations[-1]
            
        except Exception as e:
            print(f"Error in prediction: {e}")
            return np.array([])

# 使用示例
def main():
    # 創建示例數據
    X = np.random.rand(100, 2)  # 100個樣本，2個特徵
    y = np.random.randint(0, 2, (100, 1))  # 二分類問題
    
    try:
        # 創建神經網絡
        nn = FeedForwardNeuralNetwork([2, 4, 1])
        
        # 訓練模型
        nn.train(X, y, epochs=1000, learning_rate=0.01)
        
        # 預測
        predictions = nn.predict(X)
        print(f"Predictions shape: {predictions.shape}")
    
    except Exception as e:
        print(f"Error in main: {e}")

if __name__ == '__main__':
    main()
```

### 2. 卷積神經網絡

```python
import numpy as np
from typing import List, Tuple, Optional
import time
import sys
import os

class ConvolutionalNeuralNetwork:
    def __init__(self, input_shape: Tuple[int, int, int], num_classes: int):
        """初始化卷積神經網絡"""
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.weights = []
        self.biases = []
        self.initialize_parameters()
    
    def initialize_parameters(self):
        """初始化參數"""
        try:
            # 第一個卷積層
            w1 = np.random.randn(3, 3, self.input_shape[2], 32) * 0.01
            b1 = np.zeros((1, 1, 1, 32))
            self.weights.append(w1)
            self.biases.append(b1)
            
            # 第二個卷積層
            w2 = np.random.randn(3, 3, 32, 64) * 0.01
            b2 = np.zeros((1, 1, 1, 64))
            self.weights.append(w2)
            self.biases.append(b2)
            
            # 全連接層
            w3 = np.random.randn(64 * 7 * 7, self.num_classes) * 0.01
            b3 = np.zeros((1, self.num_classes))
            self.weights.append(w3)
            self.biases.append(b3)
            
            print("Parameters initialized")
            
        except Exception as e:
            print(f"Error initializing parameters: {e}")
    
    def conv2d(self, X: np.ndarray, W: np.ndarray, b: np.ndarray, stride: int = 1, padding: int = 0) -> np.ndarray:
        """2D卷積操作"""
        try:
            m, n_H, n_W, n_C = X.shape
            f, f, n_C_prev, n_C = W.shape
            
            # 計算輸出維度
            n_H_out = int((n_H + 2 * padding - f) / stride) + 1
            n_W_out = int((n_W + 2 * padding - f) / stride) + 1
            
            # 初始化輸出
            Z = np.zeros((m, n_H_out, n_W_out, n_C))
            
            # 執行卷積
            for i in range(m):
                for h in range(n_H_out):
                    for w in range(n_W_out):
                        for c in range(n_C):
                            vert_start = h * stride
                            vert_end = vert_start + f
                            horiz_start = w * stride
                            horiz_end = horiz_start + f
                            
                            X_slice = X[i, vert_start:vert_end, horiz_start:horiz_end, :]
                            Z[i, h, w, c] = np.sum(X_slice * W[:, :, :, c]) + b[0, 0, 0, c]
            
            return Z
            
        except Exception as e:
            print(f"Error in convolution: {e}")
            return np.array([])
    
    def max_pool2d(self, X: np.ndarray, f: int = 2, stride: int = 2) -> np.ndarray:
        """2D最大池化"""
        try:
            m, n_H, n_W, n_C = X.shape
            
            # 計算輸出維度
            n_H_out = int((n_H - f) / stride) + 1
            n_W_out = int((n_W - f) / stride) + 1
            
            # 初始化輸出
            Z = np.zeros((m, n_H_out, n_W_out, n_C))
            
            # 執行池化
            for i in range(m):
                for h in range(n_H_out):
                    for w in range(n_W_out):
                        for c in range(n_C):
                            vert_start = h * stride
                            vert_end = vert_start + f
                            horiz_start = w * stride
                            horiz_end = horiz_start + f
                            
                            X_slice = X[i, vert_start:vert_end, horiz_start:horiz_end, c]
                            Z[i, h, w, c] = np.max(X_slice)
            
            return Z
            
        except Exception as e:
            print(f"Error in max pooling: {e}")
            return np.array([])
    
    def relu(self, Z: np.ndarray) -> np.ndarray:
        """ReLU激活函數"""
        return np.maximum(0, Z)
    
    def softmax(self, Z: np.ndarray) -> np.ndarray:
        """Softmax激活函數"""
        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))
        return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)
    
    def forward_propagation(self, X: np.ndarray) -> List[np.ndarray]:
        """前向傳播"""
        try:
            activations = [X]
            
            # 第一個卷積層
            Z1 = self.conv2d(X, self.weights[0], self.biases[0])
            A1 = self.relu(Z1)
            P1 = self.max_pool2d(A1)
            activations.append(P1)
            
            # 第二個卷積層
            Z2 = self.conv2d(P1, self.weights[1], self.biases[1])
            A2 = self.relu(Z2)
            P2 = self.max_pool2d(A2)
            activations.append(P2)
            
            # 展平
            F = P2.reshape(P2.shape[0], -1)
            
            # 全連接層
            Z3 = np.dot(F, self.weights[2]) + self.biases[2]
            A3 = self.softmax(Z3)
            activations.append(A3)
            
            return activations
            
        except Exception as e:
            print(f"Error in forward propagation: {e}")
            return []
    
    def compute_loss(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:
        """計算損失"""
        try:
            m = y_true.shape[0]
            loss = -np.sum(y_true * np.log(y_pred + 1e-15)) / m
            return loss
            
        except Exception as e:
            print(f"Error computing loss: {e}")
            return 0.0
    
    def train(self, X: np.ndarray, y: np.ndarray, epochs: int, learning_rate: float):
        """訓練模型"""
        try:
            for epoch in range(epochs):
                # 前向傳播
                activations = self.forward_propagation(X)
                
                # 計算損失
                loss = self.compute_loss(activations[-1], y)
                
                if (epoch + 1) % 10 == 0:
                    print(f"Epoch {epoch + 1}, Loss: {loss:.4f}")
            
            print("Training completed")
            
        except Exception as e:
            print(f"Error in training: {e}")
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """預測"""
        try:
            activations = self.forward_propagation(X)
            return activations[-1]
            
        except Exception as e:
            print(f"Error in prediction: {e}")
            return np.array([])

# 使用示例
def main():
    # 創建示例數據
    X = np.random.rand(32, 28, 28, 3)  # 32個樣本，28x28像素，3個通道
    y = np.random.randint(0, 10, (32, 10))  # 10個類別
    
    try:
        # 創建CNN
        cnn = ConvolutionalNeuralNetwork((28, 28, 3), 10)
        
        # 訓練模型
        cnn.train(X, y, epochs=100, learning_rate=0.01)
        
        # 預測
        predictions = cnn.predict(X)
        print(f"Predictions shape: {predictions.shape}")
    
    except Exception as e:
        print(f"Error in main: {e}")

if __name__ == '__main__':
    main()
```

## 練習題

1. **前饋神經網絡**
   開發前饋神經網絡：
   - 初始化參數
   - 前向傳播
   - 反向傳播
   - 優化性能

2. **卷積神經網絡**
   創建卷積神經網絡：
   - 卷積操作
   - 池化操作
   - 前向傳播
   - 優化性能

3. **深度學習**
   實現深度學習：
   - 處理數據
   - 訓練模型
   - 優化性能
   - 處理異常

## 小提醒 💡

1. 前饋神經網絡
   - 選擇合適架構
   - 優化參數
   - 處理異常
   - 提供監控

2. 卷積神經網絡
   - 選擇合適架構
   - 優化性能
   - 處理異常
   - 提供結果

3. 深度學習
   - 選擇合適算法
   - 優化性能
   - 處理異常
   - 提供監控

4. 調試技巧
   - 使用開發工具
   - 分析性能
   - 優化關鍵路徑
   - 監控訓練狀態

[上一章：機器學習進階](097_機器學習進階.md) | [下一章：深度學習進階](099_深度學習進階.md) 