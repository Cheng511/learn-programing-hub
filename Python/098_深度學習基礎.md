[ä¸Šä¸€ç« ï¼šæ©Ÿå™¨å­¸ç¿’é€²éš](097_æ©Ÿå™¨å­¸ç¿’é€²éš.md) | [ä¸‹ä¸€ç« ï¼šæ·±åº¦å­¸ç¿’é€²éš](099_æ·±åº¦å­¸ç¿’é€²éš.md)

# Python æ·±åº¦å­¸ç¿’åŸºç¤ ğŸ§ 

## ç¥ç¶“ç¶²çµ¡åŸºç¤

### 1. å‰é¥‹ç¥ç¶“ç¶²çµ¡

```python
import numpy as np
from typing import List, Tuple, Optional
import time
import sys
import os

class FeedForwardNeuralNetwork:
    def __init__(self, layer_sizes: List[int]):
        """åˆå§‹åŒ–å‰é¥‹ç¥ç¶“ç¶²çµ¡"""
        self.layer_sizes = layer_sizes
        self.weights = []
        self.biases = []
        self.activations = []
        self.initialize_parameters()
    
    def initialize_parameters(self):
        """åˆå§‹åŒ–åƒæ•¸"""
        try:
            for i in range(len(self.layer_sizes) - 1):
                # åˆå§‹åŒ–æ¬Šé‡
                w = np.random.randn(self.layer_sizes[i], self.layer_sizes[i + 1]) * 0.01
                self.weights.append(w)
                
                # åˆå§‹åŒ–åç½®
                b = np.zeros((1, self.layer_sizes[i + 1]))
                self.biases.append(b)
            
            print("Parameters initialized")
            
        except Exception as e:
            print(f"Error initializing parameters: {e}")
    
    def sigmoid(self, z: np.ndarray) -> np.ndarray:
        """sigmoidæ¿€æ´»å‡½æ•¸"""
        return 1 / (1 + np.exp(-z))
    
    def sigmoid_derivative(self, z: np.ndarray) -> np.ndarray:
        """sigmoidå°æ•¸"""
        return z * (1 - z)
    
    def forward_propagation(self, X: np.ndarray) -> Tuple[List[np.ndarray], List[np.ndarray]]:
        """å‰å‘å‚³æ’­"""
        try:
            activations = [X]
            z_values = []
            
            for i in range(len(self.weights)):
                # è¨ˆç®—ç·šæ€§çµ„åˆ
                z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
                z_values.append(z)
                
                # æ‡‰ç”¨æ¿€æ´»å‡½æ•¸
                a = self.sigmoid(z)
                activations.append(a)
            
            return activations, z_values
            
        except Exception as e:
            print(f"Error in forward propagation: {e}")
            return [], []
    
    def backward_propagation(self, X: np.ndarray, y: np.ndarray, activations: List[np.ndarray], z_values: List[np.ndarray]) -> Tuple[List[np.ndarray], List[np.ndarray]]:
        """åå‘å‚³æ’­"""
        try:
            m = X.shape[0]
            delta_weights = []
            delta_biases = []
            
            # è¨ˆç®—è¼¸å‡ºå±¤èª¤å·®
            delta = activations[-1] - y
            
            for i in range(len(self.weights) - 1, -1, -1):
                # è¨ˆç®—æ¬Šé‡æ¢¯åº¦
                dw = np.dot(activations[i].T, delta)
                delta_weights.insert(0, dw)
                
                # è¨ˆç®—åç½®æ¢¯åº¦
                db = np.sum(delta, axis=0, keepdims=True)
                delta_biases.insert(0, db)
                
                if i > 0:
                    # è¨ˆç®—éš±è—å±¤èª¤å·®
                    delta = np.dot(delta, self.weights[i].T) * self.sigmoid_derivative(activations[i])
            
            return delta_weights, delta_biases
            
        except Exception as e:
            print(f"Error in backward propagation: {e}")
            return [], []
    
    def update_parameters(self, delta_weights: List[np.ndarray], delta_biases: List[np.ndarray], learning_rate: float):
        """æ›´æ–°åƒæ•¸"""
        try:
            for i in range(len(self.weights)):
                self.weights[i] -= learning_rate * delta_weights[i]
                self.biases[i] -= learning_rate * delta_biases[i]
            
            print("Parameters updated")
            
        except Exception as e:
            print(f"Error updating parameters: {e}")
    
    def train(self, X: np.ndarray, y: np.ndarray, epochs: int, learning_rate: float):
        """è¨“ç·´æ¨¡å‹"""
        try:
            for epoch in range(epochs):
                # å‰å‘å‚³æ’­
                activations, z_values = self.forward_propagation(X)
                
                # åå‘å‚³æ’­
                delta_weights, delta_biases = self.backward_propagation(X, y, activations, z_values)
                
                # æ›´æ–°åƒæ•¸
                self.update_parameters(delta_weights, delta_biases, learning_rate)
                
                if (epoch + 1) % 100 == 0:
                    loss = np.mean((activations[-1] - y) ** 2)
                    print(f"Epoch {epoch + 1}, Loss: {loss:.4f}")
            
            print("Training completed")
            
        except Exception as e:
            print(f"Error in training: {e}")
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é æ¸¬"""
        try:
            activations, _ = self.forward_propagation(X)
            return activations[-1]
            
        except Exception as e:
            print(f"Error in prediction: {e}")
            return np.array([])

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # å‰µå»ºç¤ºä¾‹æ•¸æ“š
    X = np.random.rand(100, 2)  # 100å€‹æ¨£æœ¬ï¼Œ2å€‹ç‰¹å¾µ
    y = np.random.randint(0, 2, (100, 1))  # äºŒåˆ†é¡å•é¡Œ
    
    try:
        # å‰µå»ºç¥ç¶“ç¶²çµ¡
        nn = FeedForwardNeuralNetwork([2, 4, 1])
        
        # è¨“ç·´æ¨¡å‹
        nn.train(X, y, epochs=1000, learning_rate=0.01)
        
        # é æ¸¬
        predictions = nn.predict(X)
        print(f"Predictions shape: {predictions.shape}")
    
    except Exception as e:
        print(f"Error in main: {e}")

if __name__ == '__main__':
    main()
```

### 2. å·ç©ç¥ç¶“ç¶²çµ¡

```python
import numpy as np
from typing import List, Tuple, Optional
import time
import sys
import os

class ConvolutionalNeuralNetwork:
    def __init__(self, input_shape: Tuple[int, int, int], num_classes: int):
        """åˆå§‹åŒ–å·ç©ç¥ç¶“ç¶²çµ¡"""
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.weights = []
        self.biases = []
        self.initialize_parameters()
    
    def initialize_parameters(self):
        """åˆå§‹åŒ–åƒæ•¸"""
        try:
            # ç¬¬ä¸€å€‹å·ç©å±¤
            w1 = np.random.randn(3, 3, self.input_shape[2], 32) * 0.01
            b1 = np.zeros((1, 1, 1, 32))
            self.weights.append(w1)
            self.biases.append(b1)
            
            # ç¬¬äºŒå€‹å·ç©å±¤
            w2 = np.random.randn(3, 3, 32, 64) * 0.01
            b2 = np.zeros((1, 1, 1, 64))
            self.weights.append(w2)
            self.biases.append(b2)
            
            # å…¨é€£æ¥å±¤
            w3 = np.random.randn(64 * 7 * 7, self.num_classes) * 0.01
            b3 = np.zeros((1, self.num_classes))
            self.weights.append(w3)
            self.biases.append(b3)
            
            print("Parameters initialized")
            
        except Exception as e:
            print(f"Error initializing parameters: {e}")
    
    def conv2d(self, X: np.ndarray, W: np.ndarray, b: np.ndarray, stride: int = 1, padding: int = 0) -> np.ndarray:
        """2Då·ç©æ“ä½œ"""
        try:
            m, n_H, n_W, n_C = X.shape
            f, f, n_C_prev, n_C = W.shape
            
            # è¨ˆç®—è¼¸å‡ºç¶­åº¦
            n_H_out = int((n_H + 2 * padding - f) / stride) + 1
            n_W_out = int((n_W + 2 * padding - f) / stride) + 1
            
            # åˆå§‹åŒ–è¼¸å‡º
            Z = np.zeros((m, n_H_out, n_W_out, n_C))
            
            # åŸ·è¡Œå·ç©
            for i in range(m):
                for h in range(n_H_out):
                    for w in range(n_W_out):
                        for c in range(n_C):
                            vert_start = h * stride
                            vert_end = vert_start + f
                            horiz_start = w * stride
                            horiz_end = horiz_start + f
                            
                            X_slice = X[i, vert_start:vert_end, horiz_start:horiz_end, :]
                            Z[i, h, w, c] = np.sum(X_slice * W[:, :, :, c]) + b[0, 0, 0, c]
            
            return Z
            
        except Exception as e:
            print(f"Error in convolution: {e}")
            return np.array([])
    
    def max_pool2d(self, X: np.ndarray, f: int = 2, stride: int = 2) -> np.ndarray:
        """2Dæœ€å¤§æ± åŒ–"""
        try:
            m, n_H, n_W, n_C = X.shape
            
            # è¨ˆç®—è¼¸å‡ºç¶­åº¦
            n_H_out = int((n_H - f) / stride) + 1
            n_W_out = int((n_W - f) / stride) + 1
            
            # åˆå§‹åŒ–è¼¸å‡º
            Z = np.zeros((m, n_H_out, n_W_out, n_C))
            
            # åŸ·è¡Œæ± åŒ–
            for i in range(m):
                for h in range(n_H_out):
                    for w in range(n_W_out):
                        for c in range(n_C):
                            vert_start = h * stride
                            vert_end = vert_start + f
                            horiz_start = w * stride
                            horiz_end = horiz_start + f
                            
                            X_slice = X[i, vert_start:vert_end, horiz_start:horiz_end, c]
                            Z[i, h, w, c] = np.max(X_slice)
            
            return Z
            
        except Exception as e:
            print(f"Error in max pooling: {e}")
            return np.array([])
    
    def relu(self, Z: np.ndarray) -> np.ndarray:
        """ReLUæ¿€æ´»å‡½æ•¸"""
        return np.maximum(0, Z)
    
    def softmax(self, Z: np.ndarray) -> np.ndarray:
        """Softmaxæ¿€æ´»å‡½æ•¸"""
        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))
        return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)
    
    def forward_propagation(self, X: np.ndarray) -> List[np.ndarray]:
        """å‰å‘å‚³æ’­"""
        try:
            activations = [X]
            
            # ç¬¬ä¸€å€‹å·ç©å±¤
            Z1 = self.conv2d(X, self.weights[0], self.biases[0])
            A1 = self.relu(Z1)
            P1 = self.max_pool2d(A1)
            activations.append(P1)
            
            # ç¬¬äºŒå€‹å·ç©å±¤
            Z2 = self.conv2d(P1, self.weights[1], self.biases[1])
            A2 = self.relu(Z2)
            P2 = self.max_pool2d(A2)
            activations.append(P2)
            
            # å±•å¹³
            F = P2.reshape(P2.shape[0], -1)
            
            # å…¨é€£æ¥å±¤
            Z3 = np.dot(F, self.weights[2]) + self.biases[2]
            A3 = self.softmax(Z3)
            activations.append(A3)
            
            return activations
            
        except Exception as e:
            print(f"Error in forward propagation: {e}")
            return []
    
    def compute_loss(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:
        """è¨ˆç®—æå¤±"""
        try:
            m = y_true.shape[0]
            loss = -np.sum(y_true * np.log(y_pred + 1e-15)) / m
            return loss
            
        except Exception as e:
            print(f"Error computing loss: {e}")
            return 0.0
    
    def train(self, X: np.ndarray, y: np.ndarray, epochs: int, learning_rate: float):
        """è¨“ç·´æ¨¡å‹"""
        try:
            for epoch in range(epochs):
                # å‰å‘å‚³æ’­
                activations = self.forward_propagation(X)
                
                # è¨ˆç®—æå¤±
                loss = self.compute_loss(activations[-1], y)
                
                if (epoch + 1) % 10 == 0:
                    print(f"Epoch {epoch + 1}, Loss: {loss:.4f}")
            
            print("Training completed")
            
        except Exception as e:
            print(f"Error in training: {e}")
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é æ¸¬"""
        try:
            activations = self.forward_propagation(X)
            return activations[-1]
            
        except Exception as e:
            print(f"Error in prediction: {e}")
            return np.array([])

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # å‰µå»ºç¤ºä¾‹æ•¸æ“š
    X = np.random.rand(32, 28, 28, 3)  # 32å€‹æ¨£æœ¬ï¼Œ28x28åƒç´ ï¼Œ3å€‹é€šé“
    y = np.random.randint(0, 10, (32, 10))  # 10å€‹é¡åˆ¥
    
    try:
        # å‰µå»ºCNN
        cnn = ConvolutionalNeuralNetwork((28, 28, 3), 10)
        
        # è¨“ç·´æ¨¡å‹
        cnn.train(X, y, epochs=100, learning_rate=0.01)
        
        # é æ¸¬
        predictions = cnn.predict(X)
        print(f"Predictions shape: {predictions.shape}")
    
    except Exception as e:
        print(f"Error in main: {e}")

if __name__ == '__main__':
    main()
```

## ç·´ç¿’é¡Œ

1. **å‰é¥‹ç¥ç¶“ç¶²çµ¡**
   é–‹ç™¼å‰é¥‹ç¥ç¶“ç¶²çµ¡ï¼š
   - åˆå§‹åŒ–åƒæ•¸
   - å‰å‘å‚³æ’­
   - åå‘å‚³æ’­
   - å„ªåŒ–æ€§èƒ½

2. **å·ç©ç¥ç¶“ç¶²çµ¡**
   å‰µå»ºå·ç©ç¥ç¶“ç¶²çµ¡ï¼š
   - å·ç©æ“ä½œ
   - æ± åŒ–æ“ä½œ
   - å‰å‘å‚³æ’­
   - å„ªåŒ–æ€§èƒ½

3. **æ·±åº¦å­¸ç¿’**
   å¯¦ç¾æ·±åº¦å­¸ç¿’ï¼š
   - è™•ç†æ•¸æ“š
   - è¨“ç·´æ¨¡å‹
   - å„ªåŒ–æ€§èƒ½
   - è™•ç†ç•°å¸¸

## å°æé†’ ğŸ’¡

1. å‰é¥‹ç¥ç¶“ç¶²çµ¡
   - é¸æ“‡åˆé©æ¶æ§‹
   - å„ªåŒ–åƒæ•¸
   - è™•ç†ç•°å¸¸
   - æä¾›ç›£æ§

2. å·ç©ç¥ç¶“ç¶²çµ¡
   - é¸æ“‡åˆé©æ¶æ§‹
   - å„ªåŒ–æ€§èƒ½
   - è™•ç†ç•°å¸¸
   - æä¾›çµæœ

3. æ·±åº¦å­¸ç¿’
   - é¸æ“‡åˆé©ç®—æ³•
   - å„ªåŒ–æ€§èƒ½
   - è™•ç†ç•°å¸¸
   - æä¾›ç›£æ§

4. èª¿è©¦æŠ€å·§
   - ä½¿ç”¨é–‹ç™¼å·¥å…·
   - åˆ†ææ€§èƒ½
   - å„ªåŒ–é—œéµè·¯å¾‘
   - ç›£æ§è¨“ç·´ç‹€æ…‹

[ä¸Šä¸€ç« ï¼šæ©Ÿå™¨å­¸ç¿’é€²éš](097_æ©Ÿå™¨å­¸ç¿’é€²éš.md) | [ä¸‹ä¸€ç« ï¼šæ·±åº¦å­¸ç¿’é€²éš](099_æ·±åº¦å­¸ç¿’é€²éš.md) 