[ä¸Šä¸€ç« ï¼šæ©Ÿå™¨å­¸ç¿’éƒ¨ç½²](118_æ©Ÿå™¨å­¸ç¿’éƒ¨ç½².md) | [ä¸‹ä¸€ç« ï¼šç«¯åˆ°ç«¯AIæ‡‰ç”¨é–‹ç™¼](120_ç«¯åˆ°ç«¯AIæ‡‰ç”¨é–‹ç™¼.md)

# Python AIæ¨¡å‹å„ªåŒ– ğŸ¯

## 1. æ¨¡å‹æ€§èƒ½å„ªåŒ–

### 1.1 æ¨¡å‹å£“ç¸®

```python
import torch
import torch.nn as nn
from typing import Dict, List
import numpy as np

class ModelCompressor:
    def __init__(self, model: nn.Module):
        self.model = model
        
    def quantize_weights(self, bits: int = 8):
        """é‡åŒ–æ¨¡å‹æ¬Šé‡"""
        state_dict = self.model.state_dict()
        quantized_dict = {}
        
        for name, param in state_dict.items():
            if param.dtype == torch.float32:
                # è¨ˆç®—ç¸®æ”¾å› å­
                max_val = torch.max(torch.abs(param))
                scale = (2 ** (bits - 1) - 1) / max_val
                
                # é‡åŒ–
                quantized = torch.round(param * scale)
                quantized_dict[name] = quantized / scale
        
        self.model.load_state_dict(quantized_dict)
        return self.model
    
    def prune_weights(self, threshold: float = 0.01):
        """å‰ªææ¨¡å‹æ¬Šé‡"""
        for name, param in self.model.named_parameters():
            mask = torch.abs(param.data) > threshold
            param.data *= mask
```

### 1.2 è¨ˆç®—å„ªåŒ–

```python
import onnx
import onnxruntime
from typing import List, Tuple

class ModelOptimizer:
    def __init__(self, model_path: str):
        self.model = onnx.load(model_path)
    
    def optimize_graph(self):
        """å„ªåŒ–è¨ˆç®—åœ–"""
        # é€²è¡Œå¸¸é‡æŠ˜ç–Š
        optimized_model = self._fold_constants()
        
        # æ¶ˆé™¤å†—é¤˜ç¯€é»
        optimized_model = self._eliminate_redundant_nodes(optimized_model)
        
        return optimized_model
    
    def _fold_constants(self):
        """å¸¸é‡æŠ˜ç–Šå„ªåŒ–"""
        passes = ['extract_constant_to_initializer',
                 'eliminate_unused_initializer',
                 'fuse_consecutive_transposes',
                 'fuse_transpose_into_gemm']
        
        optimized_model = self.model
        for pass_name in passes:
            optimized_model = onnx.optimizer.optimize(optimized_model, [pass_name])
        
        return optimized_model
```

## 2. è¶…åƒæ•¸å„ªåŒ–

### 2.1 ç¶²æ ¼æœç´¢

```python
from sklearn.model_selection import GridSearchCV
from typing import Dict, List
import numpy as np

class HyperparameterOptimizer:
    def __init__(self, model, param_grid: Dict, cv: int = 5):
        self.model = model
        self.param_grid = param_grid
        self.cv = cv
        
    def grid_search(self, X, y):
        """åŸ·è¡Œç¶²æ ¼æœç´¢"""
        grid_search = GridSearchCV(
            estimator=self.model,
            param_grid=self.param_grid,
            cv=self.cv,
            n_jobs=-1,
            verbose=2
        )
        
        grid_search.fit(X, y)
        return {
            'best_params': grid_search.best_params_,
            'best_score': grid_search.best_score_,
            'cv_results': grid_search.cv_results_
        }
```

### 2.2 è²è‘‰æ–¯å„ªåŒ–

```python
from bayes_opt import BayesianOptimization
import numpy as np

class BayesOptimizer:
    def __init__(self, model, param_bounds: Dict, n_iter: int = 50):
        self.model = model
        self.param_bounds = param_bounds
        self.n_iter = n_iter
    
    def optimize(self, X, y):
        """åŸ·è¡Œè²è‘‰æ–¯å„ªåŒ–"""
        def objective(**params):
            self.model.set_params(**params)
            cv_scores = cross_val_score(
                self.model, X, y,
                cv=5, scoring='accuracy'
            )
            return np.mean(cv_scores)
        
        optimizer = BayesianOptimization(
            f=objective,
            pbounds=self.param_bounds,
            random_state=42
        )
        
        optimizer.maximize(
            init_points=5,
            n_iter=self.n_iter
        )
        
        return optimizer.max
```

## 3. æ¨¡å‹è’¸é¤¾

### 3.1 çŸ¥è­˜è’¸é¤¾

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class DistillationLoss(nn.Module):
    def __init__(self, temperature: float = 3.0, alpha: float = 0.5):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha
        
    def forward(self, student_logits: torch.Tensor,
                teacher_logits: torch.Tensor,
                targets: torch.Tensor) -> torch.Tensor:
        """è¨ˆç®—è’¸é¤¾æå¤±"""
        # è»Ÿç›®æ¨™æå¤±
        soft_targets = F.softmax(teacher_logits / self.temperature, dim=1)
        soft_prob = F.log_softmax(student_logits / self.temperature, dim=1)
        soft_loss = F.kl_div(soft_prob, soft_targets, reduction='batchmean')
        
        # ç¡¬ç›®æ¨™æå¤±
        hard_loss = F.cross_entropy(student_logits, targets)
        
        # çµ„åˆæå¤±
        return self.alpha * hard_loss + (1 - self.alpha) * soft_loss
```

### 3.2 æ¨¡å‹è’¸é¤¾è¨“ç·´

```python
class DistillationTrainer:
    def __init__(self, teacher_model: nn.Module,
                 student_model: nn.Module,
                 optimizer: torch.optim.Optimizer,
                 distillation_loss: DistillationLoss):
        self.teacher_model = teacher_model
        self.student_model = student_model
        self.optimizer = optimizer
        self.distillation_loss = distillation_loss
    
    def train_step(self, inputs: torch.Tensor,
                   targets: torch.Tensor) -> float:
        """åŸ·è¡Œä¸€æ­¥è¨“ç·´"""
        # æ•™å¸«æ¨¡å‹æ¨ç†
        with torch.no_grad():
            teacher_logits = self.teacher_model(inputs)
        
        # å­¸ç”Ÿæ¨¡å‹è¨“ç·´
        self.optimizer.zero_grad()
        student_logits = self.student_model(inputs)
        
        # è¨ˆç®—æå¤±
        loss = self.distillation_loss(
            student_logits,
            teacher_logits,
            targets
        )
        
        # åå‘å‚³æ’­
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
```

## 4. ç¡¬ä»¶åŠ é€Ÿ

### 4.1 GPU å„ªåŒ–

```python
import torch
import torch.cuda.amp as amp
from typing import Optional

class GPUOptimizer:
    def __init__(self, model: nn.Module, device: Optional[str] = None):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = model.to(self.device)
        self.scaler = amp.GradScaler()
    
    def optimize_memory(self):
        """å„ªåŒ–GPUå…§å­˜ä½¿ç”¨"""
        if self.device == 'cuda':
            torch.cuda.empty_cache()
            torch.backends.cudnn.benchmark = True
    
    def mixed_precision_step(self, inputs: torch.Tensor,
                            targets: torch.Tensor,
                            optimizer: torch.optim.Optimizer) -> float:
        """æ··åˆç²¾åº¦è¨“ç·´æ­¥é©Ÿ"""
        optimizer.zero_grad()
        
        # ä½¿ç”¨è‡ªå‹•æ··åˆç²¾åº¦
        with amp.autocast():
            outputs = self.model(inputs)
            loss = F.cross_entropy(outputs, targets)
        
        # ç¸®æ”¾æå¤±ä¸¦åå‘å‚³æ’­
        self.scaler.scale(loss).backward()
        self.scaler.step(optimizer)
        self.scaler.update()
        
        return loss.item()
```

### 4.2 åˆ†å¸ƒå¼è¨“ç·´

```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel

class DistributedTrainer:
    def __init__(self, model: nn.Module, world_size: int):
        self.model = model
        self.world_size = world_size
    
    def setup(self, rank: int):
        """è¨­ç½®åˆ†å¸ƒå¼ç’°å¢ƒ"""
        dist.init_process_group(
            backend='nccl',
            init_method='tcp://localhost:23456',
            world_size=self.world_size,
            rank=rank
        )
        
        # å°‡æ¨¡å‹ç§»åˆ°ç•¶å‰è¨­å‚™
        device = torch.device(f'cuda:{rank}')
        self.model = self.model.to(device)
        
        # åŒ…è£ç‚ºåˆ†å¸ƒå¼æ¨¡å‹
        self.model = DistributedDataParallel(
            self.model,
            device_ids=[rank]
        )
    
    def cleanup(self):
        """æ¸…ç†åˆ†å¸ƒå¼ç’°å¢ƒ"""
        dist.destroy_process_group()
```

## ç·´ç¿’é¡Œ ğŸƒ

1. å¯¦ç¾ä¸€å€‹æ¨¡å‹é‡åŒ–å’Œå‰ªæçš„å®Œæ•´æµç¨‹ã€‚
2. ä½¿ç”¨è²è‘‰æ–¯å„ªåŒ–å°‹æ‰¾æœ€ä½³è¶…åƒæ•¸ã€‚
3. å¯¦ç¾ä¸€å€‹çŸ¥è­˜è’¸é¤¾æ¡†æ¶ï¼Œå°‡å¤§æ¨¡å‹çŸ¥è­˜é·ç§»åˆ°å°æ¨¡å‹ã€‚
4. é–‹ç™¼ä¸€å€‹GPUå…§å­˜å„ªåŒ–å·¥å…·ã€‚
5. å¯¦ç¾ä¸€å€‹åˆ†å¸ƒå¼è¨“ç·´ç³»çµ±ã€‚

## å°çµ ğŸ“

- å­¸ç¿’äº†æ¨¡å‹å£“ç¸®å’Œå„ªåŒ–æŠ€è¡“
- æŒæ¡äº†è¶…åƒæ•¸å„ªåŒ–æ–¹æ³•
- ç†è§£äº†çŸ¥è­˜è’¸é¤¾åŸç†
- å­¸æœƒäº†GPUåŠ é€ŸæŠ€è¡“
- äº†è§£äº†åˆ†å¸ƒå¼è¨“ç·´å¯¦ç¾

## å»¶ä¼¸é–±è®€ ğŸ“š

1. Deep Learning Model Optimization
2. Hyperparameter Optimization Techniques
3. Knowledge Distillation in Practice
4. GPU Computing with PyTorch
5. Distributed Deep Learning

[ä¸Šä¸€ç« ï¼šæ©Ÿå™¨å­¸ç¿’éƒ¨ç½²](118_æ©Ÿå™¨å­¸ç¿’éƒ¨ç½².md) | [ä¸‹ä¸€ç« ï¼šç«¯åˆ°ç«¯AIæ‡‰ç”¨é–‹ç™¼](120_ç«¯åˆ°ç«¯AIæ‡‰ç”¨é–‹ç™¼.md) 