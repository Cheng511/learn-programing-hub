[上一章：機器學習部署](118_機器學習部署.md) | [下一章：端到端AI應用開發](120_端到端AI應用開發.md)

# Python AI模型優化 🎯

## 1. 模型性能優化

### 1.1 模型壓縮

```python
import torch
import torch.nn as nn
from typing import Dict, List
import numpy as np

class ModelCompressor:
    def __init__(self, model: nn.Module):
        self.model = model
        
    def quantize_weights(self, bits: int = 8):
        """量化模型權重"""
        state_dict = self.model.state_dict()
        quantized_dict = {}
        
        for name, param in state_dict.items():
            if param.dtype == torch.float32:
                # 計算縮放因子
                max_val = torch.max(torch.abs(param))
                scale = (2 ** (bits - 1) - 1) / max_val
                
                # 量化
                quantized = torch.round(param * scale)
                quantized_dict[name] = quantized / scale
        
        self.model.load_state_dict(quantized_dict)
        return self.model
    
    def prune_weights(self, threshold: float = 0.01):
        """剪枝模型權重"""
        for name, param in self.model.named_parameters():
            mask = torch.abs(param.data) > threshold
            param.data *= mask
```

### 1.2 計算優化

```python
import onnx
import onnxruntime
from typing import List, Tuple

class ModelOptimizer:
    def __init__(self, model_path: str):
        self.model = onnx.load(model_path)
    
    def optimize_graph(self):
        """優化計算圖"""
        # 進行常量折疊
        optimized_model = self._fold_constants()
        
        # 消除冗餘節點
        optimized_model = self._eliminate_redundant_nodes(optimized_model)
        
        return optimized_model
    
    def _fold_constants(self):
        """常量折疊優化"""
        passes = ['extract_constant_to_initializer',
                 'eliminate_unused_initializer',
                 'fuse_consecutive_transposes',
                 'fuse_transpose_into_gemm']
        
        optimized_model = self.model
        for pass_name in passes:
            optimized_model = onnx.optimizer.optimize(optimized_model, [pass_name])
        
        return optimized_model
```

## 2. 超參數優化

### 2.1 網格搜索

```python
from sklearn.model_selection import GridSearchCV
from typing import Dict, List
import numpy as np

class HyperparameterOptimizer:
    def __init__(self, model, param_grid: Dict, cv: int = 5):
        self.model = model
        self.param_grid = param_grid
        self.cv = cv
        
    def grid_search(self, X, y):
        """執行網格搜索"""
        grid_search = GridSearchCV(
            estimator=self.model,
            param_grid=self.param_grid,
            cv=self.cv,
            n_jobs=-1,
            verbose=2
        )
        
        grid_search.fit(X, y)
        return {
            'best_params': grid_search.best_params_,
            'best_score': grid_search.best_score_,
            'cv_results': grid_search.cv_results_
        }
```

### 2.2 貝葉斯優化

```python
from bayes_opt import BayesianOptimization
import numpy as np

class BayesOptimizer:
    def __init__(self, model, param_bounds: Dict, n_iter: int = 50):
        self.model = model
        self.param_bounds = param_bounds
        self.n_iter = n_iter
    
    def optimize(self, X, y):
        """執行貝葉斯優化"""
        def objective(**params):
            self.model.set_params(**params)
            cv_scores = cross_val_score(
                self.model, X, y,
                cv=5, scoring='accuracy'
            )
            return np.mean(cv_scores)
        
        optimizer = BayesianOptimization(
            f=objective,
            pbounds=self.param_bounds,
            random_state=42
        )
        
        optimizer.maximize(
            init_points=5,
            n_iter=self.n_iter
        )
        
        return optimizer.max
```

## 3. 模型蒸餾

### 3.1 知識蒸餾

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class DistillationLoss(nn.Module):
    def __init__(self, temperature: float = 3.0, alpha: float = 0.5):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha
        
    def forward(self, student_logits: torch.Tensor,
                teacher_logits: torch.Tensor,
                targets: torch.Tensor) -> torch.Tensor:
        """計算蒸餾損失"""
        # 軟目標損失
        soft_targets = F.softmax(teacher_logits / self.temperature, dim=1)
        soft_prob = F.log_softmax(student_logits / self.temperature, dim=1)
        soft_loss = F.kl_div(soft_prob, soft_targets, reduction='batchmean')
        
        # 硬目標損失
        hard_loss = F.cross_entropy(student_logits, targets)
        
        # 組合損失
        return self.alpha * hard_loss + (1 - self.alpha) * soft_loss
```

### 3.2 模型蒸餾訓練

```python
class DistillationTrainer:
    def __init__(self, teacher_model: nn.Module,
                 student_model: nn.Module,
                 optimizer: torch.optim.Optimizer,
                 distillation_loss: DistillationLoss):
        self.teacher_model = teacher_model
        self.student_model = student_model
        self.optimizer = optimizer
        self.distillation_loss = distillation_loss
    
    def train_step(self, inputs: torch.Tensor,
                   targets: torch.Tensor) -> float:
        """執行一步訓練"""
        # 教師模型推理
        with torch.no_grad():
            teacher_logits = self.teacher_model(inputs)
        
        # 學生模型訓練
        self.optimizer.zero_grad()
        student_logits = self.student_model(inputs)
        
        # 計算損失
        loss = self.distillation_loss(
            student_logits,
            teacher_logits,
            targets
        )
        
        # 反向傳播
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
```

## 4. 硬件加速

### 4.1 GPU 優化

```python
import torch
import torch.cuda.amp as amp
from typing import Optional

class GPUOptimizer:
    def __init__(self, model: nn.Module, device: Optional[str] = None):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = model.to(self.device)
        self.scaler = amp.GradScaler()
    
    def optimize_memory(self):
        """優化GPU內存使用"""
        if self.device == 'cuda':
            torch.cuda.empty_cache()
            torch.backends.cudnn.benchmark = True
    
    def mixed_precision_step(self, inputs: torch.Tensor,
                            targets: torch.Tensor,
                            optimizer: torch.optim.Optimizer) -> float:
        """混合精度訓練步驟"""
        optimizer.zero_grad()
        
        # 使用自動混合精度
        with amp.autocast():
            outputs = self.model(inputs)
            loss = F.cross_entropy(outputs, targets)
        
        # 縮放損失並反向傳播
        self.scaler.scale(loss).backward()
        self.scaler.step(optimizer)
        self.scaler.update()
        
        return loss.item()
```

### 4.2 分布式訓練

```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel

class DistributedTrainer:
    def __init__(self, model: nn.Module, world_size: int):
        self.model = model
        self.world_size = world_size
    
    def setup(self, rank: int):
        """設置分布式環境"""
        dist.init_process_group(
            backend='nccl',
            init_method='tcp://localhost:23456',
            world_size=self.world_size,
            rank=rank
        )
        
        # 將模型移到當前設備
        device = torch.device(f'cuda:{rank}')
        self.model = self.model.to(device)
        
        # 包裝為分布式模型
        self.model = DistributedDataParallel(
            self.model,
            device_ids=[rank]
        )
    
    def cleanup(self):
        """清理分布式環境"""
        dist.destroy_process_group()
```

## 練習題 🏃

1. 實現一個模型量化和剪枝的完整流程。
2. 使用貝葉斯優化尋找最佳超參數。
3. 實現一個知識蒸餾框架，將大模型知識遷移到小模型。
4. 開發一個GPU內存優化工具。
5. 實現一個分布式訓練系統。

## 小結 📝

- 學習了模型壓縮和優化技術
- 掌握了超參數優化方法
- 理解了知識蒸餾原理
- 學會了GPU加速技術
- 了解了分布式訓練實現

## 延伸閱讀 📚

1. Deep Learning Model Optimization
2. Hyperparameter Optimization Techniques
3. Knowledge Distillation in Practice
4. GPU Computing with PyTorch
5. Distributed Deep Learning

[上一章：機器學習部署](118_機器學習部署.md) | [下一章：端到端AI應用開發](120_端到端AI應用開發.md) 