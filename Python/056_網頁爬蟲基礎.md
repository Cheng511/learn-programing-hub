[上一章：自然語言處理基礎](055_自然語言處理基礎.md) | [下一章：進階網頁爬蟲](057_進階網頁爬蟲.md)

# Python 網頁爬蟲基礎 🕷️

## 網頁爬蟲概述

網頁爬蟲是一種自動化獲取網頁數據的技術。本章將介紹使用Python進行網頁爬蟲的基礎知識，包括基本的HTTP請求和HTML解析。

## HTTP請求基礎

### 1. Requests庫使用

```python
import requests
from requests.exceptions import RequestException
import time

# 基本GET請求
def basic_get_request():
    url = 'https://api.github.com/events'
    try:
        response = requests.get(url)
        response.raise_for_status()  # 檢查請求是否成功
        print('狀態碼:', response.status_code)
        print('響應頭:', response.headers)
        print('內容類型:', response.headers['content-type'])
        return response.json()
    except RequestException as e:
        print(f'請求失敗: {e}')
        return None

# 帶參數的GET請求
def get_with_params():
    params = {
        'q': 'python',
        'sort': 'stars',
        'order': 'desc'
    }
    url = 'https://api.github.com/search/repositories'
    try:
        response = requests.get(url, params=params)
        return response.json()
    except RequestException as e:
        print(f'請求失敗: {e}')
        return None

# POST請求
def post_request():
    url = 'https://httpbin.org/post'
    data = {'key': 'value'}
    try:
        response = requests.post(url, data=data)
        return response.json()
    except RequestException as e:
        print(f'請求失敗: {e}')
        return None

# 自定義請求頭
def custom_headers_request():
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    url = 'https://api.github.com/user'
    try:
        response = requests.get(url, headers=headers)
        return response.json()
    except RequestException as e:
        print(f'請求失敗: {e}')
        return None

# 處理會話
def session_request():
    with requests.Session() as session:
        session.headers.update({'User-Agent': 'Mozilla/5.0'})
        try:
            response = session.get('https://httpbin.org/cookies/set/sessioncookie/123456789')
            print('Cookies:', response.cookies)
            response = session.get('https://httpbin.org/cookies')
            return response.json()
        except RequestException as e:
            print(f'請求失敗: {e}')
            return None
```

## HTML解析

### 1. BeautifulSoup基礎

```python
from bs4 import BeautifulSoup
import requests

# 解析HTML
def parse_html():
    html_doc = """
    <html>
        <head><title>網頁爬蟲示例</title></head>
        <body>
            <div class="content">
                <h1>歡迎學習Python爬蟲</h1>
                <p class="description">這是一個示例段落</p>
                <ul>
                    <li>第一項</li>
                    <li>第二項</li>
                    <li>第三項</li>
                </ul>
            </div>
        </body>
    </html>
    """
    
    # 創建BeautifulSoup對象
    soup = BeautifulSoup(html_doc, 'html.parser')
    
    # 查找元素
    print('標題:', soup.title.string)
    print('h1文本:', soup.h1.text)
    print('所有li元素:', [li.text for li in soup.find_all('li')])
    print('class為description的元素:', soup.find(class_='description').text)
    
    # CSS選擇器
    content = soup.select('.content')
    items = soup.select('ul li')
    print('使用CSS選擇器:', [item.text for item in items])

# 實際網頁解析
def parse_real_webpage():
    url = 'https://example.com'
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 提取所有鏈接
        links = soup.find_all('a')
        print('所有鏈接:', [(link.text, link.get('href')) for link in links])
        
        # 提取所有段落
        paragraphs = soup.find_all('p')
        print('所有段落:', [p.text for p in paragraphs])
        
    except RequestException as e:
        print(f'請求失敗: {e}')
```

### 2. 正則表達式輔助解析

```python
import re
from bs4 import BeautifulSoup

def regex_parsing():
    html = """
    <div class="price">$99.99</div>
    <div class="price">$149.99</div>
    <div class="price">$199.99</div>
    """
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # 使用正則表達式查找價格
    prices = soup.find_all(string=re.compile(r'\$\d+\.\d{2}'))
    print('價格列表:', prices)
    
    # 提取數字
    price_values = [float(re.search(r'\d+\.\d{2}', price).group()) 
                   for price in prices]
    print('數值列表:', price_values)
```

## 數據提取與存儲

### 1. CSV文件操作

```python
import csv
import pandas as pd

def save_to_csv(data, filename):
    # 使用csv模塊
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['標題', '鏈接', '描述'])
        for item in data:
            writer.writerow([item['title'], item['link'], item['description']])
    
    # 使用pandas
    df = pd.DataFrame(data)
    df.to_csv(f'{filename}_pandas.csv', index=False, encoding='utf-8')
```

### 2. JSON文件操作

```python
import json

def save_to_json(data, filename):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_from_json(filename):
    with open(filename, 'r', encoding='utf-8') as f:
        return json.load(f)
```

## 實戰項目：網站文章爬蟲

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
from typing import List, Dict

class ArticleScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def get_page(self, url: str) -> BeautifulSoup:
        """獲取頁面內容並返回BeautifulSoup對象"""
        try:
            response = self.session.get(url)
            response.raise_for_status()
            return BeautifulSoup(response.text, 'html.parser')
        except Exception as e:
            print(f'獲取頁面失敗: {e}')
            return None
    
    def parse_article(self, article_soup: BeautifulSoup) -> Dict:
        """解析文章內容"""
        try:
            return {
                'title': article_soup.find('h1').text.strip(),
                'content': article_soup.find('article').text.strip(),
                'date': article_soup.find('time').text.strip(),
                'author': article_soup.find(class_='author').text.strip()
            }
        except Exception as e:
            print(f'解析文章失敗: {e}')
            return None
    
    def scrape_articles(self, base_url: str, num_pages: int) -> List[Dict]:
        """爬取多個頁面的文章"""
        articles = []
        
        for page in range(1, num_pages + 1):
            print(f'正在爬取第 {page} 頁...')
            url = f'{base_url}/page/{page}'
            soup = self.get_page(url)
            
            if not soup:
                continue
            
            # 獲取文章鏈接
            article_links = soup.select('article h2 a')
            
            for link in article_links:
                article_url = link.get('href')
                article_soup = self.get_page(article_url)
                
                if article_soup:
                    article_data = self.parse_article(article_soup)
                    if article_data:
                        articles.append(article_data)
                
                # 添加延遲避免請求過快
                time.sleep(1)
        
        return articles
    
    def save_articles(self, articles: List[Dict], filename: str):
        """保存文章數據"""
        # 保存為CSV
        df = pd.DataFrame(articles)
        df.to_csv(f'{filename}.csv', index=False, encoding='utf-8')
        
        # 保存為JSON
        with open(f'{filename}.json', 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)

def main():
    scraper = ArticleScraper()
    base_url = 'https://example.com/blog'
    articles = scraper.scrape_articles(base_url, num_pages=5)
    
    if articles:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'articles_{timestamp}'
        scraper.save_articles(articles, filename)
        print(f'成功爬取 {len(articles)} 篇文章')
    else:
        print('爬取失敗')

if __name__ == '__main__':
    main()
```

## 練習題

1. **新聞爬蟲**
   實現一個新聞網站爬蟲：
   - 提取新聞標題和內容
   - 保存圖片和視頻
   - 數據存儲和導出
   - 定時更新功能

2. **電商數據爬蟲**
   開發一個電商網站爬蟲：
   - 商品信息提取
   - 價格監控
   - 評論分析
   - 數據可視化

3. **社交媒體爬蟲**
   實現一個社交媒體爬蟲：
   - 用戶信息收集
   - 互動數據統計
   - 話題趨勢分析
   - 數據報告生成

## 小提醒 💡

1. 請求設置
   - 設置合適的請求頭
   - 使用代理IP
   - 控制請求頻率
   - 處理異常情況

2. 數據解析
   - 選擇合適的解析方法
   - 處理特殊字符
   - 注意數據清理
   - 保持數據結構

3. 反爬處理
   - 遵守robots.txt
   - 添加請求延遲
   - 處理驗證碼
   - 模擬正常訪問

4. 最佳實踐
   - 編寫健壯的代碼
   - 做好異常處理
   - 保存原始數據
   - 定期維護更新

[上一章：自然語言處理基礎](055_自然語言處理基礎.md) | [下一章：進階網頁爬蟲](057_進階網頁爬蟲.md) 