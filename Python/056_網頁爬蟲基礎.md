[ä¸Šä¸€ç« ï¼šè‡ªç„¶èªè¨€è™•ç†åŸºç¤](055_è‡ªç„¶èªè¨€è™•ç†åŸºç¤.md) | [ä¸‹ä¸€ç« ï¼šé€²éšç¶²é çˆ¬èŸ²](057_é€²éšç¶²é çˆ¬èŸ².md)

# Python ç¶²é çˆ¬èŸ²åŸºç¤ ğŸ•·ï¸

## ç¶²é çˆ¬èŸ²æ¦‚è¿°

ç¶²é çˆ¬èŸ²æ˜¯ä¸€ç¨®è‡ªå‹•åŒ–ç²å–ç¶²é æ•¸æ“šçš„æŠ€è¡“ã€‚æœ¬ç« å°‡ä»‹ç´¹ä½¿ç”¨Pythoné€²è¡Œç¶²é çˆ¬èŸ²çš„åŸºç¤çŸ¥è­˜ï¼ŒåŒ…æ‹¬åŸºæœ¬çš„HTTPè«‹æ±‚å’ŒHTMLè§£æã€‚

## HTTPè«‹æ±‚åŸºç¤

### 1. Requestsåº«ä½¿ç”¨

```python
import requests
from requests.exceptions import RequestException
import time

# åŸºæœ¬GETè«‹æ±‚
def basic_get_request():
    url = 'https://api.github.com/events'
    try:
        response = requests.get(url)
        response.raise_for_status()  # æª¢æŸ¥è«‹æ±‚æ˜¯å¦æˆåŠŸ
        print('ç‹€æ…‹ç¢¼:', response.status_code)
        print('éŸ¿æ‡‰é ­:', response.headers)
        print('å…§å®¹é¡å‹:', response.headers['content-type'])
        return response.json()
    except RequestException as e:
        print(f'è«‹æ±‚å¤±æ•—: {e}')
        return None

# å¸¶åƒæ•¸çš„GETè«‹æ±‚
def get_with_params():
    params = {
        'q': 'python',
        'sort': 'stars',
        'order': 'desc'
    }
    url = 'https://api.github.com/search/repositories'
    try:
        response = requests.get(url, params=params)
        return response.json()
    except RequestException as e:
        print(f'è«‹æ±‚å¤±æ•—: {e}')
        return None

# POSTè«‹æ±‚
def post_request():
    url = 'https://httpbin.org/post'
    data = {'key': 'value'}
    try:
        response = requests.post(url, data=data)
        return response.json()
    except RequestException as e:
        print(f'è«‹æ±‚å¤±æ•—: {e}')
        return None

# è‡ªå®šç¾©è«‹æ±‚é ­
def custom_headers_request():
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    url = 'https://api.github.com/user'
    try:
        response = requests.get(url, headers=headers)
        return response.json()
    except RequestException as e:
        print(f'è«‹æ±‚å¤±æ•—: {e}')
        return None

# è™•ç†æœƒè©±
def session_request():
    with requests.Session() as session:
        session.headers.update({'User-Agent': 'Mozilla/5.0'})
        try:
            response = session.get('https://httpbin.org/cookies/set/sessioncookie/123456789')
            print('Cookies:', response.cookies)
            response = session.get('https://httpbin.org/cookies')
            return response.json()
        except RequestException as e:
            print(f'è«‹æ±‚å¤±æ•—: {e}')
            return None
```

## HTMLè§£æ

### 1. BeautifulSoupåŸºç¤

```python
from bs4 import BeautifulSoup
import requests

# è§£æHTML
def parse_html():
    html_doc = """
    <html>
        <head><title>ç¶²é çˆ¬èŸ²ç¤ºä¾‹</title></head>
        <body>
            <div class="content">
                <h1>æ­¡è¿å­¸ç¿’Pythonçˆ¬èŸ²</h1>
                <p class="description">é€™æ˜¯ä¸€å€‹ç¤ºä¾‹æ®µè½</p>
                <ul>
                    <li>ç¬¬ä¸€é …</li>
                    <li>ç¬¬äºŒé …</li>
                    <li>ç¬¬ä¸‰é …</li>
                </ul>
            </div>
        </body>
    </html>
    """
    
    # å‰µå»ºBeautifulSoupå°è±¡
    soup = BeautifulSoup(html_doc, 'html.parser')
    
    # æŸ¥æ‰¾å…ƒç´ 
    print('æ¨™é¡Œ:', soup.title.string)
    print('h1æ–‡æœ¬:', soup.h1.text)
    print('æ‰€æœ‰liå…ƒç´ :', [li.text for li in soup.find_all('li')])
    print('classç‚ºdescriptionçš„å…ƒç´ :', soup.find(class_='description').text)
    
    # CSSé¸æ“‡å™¨
    content = soup.select('.content')
    items = soup.select('ul li')
    print('ä½¿ç”¨CSSé¸æ“‡å™¨:', [item.text for item in items])

# å¯¦éš›ç¶²é è§£æ
def parse_real_webpage():
    url = 'https://example.com'
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ‰€æœ‰éˆæ¥
        links = soup.find_all('a')
        print('æ‰€æœ‰éˆæ¥:', [(link.text, link.get('href')) for link in links])
        
        # æå–æ‰€æœ‰æ®µè½
        paragraphs = soup.find_all('p')
        print('æ‰€æœ‰æ®µè½:', [p.text for p in paragraphs])
        
    except RequestException as e:
        print(f'è«‹æ±‚å¤±æ•—: {e}')
```

### 2. æ­£å‰‡è¡¨é”å¼è¼”åŠ©è§£æ

```python
import re
from bs4 import BeautifulSoup

def regex_parsing():
    html = """
    <div class="price">$99.99</div>
    <div class="price">$149.99</div>
    <div class="price">$199.99</div>
    """
    
    soup = BeautifulSoup(html, 'html.parser')
    
    # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æŸ¥æ‰¾åƒ¹æ ¼
    prices = soup.find_all(string=re.compile(r'\$\d+\.\d{2}'))
    print('åƒ¹æ ¼åˆ—è¡¨:', prices)
    
    # æå–æ•¸å­—
    price_values = [float(re.search(r'\d+\.\d{2}', price).group()) 
                   for price in prices]
    print('æ•¸å€¼åˆ—è¡¨:', price_values)
```

## æ•¸æ“šæå–èˆ‡å­˜å„²

### 1. CSVæ–‡ä»¶æ“ä½œ

```python
import csv
import pandas as pd

def save_to_csv(data, filename):
    # ä½¿ç”¨csvæ¨¡å¡Š
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['æ¨™é¡Œ', 'éˆæ¥', 'æè¿°'])
        for item in data:
            writer.writerow([item['title'], item['link'], item['description']])
    
    # ä½¿ç”¨pandas
    df = pd.DataFrame(data)
    df.to_csv(f'{filename}_pandas.csv', index=False, encoding='utf-8')
```

### 2. JSONæ–‡ä»¶æ“ä½œ

```python
import json

def save_to_json(data, filename):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_from_json(filename):
    with open(filename, 'r', encoding='utf-8') as f:
        return json.load(f)
```

## å¯¦æˆ°é …ç›®ï¼šç¶²ç«™æ–‡ç« çˆ¬èŸ²

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time
from typing import List, Dict

class ArticleScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def get_page(self, url: str) -> BeautifulSoup:
        """ç²å–é é¢å…§å®¹ä¸¦è¿”å›BeautifulSoupå°è±¡"""
        try:
            response = self.session.get(url)
            response.raise_for_status()
            return BeautifulSoup(response.text, 'html.parser')
        except Exception as e:
            print(f'ç²å–é é¢å¤±æ•—: {e}')
            return None
    
    def parse_article(self, article_soup: BeautifulSoup) -> Dict:
        """è§£ææ–‡ç« å…§å®¹"""
        try:
            return {
                'title': article_soup.find('h1').text.strip(),
                'content': article_soup.find('article').text.strip(),
                'date': article_soup.find('time').text.strip(),
                'author': article_soup.find(class_='author').text.strip()
            }
        except Exception as e:
            print(f'è§£ææ–‡ç« å¤±æ•—: {e}')
            return None
    
    def scrape_articles(self, base_url: str, num_pages: int) -> List[Dict]:
        """çˆ¬å–å¤šå€‹é é¢çš„æ–‡ç« """
        articles = []
        
        for page in range(1, num_pages + 1):
            print(f'æ­£åœ¨çˆ¬å–ç¬¬ {page} é ...')
            url = f'{base_url}/page/{page}'
            soup = self.get_page(url)
            
            if not soup:
                continue
            
            # ç²å–æ–‡ç« éˆæ¥
            article_links = soup.select('article h2 a')
            
            for link in article_links:
                article_url = link.get('href')
                article_soup = self.get_page(article_url)
                
                if article_soup:
                    article_data = self.parse_article(article_soup)
                    if article_data:
                        articles.append(article_data)
                
                # æ·»åŠ å»¶é²é¿å…è«‹æ±‚éå¿«
                time.sleep(1)
        
        return articles
    
    def save_articles(self, articles: List[Dict], filename: str):
        """ä¿å­˜æ–‡ç« æ•¸æ“š"""
        # ä¿å­˜ç‚ºCSV
        df = pd.DataFrame(articles)
        df.to_csv(f'{filename}.csv', index=False, encoding='utf-8')
        
        # ä¿å­˜ç‚ºJSON
        with open(f'{filename}.json', 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)

def main():
    scraper = ArticleScraper()
    base_url = 'https://example.com/blog'
    articles = scraper.scrape_articles(base_url, num_pages=5)
    
    if articles:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'articles_{timestamp}'
        scraper.save_articles(articles, filename)
        print(f'æˆåŠŸçˆ¬å– {len(articles)} ç¯‡æ–‡ç« ')
    else:
        print('çˆ¬å–å¤±æ•—')

if __name__ == '__main__':
    main()
```

## ç·´ç¿’é¡Œ

1. **æ–°èçˆ¬èŸ²**
   å¯¦ç¾ä¸€å€‹æ–°èç¶²ç«™çˆ¬èŸ²ï¼š
   - æå–æ–°èæ¨™é¡Œå’Œå…§å®¹
   - ä¿å­˜åœ–ç‰‡å’Œè¦–é »
   - æ•¸æ“šå­˜å„²å’Œå°å‡º
   - å®šæ™‚æ›´æ–°åŠŸèƒ½

2. **é›»å•†æ•¸æ“šçˆ¬èŸ²**
   é–‹ç™¼ä¸€å€‹é›»å•†ç¶²ç«™çˆ¬èŸ²ï¼š
   - å•†å“ä¿¡æ¯æå–
   - åƒ¹æ ¼ç›£æ§
   - è©•è«–åˆ†æ
   - æ•¸æ“šå¯è¦–åŒ–

3. **ç¤¾äº¤åª’é«”çˆ¬èŸ²**
   å¯¦ç¾ä¸€å€‹ç¤¾äº¤åª’é«”çˆ¬èŸ²ï¼š
   - ç”¨æˆ¶ä¿¡æ¯æ”¶é›†
   - äº’å‹•æ•¸æ“šçµ±è¨ˆ
   - è©±é¡Œè¶¨å‹¢åˆ†æ
   - æ•¸æ“šå ±å‘Šç”Ÿæˆ

## å°æé†’ ğŸ’¡

1. è«‹æ±‚è¨­ç½®
   - è¨­ç½®åˆé©çš„è«‹æ±‚é ­
   - ä½¿ç”¨ä»£ç†IP
   - æ§åˆ¶è«‹æ±‚é »ç‡
   - è™•ç†ç•°å¸¸æƒ…æ³

2. æ•¸æ“šè§£æ
   - é¸æ“‡åˆé©çš„è§£ææ–¹æ³•
   - è™•ç†ç‰¹æ®Šå­—ç¬¦
   - æ³¨æ„æ•¸æ“šæ¸…ç†
   - ä¿æŒæ•¸æ“šçµæ§‹

3. åçˆ¬è™•ç†
   - éµå®ˆrobots.txt
   - æ·»åŠ è«‹æ±‚å»¶é²
   - è™•ç†é©—è­‰ç¢¼
   - æ¨¡æ“¬æ­£å¸¸è¨ªå•

4. æœ€ä½³å¯¦è¸
   - ç·¨å¯«å¥å£¯çš„ä»£ç¢¼
   - åšå¥½ç•°å¸¸è™•ç†
   - ä¿å­˜åŸå§‹æ•¸æ“š
   - å®šæœŸç¶­è­·æ›´æ–°

[ä¸Šä¸€ç« ï¼šè‡ªç„¶èªè¨€è™•ç†åŸºç¤](055_è‡ªç„¶èªè¨€è™•ç†åŸºç¤.md) | [ä¸‹ä¸€ç« ï¼šé€²éšç¶²é çˆ¬èŸ²](057_é€²éšç¶²é çˆ¬èŸ².md) 