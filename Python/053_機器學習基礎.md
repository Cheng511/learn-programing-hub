[ä¸Šä¸€ç« ï¼šæ•¸æ“šåˆ†æèˆ‡è¦–è¦ºåŒ–](052_æ•¸æ“šåˆ†æèˆ‡è¦–è¦ºåŒ–.md) | [ä¸‹ä¸€ç« ï¼šæ·±åº¦å­¸ç¿’å…¥é–€](054_æ·±åº¦å­¸ç¿’å…¥é–€.md)

# Python æ©Ÿå™¨å­¸ç¿’åŸºç¤ ğŸ¤–

## æ©Ÿå™¨å­¸ç¿’æ¦‚è¿°

æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€å€‹å­é ˜åŸŸï¼Œå®ƒä½¿è¨ˆç®—æ©Ÿç³»çµ±èƒ½å¤ é€šéç¶“é©—è‡ªå‹•æ”¹é€²å…¶æ€§èƒ½ã€‚æœ¬ç« å°‡ä»‹ç´¹ä½¿ç”¨Pythoné€²è¡Œæ©Ÿå™¨å­¸ç¿’çš„åŸºç¤çŸ¥è­˜ã€‚

## æ•¸æ“šé è™•ç†

### 1. æ•¸æ“šåŠ è¼‰å’Œæ¸…ç†

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# åŠ è¼‰æ•¸æ“š
def load_and_clean_data(file_path):
    # è®€å–æ•¸æ“š
    df = pd.read_csv(file_path)
    
    # è™•ç†ç¼ºå¤±å€¼
    df = df.dropna()  # æˆ–ä½¿ç”¨å¡«å……: df.fillna(method='ffill')
    
    # è™•ç†ç•°å¸¸å€¼
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3 - Q1
    df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]
    
    return df

# ç‰¹å¾µå·¥ç¨‹
def prepare_features(df):
    # æ•¸å€¼ç‰¹å¾µæ¨™æº–åŒ–
    scaler = StandardScaler()
    numeric_features = df.select_dtypes(include=['float64', 'int64']).columns
    df[numeric_features] = scaler.fit_transform(df[numeric_features])
    
    # é¡åˆ¥ç‰¹å¾µç·¨ç¢¼
    categorical_features = df.select_dtypes(include=['object']).columns
    df = pd.get_dummies(df, columns=categorical_features)
    
    return df
```

### 2. æ•¸æ“šåˆ†å‰²

```python
# åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›†
def split_data(X, y, test_size=0.2, random_state=42):
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    return X_train, X_test, y_train, y_test
```

## ç›£ç£å­¸ç¿’

### 1. ç·šæ€§å›æ­¸

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# å‰µå»ºç¤ºä¾‹æ•¸æ“š
np.random.seed(42)
X = np.random.rand(100, 1) * 10
y = 2 * X + 1 + np.random.randn(100, 1) * 0.5

# è¨“ç·´æ¨¡å‹
model = LinearRegression()
model.fit(X, y)

# é æ¸¬
y_pred = model.predict(X)

# è©•ä¼°
print(f'ä¿‚æ•¸: {model.coef_[0][0]:.2f}')
print(f'æˆªè·: {model.intercept_[0]:.2f}')
print(f'R2åˆ†æ•¸: {r2_score(y, y_pred):.2f}')
print(f'å‡æ–¹èª¤å·®: {mean_squared_error(y, y_pred):.2f}')

# è¦–è¦ºåŒ–
import matplotlib.pyplot as plt

plt.scatter(X, y, color='blue', label='å¯¦éš›æ•¸æ“š')
plt.plot(X, y_pred, color='red', label='é æ¸¬ç·š')
plt.xlabel('X')
plt.ylabel('y')
plt.title('ç·šæ€§å›æ­¸ç¤ºä¾‹')
plt.legend()
plt.show()
```

### 2. é‚è¼¯å›æ­¸

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.datasets import make_classification

# å‰µå»ºåˆ†é¡æ•¸æ“š
X, y = make_classification(
    n_samples=100, n_features=2, n_redundant=0,
    n_clusters_per_class=1, random_state=42
)

# åˆ†å‰²æ•¸æ“š
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# è¨“ç·´æ¨¡å‹
model = LogisticRegression()
model.fit(X_train, y_train)

# é æ¸¬
y_pred = model.predict(X_test)

# è©•ä¼°
print('æº–ç¢ºç‡:', accuracy_score(y_test, y_pred))
print('\nåˆ†é¡å ±å‘Š:\n', classification_report(y_test, y_pred))

# è¦–è¦ºåŒ–æ±ºç­–é‚Šç•Œ
def plot_decision_boundary(X, y, model):
    h = 0.02  # ç¶²æ ¼æ­¥é•·
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.contourf(xx, yy, Z, alpha=0.4)
    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Logistic Regression Decision Boundary')
    plt.show()

plot_decision_boundary(X, y, model)
```

### 3. æ±ºç­–æ¨¹

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree

# å‰µå»ºå’Œè¨“ç·´æ±ºç­–æ¨¹
tree = DecisionTreeClassifier(max_depth=3, random_state=42)
tree.fit(X_train, y_train)

# é æ¸¬å’Œè©•ä¼°
y_pred = tree.predict(X_test)
print('æ±ºç­–æ¨¹æº–ç¢ºç‡:', accuracy_score(y_test, y_pred))

# è¦–è¦ºåŒ–æ±ºç­–æ¨¹
plt.figure(figsize=(15,10))
plot_tree(tree, filled=True, feature_names=['Feature 1', 'Feature 2'])
plt.title('Decision Tree Visualization')
plt.show()
```

## éç›£ç£å­¸ç¿’

### 1. K-meansèšé¡

```python
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# å‰µå»ºèšé¡æ•¸æ“š
X, y = make_blobs(n_samples=300, centers=4, random_state=42)

# è¨“ç·´K-meansæ¨¡å‹
kmeans = KMeans(n_clusters=4, random_state=42)
kmeans.fit(X)

# ç²å–èšé¡æ¨™ç±¤
labels = kmeans.labels_

# è¦–è¦ºåŒ–èšé¡çµæœ
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0],
           kmeans.cluster_centers_[:, 1],
           marker='x', s=200, linewidths=3,
           color='r', label='Centroids')
plt.title('K-means Clustering')
plt.legend()
plt.show()
```

### 2. ä¸»æˆåˆ†åˆ†æ (PCA)

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits

# åŠ è¼‰æ•¸æ“š
digits = load_digits()
X = digits.data
y = digits.target

# åŸ·è¡ŒPCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# è¦–è¦ºåŒ–çµæœ
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')
plt.colorbar(scatter)
plt.title('PCA of Digits Dataset')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.show()

# è§£é‡‹æ–¹å·®æ¯”
print('è§£é‡‹æ–¹å·®æ¯”:', pca.explained_variance_ratio_)
```

## æ¨¡å‹è©•ä¼°èˆ‡å„ªåŒ–

### 1. äº¤å‰é©—è­‰

```python
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC

# å‰µå»ºåˆ†é¡å™¨
svc = SVC(kernel='rbf', random_state=42)

# åŸ·è¡Œäº¤å‰é©—è­‰
scores = cross_val_score(svc, X, y, cv=5)

print('äº¤å‰é©—è­‰åˆ†æ•¸:', scores)
print('å¹³å‡æº–ç¢ºç‡: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 2))
```

### 2. ç¶²æ ¼æœç´¢

```python
from sklearn.model_selection import GridSearchCV

# å®šç¾©åƒæ•¸ç¶²æ ¼
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['rbf', 'linear'],
    'gamma': ['scale', 'auto', 0.1, 1],
}

# å‰µå»ºç¶²æ ¼æœç´¢å°è±¡
grid_search = GridSearchCV(SVC(), param_grid, cv=5)
grid_search.fit(X, y)

# è¼¸å‡ºæœ€ä½³åƒæ•¸
print('æœ€ä½³åƒæ•¸:', grid_search.best_params_)
print('æœ€ä½³åˆ†æ•¸:', grid_search.best_score_)
```

## å¯¦æˆ°é …ç›®ï¼šæˆ¿åƒ¹é æ¸¬

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# åŠ è¼‰æ•¸æ“š
def load_house_data():
    # ç¤ºä¾‹æ•¸æ“š
    np.random.seed(42)
    n_samples = 1000
    
    data = {
        'size': np.random.normal(150, 40, n_samples),
        'rooms': np.random.randint(1, 6, n_samples),
        'age': np.random.randint(0, 50, n_samples),
        'location_score': np.random.uniform(0, 10, n_samples),
        'price': np.zeros(n_samples)
    }
    
    # ç”Ÿæˆåƒ¹æ ¼
    data['price'] = (
        data['size'] * 1000 +
        data['rooms'] * 50000 -
        data['age'] * 1000 +
        data['location_score'] * 10000 +
        np.random.normal(0, 10000, n_samples)
    )
    
    return pd.DataFrame(data)

# æ•¸æ“šé è™•ç†
def preprocess_data(df):
    # ç‰¹å¾µç¸®æ”¾
    scaler = StandardScaler()
    features = ['size', 'rooms', 'age', 'location_score']
    df[features] = scaler.fit_transform(df[features])
    return df

# ä¸»å‡½æ•¸
def main():
    # åŠ è¼‰å’Œé è™•ç†æ•¸æ“š
    df = load_house_data()
    df = preprocess_data(df)
    
    # æº–å‚™ç‰¹å¾µå’Œç›®æ¨™
    X = df.drop('price', axis=1)
    y = df['price']
    
    # åˆ†å‰²æ•¸æ“š
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # è¨“ç·´æ¨¡å‹
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    # é æ¸¬å’Œè©•ä¼°
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    print(f'å‡æ–¹èª¤å·®: {mse:.2f}')
    print(f'R2åˆ†æ•¸: {r2:.2f}')
    
    # ç‰¹å¾µé‡è¦æ€§
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': model.feature_importances_
    })
    print('\nç‰¹å¾µé‡è¦æ€§:')
    print(feature_importance.sort_values('importance', ascending=False))
    
    # è¦–è¦ºåŒ–é æ¸¬çµæœ
    plt.figure(figsize=(10, 6))
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
             'r--', lw=2)
    plt.xlabel('å¯¦éš›åƒ¹æ ¼')
    plt.ylabel('é æ¸¬åƒ¹æ ¼')
    plt.title('æˆ¿åƒ¹é æ¸¬çµæœ')
    plt.tight_layout()
    plt.show()

if __name__ == '__main__':
    main()
```

## ç·´ç¿’é¡Œ

1. **äºŒåˆ†é¡å•é¡Œ**
   å¯¦ç¾ä¸€å€‹ä¿¡ç”¨å¡æ¬ºè©æª¢æ¸¬ç³»çµ±ï¼š
   - æ•¸æ“šé è™•ç†å’Œç‰¹å¾µå·¥ç¨‹
   - æ¨¡å‹é¸æ“‡å’Œè¨“ç·´
   - æ¨¡å‹è©•ä¼°å’Œå„ªåŒ–
   - çµæœå¯è¦–åŒ–

2. **å›æ­¸å•é¡Œ**
   é–‹ç™¼ä¸€å€‹è‚¡ç¥¨åƒ¹æ ¼é æ¸¬æ¨¡å‹ï¼š
   - æ™‚é–“åºåˆ—æ•¸æ“šè™•ç†
   - ç‰¹å¾µå‰µå»º
   - æ¨¡å‹è¨“ç·´å’Œè©•ä¼°
   - é æ¸¬çµæœåˆ†æ

3. **èšé¡å•é¡Œ**
   å¯¦ç¾ä¸€å€‹å®¢æˆ¶åˆ†ç¾¤ç³»çµ±ï¼š
   - æ•¸æ“šæ¸…ç†å’Œæ¨™æº–åŒ–
   - æœ€ä½³èšé¡æ•¸ç¢ºå®š
   - èšé¡çµæœåˆ†æ
   - å¯è¦–åŒ–å’Œå ±å‘Š

## å°æé†’ ğŸ’¡

1. æ•¸æ“šè™•ç†
   - ä»”ç´°æª¢æŸ¥æ•¸æ“šè³ªé‡
   - è™•ç†ç¼ºå¤±å€¼å’Œç•°å¸¸å€¼
   - é€²è¡Œé©ç•¶çš„ç‰¹å¾µå·¥ç¨‹
   - æ³¨æ„æ•¸æ“šæ´©éœ²å•é¡Œ

2. æ¨¡å‹é¸æ“‡
   - æ ¹æ“šå•é¡Œé¡å‹é¸æ“‡åˆé©çš„æ¨¡å‹
   - è€ƒæ…®æ¨¡å‹çš„è§£é‡‹æ€§
   - æ¬Šè¡¡æ¨¡å‹è¤‡é›œåº¦å’Œæ€§èƒ½
   - ä½¿ç”¨äº¤å‰é©—è­‰è©•ä¼°æ¨¡å‹

3. æ¨¡å‹å„ªåŒ–
   - ä½¿ç”¨ç¶²æ ¼æœç´¢æˆ–éš¨æ©Ÿæœç´¢èª¿åƒ
   - ç›£æ§éæ“¬åˆå’Œæ¬ æ“¬åˆ
   - è€ƒæ…®ä½¿ç”¨é›†æˆæ–¹æ³•
   - å®šæœŸæ›´æ–°å’Œç¶­è­·æ¨¡å‹

4. å¯¦è¸å»ºè­°
   - å¾ç°¡å–®æ¨¡å‹é–‹å§‹
   - å»ºç«‹å®Œæ•´çš„è©•ä¼°æŒ‡æ¨™
   - ä¿æŒä»£ç¢¼çš„å¯é‡ç”¨æ€§
   - è¨˜éŒ„å¯¦é©—éç¨‹å’Œçµæœ

[ä¸Šä¸€ç« ï¼šæ•¸æ“šåˆ†æèˆ‡è¦–è¦ºåŒ–](052_æ•¸æ“šåˆ†æèˆ‡è¦–è¦ºåŒ–.md) | [ä¸‹ä¸€ç« ï¼šæ·±åº¦å­¸ç¿’å…¥é–€](054_æ·±åº¦å­¸ç¿’å…¥é–€.md) 