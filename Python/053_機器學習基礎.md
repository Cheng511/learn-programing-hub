[上一章：數據分析與視覺化](052_數據分析與視覺化.md) | [下一章：深度學習入門](054_深度學習入門.md)

# Python 機器學習基礎 🤖

## 機器學習概述

機器學習是人工智能的一個子領域，它使計算機系統能夠通過經驗自動改進其性能。本章將介紹使用Python進行機器學習的基礎知識。

## 數據預處理

### 1. 數據加載和清理

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# 加載數據
def load_and_clean_data(file_path):
    # 讀取數據
    df = pd.read_csv(file_path)
    
    # 處理缺失值
    df = df.dropna()  # 或使用填充: df.fillna(method='ffill')
    
    # 處理異常值
    Q1 = df.quantile(0.25)
    Q3 = df.quantile(0.75)
    IQR = Q3 - Q1
    df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]
    
    return df

# 特徵工程
def prepare_features(df):
    # 數值特徵標準化
    scaler = StandardScaler()
    numeric_features = df.select_dtypes(include=['float64', 'int64']).columns
    df[numeric_features] = scaler.fit_transform(df[numeric_features])
    
    # 類別特徵編碼
    categorical_features = df.select_dtypes(include=['object']).columns
    df = pd.get_dummies(df, columns=categorical_features)
    
    return df
```

### 2. 數據分割

```python
# 分割訓練集和測試集
def split_data(X, y, test_size=0.2, random_state=42):
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    return X_train, X_test, y_train, y_test
```

## 監督學習

### 1. 線性回歸

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# 創建示例數據
np.random.seed(42)
X = np.random.rand(100, 1) * 10
y = 2 * X + 1 + np.random.randn(100, 1) * 0.5

# 訓練模型
model = LinearRegression()
model.fit(X, y)

# 預測
y_pred = model.predict(X)

# 評估
print(f'係數: {model.coef_[0][0]:.2f}')
print(f'截距: {model.intercept_[0]:.2f}')
print(f'R2分數: {r2_score(y, y_pred):.2f}')
print(f'均方誤差: {mean_squared_error(y, y_pred):.2f}')

# 視覺化
import matplotlib.pyplot as plt

plt.scatter(X, y, color='blue', label='實際數據')
plt.plot(X, y_pred, color='red', label='預測線')
plt.xlabel('X')
plt.ylabel('y')
plt.title('線性回歸示例')
plt.legend()
plt.show()
```

### 2. 邏輯回歸

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.datasets import make_classification

# 創建分類數據
X, y = make_classification(
    n_samples=100, n_features=2, n_redundant=0,
    n_clusters_per_class=1, random_state=42
)

# 分割數據
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 訓練模型
model = LogisticRegression()
model.fit(X_train, y_train)

# 預測
y_pred = model.predict(X_test)

# 評估
print('準確率:', accuracy_score(y_test, y_pred))
print('\n分類報告:\n', classification_report(y_test, y_pred))

# 視覺化決策邊界
def plot_decision_boundary(X, y, model):
    h = 0.02  # 網格步長
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.contourf(xx, yy, Z, alpha=0.4)
    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Logistic Regression Decision Boundary')
    plt.show()

plot_decision_boundary(X, y, model)
```

### 3. 決策樹

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree

# 創建和訓練決策樹
tree = DecisionTreeClassifier(max_depth=3, random_state=42)
tree.fit(X_train, y_train)

# 預測和評估
y_pred = tree.predict(X_test)
print('決策樹準確率:', accuracy_score(y_test, y_pred))

# 視覺化決策樹
plt.figure(figsize=(15,10))
plot_tree(tree, filled=True, feature_names=['Feature 1', 'Feature 2'])
plt.title('Decision Tree Visualization')
plt.show()
```

## 非監督學習

### 1. K-means聚類

```python
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# 創建聚類數據
X, y = make_blobs(n_samples=300, centers=4, random_state=42)

# 訓練K-means模型
kmeans = KMeans(n_clusters=4, random_state=42)
kmeans.fit(X)

# 獲取聚類標籤
labels = kmeans.labels_

# 視覺化聚類結果
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0],
           kmeans.cluster_centers_[:, 1],
           marker='x', s=200, linewidths=3,
           color='r', label='Centroids')
plt.title('K-means Clustering')
plt.legend()
plt.show()
```

### 2. 主成分分析 (PCA)

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits

# 加載數據
digits = load_digits()
X = digits.data
y = digits.target

# 執行PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# 視覺化結果
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')
plt.colorbar(scatter)
plt.title('PCA of Digits Dataset')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.show()

# 解釋方差比
print('解釋方差比:', pca.explained_variance_ratio_)
```

## 模型評估與優化

### 1. 交叉驗證

```python
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC

# 創建分類器
svc = SVC(kernel='rbf', random_state=42)

# 執行交叉驗證
scores = cross_val_score(svc, X, y, cv=5)

print('交叉驗證分數:', scores)
print('平均準確率: %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 2))
```

### 2. 網格搜索

```python
from sklearn.model_selection import GridSearchCV

# 定義參數網格
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['rbf', 'linear'],
    'gamma': ['scale', 'auto', 0.1, 1],
}

# 創建網格搜索對象
grid_search = GridSearchCV(SVC(), param_grid, cv=5)
grid_search.fit(X, y)

# 輸出最佳參數
print('最佳參數:', grid_search.best_params_)
print('最佳分數:', grid_search.best_score_)
```

## 實戰項目：房價預測

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# 加載數據
def load_house_data():
    # 示例數據
    np.random.seed(42)
    n_samples = 1000
    
    data = {
        'size': np.random.normal(150, 40, n_samples),
        'rooms': np.random.randint(1, 6, n_samples),
        'age': np.random.randint(0, 50, n_samples),
        'location_score': np.random.uniform(0, 10, n_samples),
        'price': np.zeros(n_samples)
    }
    
    # 生成價格
    data['price'] = (
        data['size'] * 1000 +
        data['rooms'] * 50000 -
        data['age'] * 1000 +
        data['location_score'] * 10000 +
        np.random.normal(0, 10000, n_samples)
    )
    
    return pd.DataFrame(data)

# 數據預處理
def preprocess_data(df):
    # 特徵縮放
    scaler = StandardScaler()
    features = ['size', 'rooms', 'age', 'location_score']
    df[features] = scaler.fit_transform(df[features])
    return df

# 主函數
def main():
    # 加載和預處理數據
    df = load_house_data()
    df = preprocess_data(df)
    
    # 準備特徵和目標
    X = df.drop('price', axis=1)
    y = df['price']
    
    # 分割數據
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # 訓練模型
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    # 預測和評估
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    print(f'均方誤差: {mse:.2f}')
    print(f'R2分數: {r2:.2f}')
    
    # 特徵重要性
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': model.feature_importances_
    })
    print('\n特徵重要性:')
    print(feature_importance.sort_values('importance', ascending=False))
    
    # 視覺化預測結果
    plt.figure(figsize=(10, 6))
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
             'r--', lw=2)
    plt.xlabel('實際價格')
    plt.ylabel('預測價格')
    plt.title('房價預測結果')
    plt.tight_layout()
    plt.show()

if __name__ == '__main__':
    main()
```

## 練習題

1. **二分類問題**
   實現一個信用卡欺詐檢測系統：
   - 數據預處理和特徵工程
   - 模型選擇和訓練
   - 模型評估和優化
   - 結果可視化

2. **回歸問題**
   開發一個股票價格預測模型：
   - 時間序列數據處理
   - 特徵創建
   - 模型訓練和評估
   - 預測結果分析

3. **聚類問題**
   實現一個客戶分群系統：
   - 數據清理和標準化
   - 最佳聚類數確定
   - 聚類結果分析
   - 可視化和報告

## 小提醒 💡

1. 數據處理
   - 仔細檢查數據質量
   - 處理缺失值和異常值
   - 進行適當的特徵工程
   - 注意數據洩露問題

2. 模型選擇
   - 根據問題類型選擇合適的模型
   - 考慮模型的解釋性
   - 權衡模型複雜度和性能
   - 使用交叉驗證評估模型

3. 模型優化
   - 使用網格搜索或隨機搜索調參
   - 監控過擬合和欠擬合
   - 考慮使用集成方法
   - 定期更新和維護模型

4. 實踐建議
   - 從簡單模型開始
   - 建立完整的評估指標
   - 保持代碼的可重用性
   - 記錄實驗過程和結果

[上一章：數據分析與視覺化](052_數據分析與視覺化.md) | [下一章：深度學習入門](054_深度學習入門.md) 