[ä¸Šä¸€ç« ï¼šé›²ç«¯æ•¸æ“šåº«æ•´åˆ](117_é›²ç«¯æ•¸æ“šåº«æ•´åˆ.md) | [ä¸‹ä¸€ç« ï¼šAIæ¨¡å‹å„ªåŒ–](119_AIæ¨¡å‹å„ªåŒ–.md)

# Python æ©Ÿå™¨å­¸ç¿’éƒ¨ç½² ğŸš€

## 1. æ¨¡å‹åºåˆ—åŒ–èˆ‡åŠ è¼‰

### 1.1 æ¨¡å‹ä¿å­˜èˆ‡åŠ è¼‰

```python
import joblib
import pickle
from typing import Any, Dict
import os

class ModelSerializer:
    @staticmethod
    def save_model(model: Any, path: str, format: str = 'joblib'):
        """ä¿å­˜æ¨¡å‹"""
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        if format == 'joblib':
            joblib.dump(model, path)
        elif format == 'pickle':
            with open(path, 'wb') as f:
                pickle.dump(model, f)
        else:
            raise ValueError(f"Unsupported format: {format}")
    
    @staticmethod
    def load_model(path: str, format: str = 'joblib') -> Any:
        """åŠ è¼‰æ¨¡å‹"""
        if format == 'joblib':
            return joblib.load(path)
        elif format == 'pickle':
            with open(path, 'rb') as f:
                return pickle.load(f)
        else:
            raise ValueError(f"Unsupported format: {format}")
```

### 1.2 æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶

```python
from datetime import datetime
import json
from typing import Optional

class ModelVersionControl:
    def __init__(self, base_path: str):
        self.base_path = base_path
        self.metadata_file = os.path.join(base_path, 'model_metadata.json')
        self.metadata = self._load_metadata()
    
    def save_model_version(self, model: Any, version: str, 
                          metrics: Dict, description: str = ""):
        """ä¿å­˜æ¨¡å‹ç‰ˆæœ¬"""
        model_path = os.path.join(self.base_path, f"model_v{version}.joblib")
        ModelSerializer.save_model(model, model_path)
        
        self.metadata['versions'][version] = {
            'path': model_path,
            'metrics': metrics,
            'description': description,
            'created_at': datetime.now().isoformat()
        }
        self._save_metadata()
    
    def get_model_version(self, version: str) -> Optional[Any]:
        """ç²å–ç‰¹å®šç‰ˆæœ¬çš„æ¨¡å‹"""
        if version in self.metadata['versions']:
            path = self.metadata['versions'][version]['path']
            return ModelSerializer.load_model(path)
        return None
```

## 2. REST API éƒ¨ç½²

### 2.1 FastAPI æœå‹™

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import numpy as np
from typing import List, Dict

class PredictionRequest(BaseModel):
    features: List[float]

class ModelService:
    def __init__(self, model_path: str):
        self.model = ModelSerializer.load_model(model_path)
        self.app = FastAPI()
        self.setup_routes()
    
    def setup_routes(self):
        """è¨­ç½®APIè·¯ç”±"""
        @self.app.post("/predict")
        async def predict(request: PredictionRequest):
            try:
                features = np.array(request.features).reshape(1, -1)
                prediction = self.model.predict(features)
                return {"prediction": prediction.tolist()}
            except Exception as e:
                raise HTTPException(status_code=400, detail=str(e))
        
        @self.app.get("/model/info")
        async def model_info():
            return {
                "model_type": type(self.model).__name__,
                "feature_count": self.model.n_features_in_
            }
```

### 2.2 æ¨¡å‹ç›£æ§

```python
import time
from datetime import datetime
from collections import deque

class ModelMonitor:
    def __init__(self, window_size: int = 1000):
        self.predictions = deque(maxlen=window_size)
        self.latencies = deque(maxlen=window_size)
        self.errors = deque(maxlen=window_size)
    
    def log_prediction(self, input_data: Any, prediction: Any, 
                      latency: float, error: Optional[str] = None):
        """è¨˜éŒ„é æ¸¬ä¿¡æ¯"""
        timestamp = datetime.now()
        self.predictions.append({
            'timestamp': timestamp,
            'input': input_data,
            'prediction': prediction,
            'latency': latency
        })
        
        self.latencies.append(latency)
        
        if error:
            self.errors.append({
                'timestamp': timestamp,
                'error': error,
                'input': input_data
            })
    
    def get_statistics(self) -> Dict:
        """ç²å–ç›£æ§çµ±è¨ˆä¿¡æ¯"""
        return {
            'avg_latency': np.mean(self.latencies),
            'max_latency': np.max(self.latencies),
            'error_rate': len(self.errors) / len(self.predictions),
            'total_predictions': len(self.predictions)
        }
```

## 3. å®¹å™¨åŒ–éƒ¨ç½²

### 3.1 Docker é…ç½®

```python
class DockerConfig:
    @staticmethod
    def generate_dockerfile(requirements_path: str, 
                          model_path: str,
                          port: int = 8000) -> str:
        """ç”ŸæˆDockerfile"""
        return f"""
FROM python:3.8-slim

WORKDIR /app

COPY {requirements_path} /app/requirements.txt
RUN pip install -r requirements.txt

COPY {model_path} /app/model/
COPY . /app/

EXPOSE {port}

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "{port}"]
"""
    
    @staticmethod
    def generate_compose_file(service_name: str, 
                            image_name: str,
                            port: int = 8000) -> str:
        """ç”Ÿæˆdocker-compose.yml"""
        return f"""
version: '3'
services:
  {service_name}:
    build: .
    image: {image_name}
    ports:
      - "{port}:{port}"
    volumes:
      - ./model:/app/model
    environment:
      - MODEL_PATH=/app/model/model.joblib
"""
```

### 3.2 Kubernetes éƒ¨ç½²

```python
from kubernetes import client, config
from typing import Dict, List

class KubernetesDeployer:
    def __init__(self):
        config.load_kube_config()
        self.api = client.AppsV1Api()
    
    def create_deployment(self, name: str, image: str, 
                         replicas: int = 3) -> Dict:
        """å‰µå»ºKuberneteséƒ¨ç½²"""
        deployment = client.V1Deployment(
            metadata=client.V1ObjectMeta(name=name),
            spec=client.V1DeploymentSpec(
                replicas=replicas,
                selector=client.V1LabelSelector(
                    match_labels={"app": name}
                ),
                template=client.V1PodTemplateSpec(
                    metadata=client.V1ObjectMeta(
                        labels={"app": name}
                    ),
                    spec=client.V1PodSpec(
                        containers=[
                            client.V1Container(
                                name=name,
                                image=image,
                                ports=[client.V1ContainerPort(container_port=8000)]
                            )
                        ]
                    )
                )
            )
        )
        
        return self.api.create_namespaced_deployment(
            body=deployment,
            namespace="default"
        )
```

## 4. æ¨¡å‹æœå‹™æ“´å±•

### 4.1 è² è¼‰å‡è¡¡

```python
from typing import List, Dict
import random

class LoadBalancer:
    def __init__(self, endpoints: List[str]):
        self.endpoints = endpoints
        self.current = 0
    
    def round_robin(self) -> str:
        """è¼ªè©¢ç®—æ³•"""
        endpoint = self.endpoints[self.current]
        self.current = (self.current + 1) % len(self.endpoints)
        return endpoint
    
    def random_select(self) -> str:
        """éš¨æ©Ÿé¸æ“‡"""
        return random.choice(self.endpoints)
    
    def weighted_select(self, weights: Dict[str, float]) -> str:
        """åŠ æ¬Šé¸æ“‡"""
        total = sum(weights.values())
        r = random.uniform(0, total)
        upto = 0
        for endpoint, weight in weights.items():
            if upto + weight >= r:
                return endpoint
            upto += weight
        return self.endpoints[-1]
```

### 4.2 è‡ªå‹•æ“´å±•

```python
import psutil
from typing import Dict, Optional

class AutoScaler:
    def __init__(self, min_replicas: int = 1, max_replicas: int = 10,
                 cpu_threshold: float = 70.0):
        self.min_replicas = min_replicas
        self.max_replicas = max_replicas
        self.cpu_threshold = cpu_threshold
        self.current_replicas = min_replicas
    
    def check_scaling_needs(self) -> Optional[int]:
        """æª¢æŸ¥æ˜¯å¦éœ€è¦æ“´å±•"""
        cpu_percent = psutil.cpu_percent()
        
        if cpu_percent > self.cpu_threshold and \
           self.current_replicas < self.max_replicas:
            return self.current_replicas + 1
        elif cpu_percent < self.cpu_threshold / 2 and \
             self.current_replicas > self.min_replicas:
            return self.current_replicas - 1
        
        return None
    
    def scale(self, new_replicas: int):
        """åŸ·è¡Œæ“´å±•æ“ä½œ"""
        self.current_replicas = new_replicas
        # å¯¦ç¾å…·é«”çš„æ“´å±•é‚è¼¯
```

## ç·´ç¿’é¡Œ ğŸƒ

1. å¯¦ç¾ä¸€å€‹å®Œæ•´çš„æ¨¡å‹éƒ¨ç½²æµç¨‹ï¼ŒåŒ…æ‹¬åºåˆ—åŒ–ã€APIæœå‹™å’Œå®¹å™¨åŒ–ã€‚
2. é–‹ç™¼ä¸€å€‹æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶ç³»çµ±ï¼Œæ”¯æŒå›æ»¾å’ŒA/Bæ¸¬è©¦ã€‚
3. è¨­è¨ˆä¸€å€‹è‡ªå‹•æ“´å±•ç³»çµ±ï¼Œæ ¹æ“šè² è¼‰è‡ªå‹•èª¿æ•´æœå‹™å¯¦ä¾‹æ•¸ã€‚
4. å¯¦ç¾ä¸€å€‹æ¨¡å‹ç›£æ§å„€è¡¨æ¿ï¼Œé¡¯ç¤ºæ€§èƒ½æŒ‡æ¨™å’Œé æ¸¬çµ±è¨ˆã€‚
5. å‰µå»ºä¸€å€‹å¤šæ¨¡å‹é›†æˆæœå‹™ï¼Œæ”¯æŒä¸åŒé¡å‹çš„æ¨¡å‹éƒ¨ç½²ã€‚

## å°çµ ğŸ“

- å­¸ç¿’äº†æ¨¡å‹åºåˆ—åŒ–å’Œç‰ˆæœ¬æ§åˆ¶
- æŒæ¡äº†REST APIéƒ¨ç½²æ–¹æ³•
- ç†è§£äº†å®¹å™¨åŒ–éƒ¨ç½²æµç¨‹
- å­¸æœƒäº†è² è¼‰å‡è¡¡å’Œè‡ªå‹•æ“´å±•
- äº†è§£äº†æ¨¡å‹ç›£æ§å’Œç¶­è­·

## å»¶ä¼¸é–±è®€ ğŸ“š

1. MLOps: Machine Learning Operations
2. Docker and Kubernetes in Action
3. FastAPI for Machine Learning
4. Scalable Machine Learning Deployment
5. Model Monitoring Best Practices

[ä¸Šä¸€ç« ï¼šé›²ç«¯æ•¸æ“šåº«æ•´åˆ](117_é›²ç«¯æ•¸æ“šåº«æ•´åˆ.md) | [ä¸‹ä¸€ç« ï¼šAIæ¨¡å‹å„ªåŒ–](119_AIæ¨¡å‹å„ªåŒ–.md) 