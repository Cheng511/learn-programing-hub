[ä¸Šä¸€ç« ï¼šNoSQLæ•¸æ“šåº«æ‡‰ç”¨](114_NoSQLæ•¸æ“šåº«æ‡‰ç”¨.md) | [ä¸‹ä¸€ç« ï¼šåˆ†å¸ƒå¼è¨ˆç®—](116_åˆ†å¸ƒå¼è¨ˆç®—.md)

# Python å¤§æ•¸æ“šè™•ç†åŸºç¤ ğŸŒŸ

## 1. æ•¸æ“šè™•ç†æ¡†æ¶

### 1.1 Pandas å¤§æ•¸æ“šè™•ç†

```python
import pandas as pd
import numpy as np
from typing import List, Dict, Optional

class BigDataProcessor:
    def __init__(self, chunk_size: int = 10000):
        self.chunk_size = chunk_size
    
    def process_large_csv(self, file_path: str, process_func) -> pd.DataFrame:
        """åˆ†å¡Šè™•ç†å¤§å‹CSVæ–‡ä»¶"""
        chunks = []
        for chunk in pd.read_csv(file_path, chunksize=self.chunk_size):
            processed_chunk = process_func(chunk)
            chunks.append(processed_chunk)
        return pd.concat(chunks)
    
    def parallel_process(self, df: pd.DataFrame, func, n_jobs: int = -1):
        """ä¸¦è¡Œè™•ç†æ•¸æ“šæ¡†"""
        from joblib import Parallel, delayed
        
        # å°‡æ•¸æ“šæ¡†åˆ†å‰²æˆå¤šå€‹éƒ¨åˆ†
        splits = np.array_split(df, n_jobs)
        
        # ä¸¦è¡Œè™•ç†
        results = Parallel(n_jobs=n_jobs)(
            delayed(func)(split) for split in splits
        )
        
        return pd.concat(results)
```

### 1.2 Dask åˆ†å¸ƒå¼è¨ˆç®—

```python
import dask.dataframe as dd
from dask.distributed import Client

class DaskProcessor:
    def __init__(self, client: Optional[Client] = None):
        self.client = client or Client()
    
    def load_large_dataset(self, file_pattern: str) -> dd.DataFrame:
        """è¼‰å…¥å¤§å‹æ•¸æ“šé›†"""
        return dd.read_csv(file_pattern)
    
    def process_dataset(self, ddf: dd.DataFrame, func) -> dd.DataFrame:
        """è™•ç†åˆ†å¸ƒå¼æ•¸æ“šæ¡†"""
        return ddf.map_partitions(func)
    
    def aggregate_results(self, ddf: dd.DataFrame, group_by: str, agg_dict: Dict):
        """èšåˆè¨ˆç®—"""
        return ddf.groupby(group_by).agg(agg_dict).compute()
```

## 2. æµå¼è™•ç†

### 2.1 å¯¦æ™‚æ•¸æ“šè™•ç†

```python
from typing import Generator, Any
import time

class StreamProcessor:
    def __init__(self, window_size: int = 1000):
        self.window_size = window_size
        self.buffer = []
    
    def process_stream(self, data_stream: Generator[Any, None, None]):
        """è™•ç†æ•¸æ“šæµ"""
        for item in data_stream:
            self.buffer.append(item)
            
            if len(self.buffer) >= self.window_size:
                self._process_window()
                self.buffer = []
    
    def _process_window(self):
        """è™•ç†æ•¸æ“šçª—å£"""
        # åœ¨é€™è£¡å¯¦ç¾å…·é«”çš„çª—å£è™•ç†é‚è¼¯
        result = pd.DataFrame(self.buffer)
        self._save_results(result)
    
    def _save_results(self, result: pd.DataFrame):
        """ä¿å­˜è™•ç†çµæœ"""
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        result.to_parquet(f'results_{timestamp}.parquet')
```

### 2.2 Apache Kafka æ•´åˆ

```python
from kafka import KafkaConsumer, KafkaProducer
import json

class KafkaHandler:
    def __init__(self, bootstrap_servers: List[str]):
        self.bootstrap_servers = bootstrap_servers
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda x: json.dumps(x).encode('utf-8')
        )
        
    def create_consumer(self, topic: str) -> KafkaConsumer:
        """å‰µå»ºæ¶ˆè²»è€…"""
        return KafkaConsumer(
            topic,
            bootstrap_servers=self.bootstrap_servers,
            value_deserializer=lambda x: json.loads(x.decode('utf-8'))
        )
    
    def process_messages(self, topic: str, processor):
        """è™•ç†Kafkaæ¶ˆæ¯"""
        consumer = self.create_consumer(topic)
        for message in consumer:
            processed_data = processor(message.value)
            self.producer.send(f'{topic}_processed', processed_data)
```

## 3. æ•¸æ“šå­˜å„²å„ªåŒ–

### 3.1 Parquet æ–‡ä»¶æ ¼å¼

```python
import pyarrow as pa
import pyarrow.parquet as pq

class ParquetManager:
    def __init__(self, compression: str = 'snappy'):
        self.compression = compression
    
    def save_to_parquet(self, df: pd.DataFrame, file_path: str):
        """ä¿å­˜ç‚ºParquetæ ¼å¼"""
        table = pa.Table.from_pandas(df)
        pq.write_table(
            table,
            file_path,
            compression=self.compression
        )
    
    def read_from_parquet(self, file_path: str) -> pd.DataFrame:
        """è®€å–Parquetæ–‡ä»¶"""
        return pd.read_parquet(file_path)
    
    def append_to_parquet(self, df: pd.DataFrame, file_path: str):
        """è¿½åŠ æ•¸æ“šåˆ°ç¾æœ‰Parquetæ–‡ä»¶"""
        existing_df = self.read_from_parquet(file_path)
        combined_df = pd.concat([existing_df, df])
        self.save_to_parquet(combined_df, file_path)
```

### 3.2 æ•¸æ“šå£“ç¸®èˆ‡åˆ†å€

```python
class DataPartitioner:
    def __init__(self, base_path: str):
        self.base_path = base_path
    
    def partition_by_date(self, df: pd.DataFrame, date_column: str):
        """æŒ‰æ—¥æœŸåˆ†å€å­˜å„²æ•¸æ“š"""
        for date, group in df.groupby(pd.Grouper(key=date_column, freq='D')):
            path = f"{self.base_path}/year={date.year}/month={date.month}/day={date.day}"
            os.makedirs(path, exist_ok=True)
            group.to_parquet(f"{path}/data.parquet")
    
    def read_partition(self, year: int, month: int, day: int) -> pd.DataFrame:
        """è®€å–ç‰¹å®šåˆ†å€çš„æ•¸æ“š"""
        path = f"{self.base_path}/year={year}/month={month}/day={day}/data.parquet"
        return pd.read_parquet(path)
```

## 4. æ€§èƒ½å„ªåŒ–

### 4.1 å…§å­˜å„ªåŒ–

```python
class MemoryOptimizer:
    @staticmethod
    def reduce_memory_usage(df: pd.DataFrame) -> pd.DataFrame:
        """å„ªåŒ–æ•¸æ“šæ¡†çš„å…§å­˜ä½¿ç”¨"""
        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
        
        for col in df.columns:
            col_type = df[col].dtype
            
            if col_type in numerics:
                c_min = df[col].min()
                c_max = df[col].max()
                
                if str(col_type).startswith('int'):
                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                        df[col] = df[col].astype(np.int8)
                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                        df[col] = df[col].astype(np.int16)
                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                        df[col] = df[col].astype(np.int32)
                else:
                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                        df[col] = df[col].astype(np.float16)
                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                        df[col] = df[col].astype(np.float32)
        
        return df
```

### 4.2 ä¸¦è¡Œè™•ç†å„ªåŒ–

```python
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from typing import Callable, List, Any

class ParallelProcessor:
    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers
    
    def process_with_threads(self, func: Callable, items: List[Any]) -> List[Any]:
        """ä½¿ç”¨ç·šç¨‹æ± è™•ç†ä»»å‹™"""
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            results = list(executor.map(func, items))
        return results
    
    def process_with_processes(self, func: Callable, items: List[Any]) -> List[Any]:
        """ä½¿ç”¨é€²ç¨‹æ± è™•ç†ä»»å‹™"""
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            results = list(executor.map(func, items))
        return results
```

## ç·´ç¿’é¡Œ ğŸƒ

1. å¯¦ç¾ä¸€å€‹å¤§å‹CSVæ–‡ä»¶çš„åˆ†å¡Šè™•ç†ç³»çµ±ã€‚
2. ä½¿ç”¨Daskè™•ç†ä¸€å€‹è¶…éå…§å­˜å¤§å°çš„æ•¸æ“šé›†ã€‚
3. é–‹ç™¼ä¸€å€‹å¯¦æ™‚æ•¸æ“šæµè™•ç†ç®¡é“ã€‚
4. è¨­è¨ˆä¸€å€‹é«˜æ•ˆçš„æ•¸æ“šå­˜å„²å’Œæª¢ç´¢ç³»çµ±ã€‚
5. å¯¦ç¾ä¸€å€‹å¤šé€²ç¨‹æ•¸æ“šè™•ç†æ¡†æ¶ã€‚

## å°çµ ğŸ“

- å­¸ç¿’äº†å¤§æ•¸æ“šè™•ç†çš„åŸºæœ¬æ¡†æ¶
- æŒæ¡äº†æµå¼æ•¸æ“šè™•ç†æ–¹æ³•
- ç†è§£äº†æ•¸æ“šå­˜å„²å„ªåŒ–æŠ€è¡“
- å­¸æœƒäº†æ€§èƒ½å„ªåŒ–ç­–ç•¥
- äº†è§£äº†ä¸¦è¡Œè™•ç†çš„å¯¦ç¾æ–¹å¼

## å»¶ä¼¸é–±è®€ ğŸ“š

1. Python for Data Analysis
2. Learning Apache Kafka
3. Dask: Parallel Computing with Python
4. High Performance Python
5. Big Data Processing with Python

[ä¸Šä¸€ç« ï¼šNoSQLæ•¸æ“šåº«æ‡‰ç”¨](114_NoSQLæ•¸æ“šåº«æ‡‰ç”¨.md) | [ä¸‹ä¸€ç« ï¼šåˆ†å¸ƒå¼è¨ˆç®—](116_åˆ†å¸ƒå¼è¨ˆç®—.md) 