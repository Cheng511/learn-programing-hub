[上一章：NoSQL數據庫應用](114_NoSQL數據庫應用.md) | [下一章：分布式計算](116_分布式計算.md)

# Python 大數據處理基礎 🌟

## 1. 數據處理框架

### 1.1 Pandas 大數據處理

```python
import pandas as pd
import numpy as np
from typing import List, Dict, Optional

class BigDataProcessor:
    def __init__(self, chunk_size: int = 10000):
        self.chunk_size = chunk_size
    
    def process_large_csv(self, file_path: str, process_func) -> pd.DataFrame:
        """分塊處理大型CSV文件"""
        chunks = []
        for chunk in pd.read_csv(file_path, chunksize=self.chunk_size):
            processed_chunk = process_func(chunk)
            chunks.append(processed_chunk)
        return pd.concat(chunks)
    
    def parallel_process(self, df: pd.DataFrame, func, n_jobs: int = -1):
        """並行處理數據框"""
        from joblib import Parallel, delayed
        
        # 將數據框分割成多個部分
        splits = np.array_split(df, n_jobs)
        
        # 並行處理
        results = Parallel(n_jobs=n_jobs)(
            delayed(func)(split) for split in splits
        )
        
        return pd.concat(results)
```

### 1.2 Dask 分布式計算

```python
import dask.dataframe as dd
from dask.distributed import Client

class DaskProcessor:
    def __init__(self, client: Optional[Client] = None):
        self.client = client or Client()
    
    def load_large_dataset(self, file_pattern: str) -> dd.DataFrame:
        """載入大型數據集"""
        return dd.read_csv(file_pattern)
    
    def process_dataset(self, ddf: dd.DataFrame, func) -> dd.DataFrame:
        """處理分布式數據框"""
        return ddf.map_partitions(func)
    
    def aggregate_results(self, ddf: dd.DataFrame, group_by: str, agg_dict: Dict):
        """聚合計算"""
        return ddf.groupby(group_by).agg(agg_dict).compute()
```

## 2. 流式處理

### 2.1 實時數據處理

```python
from typing import Generator, Any
import time

class StreamProcessor:
    def __init__(self, window_size: int = 1000):
        self.window_size = window_size
        self.buffer = []
    
    def process_stream(self, data_stream: Generator[Any, None, None]):
        """處理數據流"""
        for item in data_stream:
            self.buffer.append(item)
            
            if len(self.buffer) >= self.window_size:
                self._process_window()
                self.buffer = []
    
    def _process_window(self):
        """處理數據窗口"""
        # 在這裡實現具體的窗口處理邏輯
        result = pd.DataFrame(self.buffer)
        self._save_results(result)
    
    def _save_results(self, result: pd.DataFrame):
        """保存處理結果"""
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        result.to_parquet(f'results_{timestamp}.parquet')
```

### 2.2 Apache Kafka 整合

```python
from kafka import KafkaConsumer, KafkaProducer
import json

class KafkaHandler:
    def __init__(self, bootstrap_servers: List[str]):
        self.bootstrap_servers = bootstrap_servers
        self.producer = KafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda x: json.dumps(x).encode('utf-8')
        )
        
    def create_consumer(self, topic: str) -> KafkaConsumer:
        """創建消費者"""
        return KafkaConsumer(
            topic,
            bootstrap_servers=self.bootstrap_servers,
            value_deserializer=lambda x: json.loads(x.decode('utf-8'))
        )
    
    def process_messages(self, topic: str, processor):
        """處理Kafka消息"""
        consumer = self.create_consumer(topic)
        for message in consumer:
            processed_data = processor(message.value)
            self.producer.send(f'{topic}_processed', processed_data)
```

## 3. 數據存儲優化

### 3.1 Parquet 文件格式

```python
import pyarrow as pa
import pyarrow.parquet as pq

class ParquetManager:
    def __init__(self, compression: str = 'snappy'):
        self.compression = compression
    
    def save_to_parquet(self, df: pd.DataFrame, file_path: str):
        """保存為Parquet格式"""
        table = pa.Table.from_pandas(df)
        pq.write_table(
            table,
            file_path,
            compression=self.compression
        )
    
    def read_from_parquet(self, file_path: str) -> pd.DataFrame:
        """讀取Parquet文件"""
        return pd.read_parquet(file_path)
    
    def append_to_parquet(self, df: pd.DataFrame, file_path: str):
        """追加數據到現有Parquet文件"""
        existing_df = self.read_from_parquet(file_path)
        combined_df = pd.concat([existing_df, df])
        self.save_to_parquet(combined_df, file_path)
```

### 3.2 數據壓縮與分區

```python
class DataPartitioner:
    def __init__(self, base_path: str):
        self.base_path = base_path
    
    def partition_by_date(self, df: pd.DataFrame, date_column: str):
        """按日期分區存儲數據"""
        for date, group in df.groupby(pd.Grouper(key=date_column, freq='D')):
            path = f"{self.base_path}/year={date.year}/month={date.month}/day={date.day}"
            os.makedirs(path, exist_ok=True)
            group.to_parquet(f"{path}/data.parquet")
    
    def read_partition(self, year: int, month: int, day: int) -> pd.DataFrame:
        """讀取特定分區的數據"""
        path = f"{self.base_path}/year={year}/month={month}/day={day}/data.parquet"
        return pd.read_parquet(path)
```

## 4. 性能優化

### 4.1 內存優化

```python
class MemoryOptimizer:
    @staticmethod
    def reduce_memory_usage(df: pd.DataFrame) -> pd.DataFrame:
        """優化數據框的內存使用"""
        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
        
        for col in df.columns:
            col_type = df[col].dtype
            
            if col_type in numerics:
                c_min = df[col].min()
                c_max = df[col].max()
                
                if str(col_type).startswith('int'):
                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                        df[col] = df[col].astype(np.int8)
                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                        df[col] = df[col].astype(np.int16)
                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                        df[col] = df[col].astype(np.int32)
                else:
                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                        df[col] = df[col].astype(np.float16)
                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                        df[col] = df[col].astype(np.float32)
        
        return df
```

### 4.2 並行處理優化

```python
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from typing import Callable, List, Any

class ParallelProcessor:
    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers
    
    def process_with_threads(self, func: Callable, items: List[Any]) -> List[Any]:
        """使用線程池處理任務"""
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            results = list(executor.map(func, items))
        return results
    
    def process_with_processes(self, func: Callable, items: List[Any]) -> List[Any]:
        """使用進程池處理任務"""
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            results = list(executor.map(func, items))
        return results
```

## 練習題 🏃

1. 實現一個大型CSV文件的分塊處理系統。
2. 使用Dask處理一個超過內存大小的數據集。
3. 開發一個實時數據流處理管道。
4. 設計一個高效的數據存儲和檢索系統。
5. 實現一個多進程數據處理框架。

## 小結 📝

- 學習了大數據處理的基本框架
- 掌握了流式數據處理方法
- 理解了數據存儲優化技術
- 學會了性能優化策略
- 了解了並行處理的實現方式

## 延伸閱讀 📚

1. Python for Data Analysis
2. Learning Apache Kafka
3. Dask: Parallel Computing with Python
4. High Performance Python
5. Big Data Processing with Python

[上一章：NoSQL數據庫應用](114_NoSQL數據庫應用.md) | [下一章：分布式計算](116_分布式計算.md) 