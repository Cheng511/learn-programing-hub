[ä¸Šä¸€ç« ï¼šå¤§è¦æ¨¡æ•¸æ“šæµè™•ç†](129_å¤§è¦æ¨¡æ•¸æ“šæµè™•ç†.md)

# Python ç³»çµ±å¯è§€æ¸¬æ€§ ğŸ”

## 1. æŒ‡æ¨™æ”¶é›†

### 1.1 æŒ‡æ¨™æ”¶é›†å™¨

```python
from typing import Dict, List, Any
import time
from dataclasses import dataclass
import prometheus_client as prom
import threading

@dataclass
class Metric:
    """æŒ‡æ¨™å®šç¾©"""
    name: str
    description: str
    type: str
    labels: List[str] = None

class MetricsCollector:
    def __init__(self):
        self.metrics: Dict[str, Any] = {}
        self.lock = threading.Lock()
    
    def create_counter(self, metric: Metric):
        """å‰µå»ºè¨ˆæ•¸å™¨"""
        with self.lock:
            if metric.name not in self.metrics:
                self.metrics[metric.name] = prom.Counter(
                    metric.name,
                    metric.description,
                    metric.labels
                )
    
    def create_gauge(self, metric: Metric):
        """å‰µå»ºé‡è¡¨"""
        with self.lock:
            if metric.name not in self.metrics:
                self.metrics[metric.name] = prom.Gauge(
                    metric.name,
                    metric.description,
                    metric.labels
                )
    
    def create_histogram(self, metric: Metric,
                        buckets: List[float] = None):
        """å‰µå»ºç›´æ–¹åœ–"""
        with self.lock:
            if metric.name not in self.metrics:
                self.metrics[metric.name] = prom.Histogram(
                    metric.name,
                    metric.description,
                    metric.labels,
                    buckets=buckets
                )
    
    def increment_counter(self, name: str,
                         value: float = 1,
                         labels: Dict[str, str] = None):
        """å¢åŠ è¨ˆæ•¸å™¨"""
        if name in self.metrics:
            if labels:
                self.metrics[name].labels(**labels).inc(value)
            else:
                self.metrics[name].inc(value)
    
    def set_gauge(self, name: str,
                  value: float,
                  labels: Dict[str, str] = None):
        """è¨­ç½®é‡è¡¨å€¼"""
        if name in self.metrics:
            if labels:
                self.metrics[name].labels(**labels).set(value)
            else:
                self.metrics[name].set(value)
    
    def observe_histogram(self, name: str,
                         value: float,
                         labels: Dict[str, str] = None):
        """è§€å¯Ÿç›´æ–¹åœ–å€¼"""
        if name in self.metrics:
            if labels:
                self.metrics[name].labels(**labels).observe(value)
            else:
                self.metrics[name].observe(value)
```

### 1.2 æ€§èƒ½æŒ‡æ¨™

```python
import psutil
import os
from typing import Dict

class SystemMetrics:
    def __init__(self, collector: MetricsCollector):
        self.collector = collector
        self.setup_metrics()
    
    def setup_metrics(self):
        """è¨­ç½®ç³»çµ±æŒ‡æ¨™"""
        self.collector.create_gauge(Metric(
            name='system_cpu_usage',
            description='CPU usage percentage',
            type='gauge'
        ))
        
        self.collector.create_gauge(Metric(
            name='system_memory_usage',
            description='Memory usage percentage',
            type='gauge'
        ))
        
        self.collector.create_gauge(Metric(
            name='system_disk_usage',
            description='Disk usage percentage',
            type='gauge',
            labels=['path']
        ))
    
    def collect_metrics(self):
        """æ”¶é›†ç³»çµ±æŒ‡æ¨™"""
        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent()
        self.collector.set_gauge('system_cpu_usage', cpu_percent)
        
        # å…§å­˜ä½¿ç”¨ç‡
        memory = psutil.virtual_memory()
        self.collector.set_gauge('system_memory_usage', memory.percent)
        
        # ç£ç›¤ä½¿ç”¨ç‡
        for partition in psutil.disk_partitions():
            try:
                usage = psutil.disk_usage(partition.mountpoint)
                self.collector.set_gauge(
                    'system_disk_usage',
                    usage.percent,
                    {'path': partition.mountpoint}
                )
            except Exception:
                continue
```

## 2. æ—¥èªŒç®¡ç†

### 2.1 çµæ§‹åŒ–æ—¥èªŒ

```python
import logging
import json
from datetime import datetime
from typing import Any, Optional

class StructuredLogger:
    def __init__(self, name: str,
                 level: int = logging.INFO):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(level)
        
        # æ·»åŠ æ§åˆ¶å°è™•ç†å™¨
        handler = logging.StreamHandler()
        handler.setFormatter(
            logging.Formatter('%(message)s')
        )
        self.logger.addHandler(handler)
    
    def log(self, level: int,
            message: str,
            **kwargs):
        """è¨˜éŒ„çµæ§‹åŒ–æ—¥èªŒ"""
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'level': logging.getLevelName(level),
            'message': message,
            **kwargs
        }
        
        self.logger.log(
            level,
            json.dumps(log_entry)
        )
    
    def info(self, message: str, **kwargs):
        self.log(logging.INFO, message, **kwargs)
    
    def error(self, message: str, **kwargs):
        self.log(logging.ERROR, message, **kwargs)
    
    def warning(self, message: str, **kwargs):
        self.log(logging.WARNING, message, **kwargs)
    
    def debug(self, message: str, **kwargs):
        self.log(logging.DEBUG, message, **kwargs)
```

### 2.2 æ—¥èªŒèšåˆ

```python
import aiohttp
import asyncio
from typing import List

class LogAggregator:
    def __init__(self, elasticsearch_url: str):
        self.elasticsearch_url = elasticsearch_url
        self.buffer: List[Dict] = []
        self.buffer_size = 100
        self.flush_interval = 5  # ç§’
    
    async def setup(self):
        """è¨­ç½®æ—¥èªŒèšåˆå™¨"""
        self.session = aiohttp.ClientSession()
        asyncio.create_task(self._periodic_flush())
    
    async def add_log(self, log_entry: Dict):
        """æ·»åŠ æ—¥èªŒ"""
        self.buffer.append(log_entry)
        
        if len(self.buffer) >= self.buffer_size:
            await self.flush()
    
    async def flush(self):
        """åˆ·æ–°æ—¥èªŒåˆ°Elasticsearch"""
        if not self.buffer:
            return
        
        bulk_data = []
        for entry in self.buffer:
            bulk_data.extend([
                {'index': {'_index': 'logs'}},
                entry
            ])
        
        try:
            async with self.session.post(
                f"{self.elasticsearch_url}/_bulk",
                json=bulk_data,
                headers={'Content-Type': 'application/x-ndjson'}
            ) as response:
                if response.status >= 400:
                    print(f"Error flushing logs: {await response.text()}")
        except Exception as e:
            print(f"Error sending logs: {str(e)}")
        finally:
            self.buffer.clear()
    
    async def _periodic_flush(self):
        """å®šæœŸåˆ·æ–°æ—¥èªŒ"""
        while True:
            await asyncio.sleep(self.flush_interval)
            await self.flush()
```

## 3. åˆ†å¸ƒå¼è¿½è¸ª

### 3.1 è¿½è¸ªç³»çµ±

```python
from opentelemetry import trace
from opentelemetry.trace import Status, StatusCode
from opentelemetry.trace.span import Span
from contextlib import contextmanager
import time

class TracingSystem:
    def __init__(self, service_name: str):
        self.tracer = trace.get_tracer(service_name)
    
    @contextmanager
    def start_span(self, name: str,
                   attributes: Dict[str, str] = None):
        """é–‹å§‹ä¸€å€‹è¿½è¸ªç¯„åœ"""
        with self.tracer.start_as_current_span(
            name,
            attributes=attributes
        ) as span:
            try:
                yield span
            except Exception as e:
                span.set_status(Status(StatusCode.ERROR))
                span.record_exception(e)
                raise
    
    def add_event(self, span: Span,
                  name: str,
                  attributes: Dict[str, str] = None):
        """æ·»åŠ äº‹ä»¶"""
        span.add_event(
            name,
            attributes=attributes,
            timestamp=time.time_ns()
        )
    
    def set_attribute(self, span: Span,
                     key: str,
                     value: str):
        """è¨­ç½®å±¬æ€§"""
        span.set_attribute(key, value)
```

### 3.2 è¿½è¸ªä¸­é–“ä»¶

```python
from fastapi import Request
from opentelemetry.trace import SpanKind
import asyncio

class TracingMiddleware:
    def __init__(self, app, tracer: TracingSystem):
        self.app = app
        self.tracer = tracer
    
    async def __call__(self, request: Request, call_next):
        with self.tracer.start_span(
            f"{request.method} {request.url.path}",
            {
                'http.method': request.method,
                'http.url': str(request.url),
                'http.scheme': request.url.scheme
            }
        ) as span:
            try:
                response = await call_next(request)
                span.set_attribute(
                    'http.status_code',
                    response.status_code
                )
                return response
            except Exception as e:
                span.record_exception(e)
                raise
```

## ç·´ç¿’é¡Œ ğŸƒ

1. å¯¦ç¾ä¸€å€‹å®Œæ•´çš„æŒ‡æ¨™æ”¶é›†ç³»çµ±ã€‚
2. é–‹ç™¼ä¸€å€‹åˆ†å¸ƒå¼æ—¥èªŒèšåˆå™¨ã€‚
3. è¨­è¨ˆä¸€å€‹è¿½è¸ªç³»çµ±çš„å¯è¦–åŒ–ç•Œé¢ã€‚
4. å¯¦ç¾è‡ªå®šç¾©çš„ç›£æ§å‘Šè­¦è¦å‰‡ã€‚
5. å‰µå»ºä¸€å€‹ç³»çµ±å¥åº·æª¢æŸ¥å„€è¡¨æ¿ã€‚

## å°çµ ğŸ“

- å­¸ç¿’äº†æŒ‡æ¨™æ”¶é›†çš„æ–¹æ³•
- æŒæ¡äº†æ—¥èªŒç®¡ç†æŠ€è¡“
- ç†è§£äº†åˆ†å¸ƒå¼è¿½è¸ªåŸç†
- å­¸æœƒäº†ç›£æ§ç³»çµ±å¯¦ç¾
- äº†è§£äº†å¯è§€æ¸¬æ€§æœ€ä½³å¯¦è¸

## å»¶ä¼¸é–±è®€ ğŸ“š

1. Observability Engineering
2. Prometheus Documentation
3. OpenTelemetry Guide
4. Elastic Stack Documentation
5. SRE Workbook

[ä¸Šä¸€ç« ï¼šå¤§è¦æ¨¡æ•¸æ“šæµè™•ç†](129_å¤§è¦æ¨¡æ•¸æ“šæµè™•ç†.md) 