[ä¸Šä¸€ç« ï¼šå¤šç·šç¨‹é€²éš](044_å¤šç·šç¨‹é€²éš.md) | [ä¸‹ä¸€ç« ï¼šGUIç¨‹å¼è¨­è¨ˆåŸºç¤](046_GUIç¨‹å¼è¨­è¨ˆåŸºç¤.md)

# Python å¤šé€²ç¨‹ç·¨ç¨‹ ğŸš€

## é€²ç¨‹åŸºç¤

### 1. å‰µå»ºé€²ç¨‹

```python
import multiprocessing
import time
import os

def worker():
    """å·¥ä½œé€²ç¨‹å‡½æ•¸"""
    print(f"é€²ç¨‹ {os.getpid()} é–‹å§‹å·¥ä½œ")
    time.sleep(2)
    print(f"é€²ç¨‹ {os.getpid()} å®Œæˆå·¥ä½œ")

if __name__ == '__main__':
    # å‰µå»ºé€²ç¨‹
    process = multiprocessing.Process(target=worker)
    process.start()
    process.join()

    # å‰µå»ºå¤šå€‹é€²ç¨‹
    processes = []
    for _ in range(3):
        p = multiprocessing.Process(target=worker)
        processes.append(p)
        p.start()

    # ç­‰å¾…æ‰€æœ‰é€²ç¨‹å®Œæˆ
    for p in processes:
        p.join()
```

### 2. é€²ç¨‹é¡

```python
class WorkerProcess(multiprocessing.Process):
    def __init__(self, name):
        super().__init__()
        self.name = name
    
    def run(self):
        """é€²ç¨‹åŸ·è¡Œçš„æ–¹æ³•"""
        print(f"é€²ç¨‹ {self.name} (PID: {os.getpid()}) é–‹å§‹å·¥ä½œ")
        time.sleep(2)
        print(f"é€²ç¨‹ {self.name} (PID: {os.getpid()}) å®Œæˆå·¥ä½œ")

if __name__ == '__main__':
    # ä½¿ç”¨é€²ç¨‹é¡
    worker = WorkerProcess("Worker-1")
    worker.start()
    worker.join()
```

## é€²ç¨‹é–“é€šä¿¡

### 1. ç®¡é“é€šä¿¡

```python
from multiprocessing import Pipe

def sender(conn):
    """ç™¼é€æ•¸æ“šçš„é€²ç¨‹"""
    conn.send(['Hello', 42, {'key': 'value'}])
    conn.close()

def receiver(conn):
    """æ¥æ”¶æ•¸æ“šçš„é€²ç¨‹"""
    data = conn.recv()
    print(f"æ¥æ”¶åˆ°æ•¸æ“š: {data}")
    conn.close()

if __name__ == '__main__':
    # å‰µå»ºç®¡é“
    parent_conn, child_conn = Pipe()

    # å‰µå»ºç™¼é€å’Œæ¥æ”¶é€²ç¨‹
    p1 = multiprocessing.Process(target=sender, args=(parent_conn,))
    p2 = multiprocessing.Process(target=receiver, args=(child_conn,))

    p1.start()
    p2.start()

    p1.join()
    p2.join()
```

### 2. éšŠåˆ—é€šä¿¡

```python
from multiprocessing import Queue
import random

class TaskQueue:
    def __init__(self):
        self.queue = Queue()
    
    def producer(self):
        """ç”Ÿç”¢ä»»å‹™"""
        for i in range(5):
            task = f"Task-{i}"
            self.queue.put(task)
            print(f"ç”Ÿç”¢ä»»å‹™: {task}")
            time.sleep(random.random())
    
    def consumer(self):
        """æ¶ˆè²»ä»»å‹™"""
        while True:
            try:
                task = self.queue.get(timeout=3)
                print(f"è™•ç†ä»»å‹™: {task}")
                time.sleep(random.random())
            except Queue.Empty:
                print("éšŠåˆ—ç‚ºç©ºï¼Œé€€å‡ºæ¶ˆè²»è€…")
                break

if __name__ == '__main__':
    # ä½¿ç”¨ä»»å‹™éšŠåˆ—
    task_queue = TaskQueue()

    # å‰µå»ºç”Ÿç”¢è€…å’Œæ¶ˆè²»è€…é€²ç¨‹
    producer = multiprocessing.Process(target=task_queue.producer)
    consumer = multiprocessing.Process(target=task_queue.consumer)

    producer.start()
    consumer.start()

    producer.join()
    consumer.join()
```

## é€²ç¨‹æ± 

### 1. åŸºæœ¬é€²ç¨‹æ± 

```python
from multiprocessing import Pool
import random

def process_task(task_id):
    """è™•ç†ä»»å‹™"""
    print(f"é–‹å§‹è™•ç†ä»»å‹™ {task_id}")
    time.sleep(random.random() * 2)  # æ¨¡æ“¬å·¥ä½œæ™‚é–“
    return f"Task {task_id} result"

if __name__ == '__main__':
    # ä½¿ç”¨é€²ç¨‹æ± 
    with Pool(processes=4) as pool:
        # æäº¤å¤šå€‹ä»»å‹™
        results = pool.map(process_task, range(10))
        
        # è¼¸å‡ºçµæœ
        for result in results:
            print(f"ç²å¾—çµæœ: {result}")
```

### 2. ç•°æ­¥é€²ç¨‹æ± 

```python
from multiprocessing import Pool
from tqdm import tqdm

def async_process_task(tasks):
    """ç•°æ­¥è™•ç†ä»»å‹™"""
    with Pool(processes=4) as pool:
        # ç•°æ­¥æäº¤ä»»å‹™
        results = []
        with tqdm(total=len(tasks)) as pbar:
            for result in pool.imap_unordered(process_task, tasks):
                results.append(result)
                pbar.update(1)
        return results

if __name__ == '__main__':
    # ä½¿ç”¨ç•°æ­¥é€²ç¨‹æ± 
    tasks = range(10)
    results = async_process_task(tasks)
    print("æ‰€æœ‰ä»»å‹™å®Œæˆï¼")
```

## å…±äº«å…§å­˜

### 1. å…±äº«è®Šé‡

```python
from multiprocessing import Value, Array

def increment_counter(counter):
    """å¢åŠ å…±äº«è¨ˆæ•¸å™¨"""
    with counter.get_lock():
        counter.value += 1

def modify_array(shared_array, index, value):
    """ä¿®æ”¹å…±äº«æ•¸çµ„"""
    shared_array[index] = value

if __name__ == '__main__':
    # å‰µå»ºå…±äº«è¨ˆæ•¸å™¨
    counter = Value('i', 0)
    
    # å‰µå»ºå…±äº«æ•¸çµ„
    array = Array('i', range(5))
    
    # å‰µå»ºå¤šå€‹é€²ç¨‹æ“ä½œå…±äº«è®Šé‡
    processes = []
    for i in range(10):
        p = multiprocessing.Process(target=increment_counter, args=(counter,))
        processes.append(p)
        p.start()
    
    for p in processes:
        p.join()
    
    print(f"æœ€çµ‚è¨ˆæ•¸: {counter.value}")
    print(f"å…±äº«æ•¸çµ„: {list(array)}")
```

### 2. å…±äº«å­—å…¸

```python
from multiprocessing import Manager

def update_dict(shared_dict, key, value):
    """æ›´æ–°å…±äº«å­—å…¸"""
    shared_dict[key] = value

if __name__ == '__main__':
    # å‰µå»ºç®¡ç†å™¨
    with Manager() as manager:
        # å‰µå»ºå…±äº«å­—å…¸
        shared_dict = manager.dict()
        
        # å‰µå»ºå¤šå€‹é€²ç¨‹æ›´æ–°å­—å…¸
        processes = []
        for i in range(5):
            p = multiprocessing.Process(
                target=update_dict, 
                args=(shared_dict, f'key{i}', f'value{i}')
            )
            processes.append(p)
            p.start()
        
        for p in processes:
            p.join()
        
        print(f"å…±äº«å­—å…¸: {dict(shared_dict)}")
```

## å¯¦éš›æ‡‰ç”¨ç¯„ä¾‹

### 1. ä¸¦è¡Œåœ–åƒè™•ç†å™¨

```python
from PIL import Image
import os
from multiprocessing import Pool

class ImageProcessor:
    def __init__(self, input_dir, output_dir):
        self.input_dir = input_dir
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
    
    def process_image(self, filename):
        """è™•ç†å–®å¼µåœ–ç‰‡"""
        try:
            input_path = os.path.join(self.input_dir, filename)
            output_path = os.path.join(self.output_dir, filename)
            
            with Image.open(input_path) as img:
                # èª¿æ•´åœ–ç‰‡å¤§å°
                resized = img.resize((800, 600))
                # å¢åŠ äº®åº¦
                enhanced = resized.point(lambda p: p * 1.2)
                # ä¿å­˜è™•ç†å¾Œçš„åœ–ç‰‡
                enhanced.save(output_path)
            
            return f"æˆåŠŸè™•ç†: {filename}"
        except Exception as e:
            return f"è™•ç†å¤±æ•— {filename}: {str(e)}"
    
    def batch_process(self):
        """æ‰¹é‡è™•ç†åœ–ç‰‡"""
        filenames = [f for f in os.listdir(self.input_dir) 
                    if f.endswith(('.jpg', '.png'))]
        
        with Pool(processes=4) as pool:
            results = pool.map(self.process_image, filenames)
        
        return results

if __name__ == '__main__':
    # ä½¿ç”¨åœ–åƒè™•ç†å™¨
    processor = ImageProcessor('input_images', 'output_images')
    results = processor.batch_process()
    
    for result in results:
        print(result)
```

### 2. æ•¸æ“šåˆ†æç³»çµ±

```python
import pandas as pd
import numpy as np
from multiprocessing import Pool

class DataAnalyzer:
    def __init__(self, data_file):
        self.data = pd.read_csv(data_file)
        self.chunks = np.array_split(self.data, 4)
    
    def analyze_chunk(self, chunk):
        """åˆ†ææ•¸æ“šå¡Š"""
        results = {
            'mean': chunk.mean(),
            'std': chunk.std(),
            'min': chunk.min(),
            'max': chunk.max()
        }
        return results
    
    def parallel_analyze(self):
        """ä¸¦è¡Œåˆ†ææ•¸æ“š"""
        with Pool(processes=4) as pool:
            chunk_results = pool.map(self.analyze_chunk, self.chunks)
        
        # åˆä½µçµæœ
        final_results = {}
        for key in ['mean', 'std', 'min', 'max']:
            values = [result[key] for result in chunk_results]
            final_results[key] = pd.concat(values)
        
        return final_results

if __name__ == '__main__':
    # ä½¿ç”¨æ•¸æ“šåˆ†æå™¨
    analyzer = DataAnalyzer('large_dataset.csv')
    results = analyzer.parallel_analyze()
    
    print("åˆ†æçµæœ:")
    for key, value in results.items():
        print(f"\n{key.upper()}:")
        print(value)
```

## ç·´ç¿’é¡Œ

1. **ä¸¦è¡Œæ–‡ä»¶è™•ç†å™¨**
   å¯¦ç¾ä¸€å€‹ä¸¦è¡Œæ–‡ä»¶è™•ç†ç³»çµ±ï¼š
   - å¤šé€²ç¨‹è®€å–æ–‡ä»¶
   - å…§å®¹è™•ç†å’Œè½‰æ›
   - çµæœåˆä½µ
   - é€²åº¦ç›£æ§

2. **åˆ†å¸ƒå¼è¨ˆç®—ç³»çµ±**
   å‰µå»ºä¸€å€‹ç°¡å–®çš„åˆ†å¸ƒå¼è¨ˆç®—ç³»çµ±ï¼š
   - ä»»å‹™åˆ†é…
   - çµæœæ”¶é›†
   - éŒ¯èª¤è™•ç†
   - è² è¼‰å‡è¡¡

3. **æ•¸æ“šè™•ç†ç®¡é“**
   é–‹ç™¼ä¸€å€‹æ•¸æ“šè™•ç†ç®¡é“ï¼š
   - æ•¸æ“šé è™•ç†
   - ä¸¦è¡Œè™•ç†
   - çµæœèšåˆ
   - æ€§èƒ½å„ªåŒ–

## å°æé†’ ğŸ’¡

1. åˆç†ä½¿ç”¨é€²ç¨‹æ•¸é‡
2. æ³¨æ„å…§å­˜ä½¿ç”¨
3. è™•ç†é€²ç¨‹é–“é€šä¿¡
4. æ­£ç¢ºè™•ç†ç•°å¸¸
5. é¿å…è³‡æºç«¶çˆ­
6. åŠæ™‚æ¸…ç†è³‡æº

[ä¸Šä¸€ç« ï¼šå¤šç·šç¨‹é€²éš](044_å¤šç·šç¨‹é€²éš.md) | [ä¸‹ä¸€ç« ï¼šGUIç¨‹å¼è¨­è¨ˆåŸºç¤](046_GUIç¨‹å¼è¨­è¨ˆåŸºç¤.md) 