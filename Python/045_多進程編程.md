[上一章：多線程進階](044_多線程進階.md) | [下一章：GUI程式設計基礎](046_GUI程式設計基礎.md)

# Python 多進程編程 🚀

## 多進程基礎概念

### 1. 進程與線程的區別

```python
"""
進程（Process）vs 線程（Thread）主要區別：

1. 資源隔離：
   - 進程：完全獨立的資源空間
   - 線程：共享所屬進程的資源

2. 通信方式：
   - 進程：需要特殊的IPC機制
   - 線程：可直接共享變量

3. 開銷：
   - 進程：創建和切換開銷大
   - 線程：創建和切換開銷小

4. 穩定性：
   - 進程：一個進程崩潰不影響其他進程
   - 線程：一個線程崩潰可能導致整個進程崩潰
"""

import os
import multiprocessing
import threading

def show_info():
    print(f'進程ID: {os.getpid()}')
    print(f'線程ID: {threading.current_thread().ident}')
    print(f'父進程ID: {os.getppid()}')

if __name__ == '__main__':
    print("主進程信息：")
    show_info()
    
    # 創建子進程
    p = multiprocessing.Process(target=show_info)
    p.start()
    p.join()
```

### 2. 創建進程的方法

```python
import multiprocessing
import time
import os

# 方法1：函數方式
def worker_function():
    print(f'工作進程 {os.getpid()} 開始執行')
    time.sleep(2)
    print(f'工作進程 {os.getpid()} 執行完成')

# 方法2：類方式
class WorkerProcess(multiprocessing.Process):
    def __init__(self, name):
        super().__init__()
        self.name = name
    
    def run(self):
        print(f'進程 {self.name} ({os.getpid()}) 開始執行')
        time.sleep(2)
        print(f'進程 {self.name} ({os.getpid()}) 執行完成')

if __name__ == '__main__':
    # 使用函數創建進程
    process1 = multiprocessing.Process(target=worker_function)
    process1.start()
    process1.join()
    
    # 使用類創建進程
    process2 = WorkerProcess("Worker-1")
    process2.start()
    process2.join()
    
    # 創建多個進程
    processes = []
    for i in range(3):
        p = WorkerProcess(f"Worker-{i}")
        processes.append(p)
        p.start()
    
    for p in processes:
        p.join()
```

## 進程間通信（IPC）

### 1. 管道（Pipe）通信

```python
from multiprocessing import Pipe, Process
import time

def sender(conn, messages):
    """發送數據的進程"""
    for msg in messages:
        conn.send(msg)
        print(f"發送: {msg}")
        time.sleep(1)
    conn.close()

def receiver(conn):
    """接收數據的進程"""
    while True:
        try:
            msg = conn.recv()
            print(f"接收: {msg}")
        except EOFError:
            break
    conn.close()

if __name__ == '__main__':
    # 創建管道
    parent_conn, child_conn = Pipe()
    
    # 準備發送的消息
    messages = ['Hello', 42, {'key': 'value'}, [1, 2, 3]]
    
    # 創建發送和接收進程
    p1 = Process(target=sender, args=(parent_conn, messages))
    p2 = Process(target=receiver, args=(child_conn,))
    
    p1.start()
    p2.start()
    
    p1.join()
    p2.join()
```

### 2. 隊列（Queue）通信

```python
from multiprocessing import Queue, Process
import time
import random

class TaskManager:
    def __init__(self):
        self.task_queue = Queue()
        self.result_queue = Queue()
    
    def producer(self, num_tasks):
        """生產任務"""
        for i in range(num_tasks):
            task = f"Task-{i}"
            self.task_queue.put(task)
            print(f"生產任務: {task}")
            time.sleep(random.random())
        
        # 添加結束標記
        self.task_queue.put(None)
    
    def consumer(self):
        """消費任務"""
        while True:
            task = self.task_queue.get()
            if task is None:
                break
            
            # 處理任務
            result = f"Result of {task}"
            print(f"處理任務: {task}")
            self.result_queue.put(result)
            time.sleep(random.random())
    
    def result_collector(self, expected_results):
        """收集結果"""
        results = []
        while len(results) < expected_results:
            result = self.result_queue.get()
            results.append(result)
            print(f"收集結果: {result}")
        return results

if __name__ == '__main__':
    manager = TaskManager()
    num_tasks = 5
    
    # 創建生產者、消費者和結果收集進程
    producer = Process(target=manager.producer, args=(num_tasks,))
    consumer = Process(target=manager.consumer)
    collector = Process(target=manager.result_collector, args=(num_tasks,))
    
    producer.start()
    consumer.start()
    collector.start()
    
    producer.join()
    consumer.join()
    collector.join()
```

## 進程池

### 1. 基本進程池

```python
from multiprocessing import Pool
import time
import random

def process_task(task_id):
    """處理單個任務"""
    print(f"開始處理任務 {task_id}")
    processing_time = random.uniform(0.5, 2)
    time.sleep(processing_time)
    return f"任務 {task_id} 完成，處理時間: {processing_time:.2f}秒"

class TaskProcessor:
    def __init__(self, num_processes):
        self.pool = Pool(processes=num_processes)
    
    def process_tasks(self, num_tasks):
        """批量處理任務"""
        tasks = range(num_tasks)
        
        # 使用map方法
        results = self.pool.map(process_task, tasks)
        
        # 關閉進程池
        self.pool.close()
        self.pool.join()
        
        return results

if __name__ == '__main__':
    processor = TaskProcessor(4)
    results = processor.process_tasks(10)
    
    print("\n處理結果:")
    for result in results:
        print(result)
```

### 2. 異步進程池

```python
from multiprocessing import Pool
import time
import random
from tqdm import tqdm

def async_task(task_id):
    """異步任務處理"""
    time.sleep(random.uniform(0.5, 2))
    return f"任務 {task_id} 完成"

class AsyncTaskProcessor:
    def __init__(self, num_processes):
        self.pool = Pool(processes=num_processes)
    
    def process_tasks_async(self, num_tasks):
        """異步處理任務"""
        # 提交所有任務
        futures = []
        for i in range(num_tasks):
            future = self.pool.apply_async(async_task, (i,))
            futures.append(future)
        
        # 使用進度條收集結果
        results = []
        with tqdm(total=num_tasks) as pbar:
            for future in futures:
                result = future.get()
                results.append(result)
                pbar.update(1)
        
        # 關閉進程池
        self.pool.close()
        self.pool.join()
        
        return results

if __name__ == '__main__':
    processor = AsyncTaskProcessor(4)
    results = processor.process_tasks_async(10)
    
    print("\n處理結果:")
    for result in results:
        print(result)
```

## 共享內存和同步

### 1. 共享內存

```python
from multiprocessing import Process, Value, Array
import time

def increment_counter(counter, array):
    """增加共享計數器並修改共享數組"""
    with counter.get_lock():
        counter.value += 1
        print(f"計數器增加到: {counter.value}")
    
    # 修改共享數組
    for i in range(len(array)):
        array[i] *= 2
        time.sleep(0.1)

if __name__ == '__main__':
    # 創建共享計數器和數組
    counter = Value('i', 0)
    array = Array('i', range(5))
    
    # 創建多個進程
    processes = []
    for _ in range(3):
        p = Process(target=increment_counter, args=(counter, array))
        processes.append(p)
        p.start()
    
    # 等待所有進程完成
    for p in processes:
        p.join()
    
    print(f"最終計數器值: {counter.value}")
    print(f"最終數組值: {list(array)}")
```

### 2. 進程鎖和信號量

```python
from multiprocessing import Process, Lock, Semaphore
import time
import random

class ResourceManager:
    def __init__(self, num_resources):
        self.lock = Lock()
        self.semaphore = Semaphore(num_resources)
        self.resources = list(range(num_resources))
    
    def acquire_resource(self, process_id):
        """獲取資源"""
        self.semaphore.acquire()
        with self.lock:
            resource = self.resources.pop()
            print(f"進程 {process_id} 獲取資源 {resource}")
            return resource
    
    def release_resource(self, process_id, resource):
        """釋放資源"""
        with self.lock:
            self.resources.append(resource)
            print(f"進程 {process_id} 釋放資源 {resource}")
        self.semaphore.release()

def worker(manager, process_id):
    """工作進程"""
    try:
        # 獲取資源
        resource = manager.acquire_resource(process_id)
        
        # 使用資源
        time.sleep(random.uniform(0.5, 2))
        
        # 釋放資源
        manager.release_resource(process_id, resource)
    except Exception as e:
        print(f"進程 {process_id} 發生錯誤: {e}")

if __name__ == '__main__':
    # 創建資源管理器
    manager = ResourceManager(3)
    
    # 創建多個工作進程
    processes = []
    for i in range(5):
        p = Process(target=worker, args=(manager, i))
        processes.append(p)
        p.start()
    
    # 等待所有進程完成
    for p in processes:
        p.join()
```

## 實際應用範例

### 1. 並行圖像處理器

```python
from multiprocessing import Pool
from PIL import Image
import os
import time

class ImageProcessor:
    def __init__(self, input_dir, output_dir, num_processes=4):
        self.input_dir = input_dir
        self.output_dir = output_dir
        self.num_processes = num_processes
        
        # 創建輸出目錄
        os.makedirs(output_dir, exist_ok=True)
    
    def process_image(self, filename):
        """處理單張圖片"""
        try:
            # 構建完整路徑
            input_path = os.path.join(self.input_dir, filename)
            output_path = os.path.join(self.output_dir, f"processed_{filename}")
            
            # 打開並處理圖片
            with Image.open(input_path) as img:
                # 調整大小
                resized = img.resize((800, 600))
                
                # 增加亮度
                enhanced = resized.point(lambda p: p * 1.2)
                
                # 添加水印
                if enhanced.mode != 'RGBA':
                    enhanced = enhanced.convert('RGBA')
                
                # 保存處理後的圖片
                enhanced.save(output_path, 'PNG')
            
            return f"成功處理: {filename}"
        except Exception as e:
            return f"處理失敗 {filename}: {str(e)}"
    
    def batch_process(self):
        """批量處理圖片"""
        # 獲取所有圖片文件
        image_files = [f for f in os.listdir(self.input_dir) 
                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        
        # 使用進程池處理圖片
        start_time = time.time()
        with Pool(processes=self.num_processes) as pool:
            results = pool.map(self.process_image, image_files)
        
        # 計算處理時間
        processing_time = time.time() - start_time
        
        # 統計結果
        success = sum(1 for r in results if r.startswith('成功'))
        failed = len(results) - success
        
        return {
            'total': len(results),
            'success': success,
            'failed': failed,
            'processing_time': processing_time,
            'results': results
        }

if __name__ == '__main__':
    # 使用圖像處理器
    processor = ImageProcessor('input_images', 'output_images')
    results = processor.batch_process()
    
    # 輸出處理結果
    print(f"\n處理完成！")
    print(f"總數: {results['total']}")
    print(f"成功: {results['success']}")
    print(f"失敗: {results['failed']}")
    print(f"處理時間: {results['processing_time']:.2f} 秒")
    
    print("\n詳細結果:")
    for result in results['results']:
        print(result)
```

### 2. 並行數據分析系統

```python
import pandas as pd
import numpy as np
from multiprocessing import Pool, Manager
import time

class DataAnalyzer:
    def __init__(self, num_processes=4):
        self.num_processes = num_processes
    
    def analyze_chunk(self, chunk_data):
        """分析數據塊"""
        try:
            results = {
                'count': len(chunk_data),
                'mean': chunk_data.mean(),
                'std': chunk_data.std(),
                'min': chunk_data.min(),
                'max': chunk_data.max(),
                'median': chunk_data.median()
            }
            return results
        except Exception as e:
            return f"分析失敗: {str(e)}"
    
    def parallel_analyze(self, data_file):
        """並行分析數據"""
        try:
            # 讀取數據
            start_time = time.time()
            df = pd.read_csv(data_file)
            
            # 將數據分割成多個塊
            chunks = np.array_split(df, self.num_processes)
            
            # 使用進程池進行分析
            with Pool(processes=self.num_processes) as pool:
                chunk_results = pool.map(self.analyze_chunk, chunks)
            
            # 合併結果
            final_results = {
                'total_records': sum(r['count'] for r in chunk_results),
                'mean': np.mean([r['mean'] for r in chunk_results]),
                'std': np.mean([r['std'] for r in chunk_results]),
                'min': min(r['min'] for r in chunk_results),
                'max': max(r['max'] for r in chunk_results),
                'median': np.median([r['median'] for r in chunk_results]),
                'processing_time': time.time() - start_time
            }
            
            return final_results
        except Exception as e:
            return f"分析失敗: {str(e)}"

if __name__ == '__main__':
    # 使用數據分析器
    analyzer = DataAnalyzer()
    results = analyzer.parallel_analyze('large_dataset.csv')
    
    # 輸出分析結果
    print("\n數據分析結果:")
    print(f"總記錄數: {results['total_records']}")
    print(f"平均值: {results['mean']:.2f}")
    print(f"標準差: {results['std']:.2f}")
    print(f"最小值: {results['min']:.2f}")
    print(f"最大值: {results['max']:.2f}")
    print(f"中位數: {results['median']:.2f}")
    print(f"處理時間: {results['processing_time']:.2f} 秒")
```

## 練習題

1. **並行文件搜索器**
   實現一個並行文件搜索系統：
   - 多進程搜索文件
   - 支持正則表達式匹配
   - 即時顯示搜索結果
   - 可中斷的搜索過程

2. **分布式計算框架**
   創建一個簡單的分布式計算框架：
   - 任務分配器
   - 工作進程池
   - 結果收集器
   - 錯誤處理機制

3. **高性能數據處理管道**
   開發一個數據處理管道：
   - 數據預處理
   - 並行特徵提取
   - 結果聚合
   - 性能監控

## 小提醒 💡

1. 合理使用進程數量
   - 考慮CPU核心數
   - 注意內存使用情況
   - 避免過度創建進程

2. 正確處理進程間通信
   - 選擇合適的通信方式
   - 處理通信異常
   - 避免死鎖情況

3. 資源管理
   - 及時釋放資源
   - 正確關閉進程
   - 處理異常情況

4. 性能優化
   - 減少進程間通信
   - 合理分配任務
   - 監控系統資源

5. 調試技巧
   - 使用日誌記錄
   - 添加調試信息
   - 進行性能分析

6. 安全考慮
   - 處理敏感數據
   - 防止資源洩露
   - 考慮並發安全

[上一章：多線程進階](044_多線程進階.md) | [下一章：GUI程式設計基礎](046_GUI程式設計基礎.md) 