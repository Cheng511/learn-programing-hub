[上一章：深度學習進階](099_深度學習進階.md) | [下一章：自然語言處理進階](101_自然語言處理進階.md)

# Python 自然語言處理基礎 📝

## 文本處理基礎

### 1. 文本預處理

```python
import re
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer
from typing import List, Dict, Set
import time
import sys
import os

class TextPreprocessor:
    def __init__(self):
        """初始化文本預處理器"""
        try:
            # 下載必要的NLTK數據
            nltk.download('punkt')
            nltk.download('stopwords')
            nltk.download('wordnet')
            
            # 初始化工具
            self.lemmatizer = WordNetLemmatizer()
            self.stemmer = PorterStemmer()
            self.stop_words = set(stopwords.words('english'))
            
            print("Text preprocessor initialized")
            
        except Exception as e:
            print(f"Error initializing text preprocessor: {e}")
    
    def clean_text(self, text: str) -> str:
        """清理文本"""
        try:
            # 轉換為小寫
            text = text.lower()
            
            # 移除特殊字符
            text = re.sub(r'[^a-zA-Z\s]', '', text)
            
            # 移除多餘空格
            text = re.sub(r'\s+', ' ', text)
            
            # 去除首尾空格
            text = text.strip()
            
            return text
            
        except Exception as e:
            print(f"Error cleaning text: {e}")
            return ""
    
    def tokenize_words(self, text: str) -> List[str]:
        """分詞"""
        try:
            # 使用NLTK進行分詞
            tokens = word_tokenize(text)
            return tokens
            
        except Exception as e:
            print(f"Error tokenizing words: {e}")
            return []
    
    def tokenize_sentences(self, text: str) -> List[str]:
        """分句"""
        try:
            # 使用NLTK進行分句
            sentences = sent_tokenize(text)
            return sentences
            
        except Exception as e:
            print(f"Error tokenizing sentences: {e}")
            return []
    
    def remove_stopwords(self, tokens: List[str]) -> List[str]:
        """移除停用詞"""
        try:
            # 移除停用詞
            filtered_tokens = [token for token in tokens if token not in self.stop_words]
            return filtered_tokens
            
        except Exception as e:
            print(f"Error removing stopwords: {e}")
            return []
    
    def lemmatize(self, tokens: List[str]) -> List[str]:
        """詞形還原"""
        try:
            # 進行詞形還原
            lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in tokens]
            return lemmatized_tokens
            
        except Exception as e:
            print(f"Error lemmatizing tokens: {e}")
            return []
    
    def stem(self, tokens: List[str]) -> List[str]:
        """詞幹提取"""
        try:
            # 進行詞幹提取
            stemmed_tokens = [self.stemmer.stem(token) for token in tokens]
            return stemmed_tokens
            
        except Exception as e:
            print(f"Error stemming tokens: {e}")
            return []
    
    def preprocess_text(self, text: str) -> List[str]:
        """完整的文本預處理流程"""
        try:
            # 清理文本
            cleaned_text = self.clean_text(text)
            
            # 分詞
            tokens = self.tokenize_words(cleaned_text)
            
            # 移除停用詞
            filtered_tokens = self.remove_stopwords(tokens)
            
            # 詞形還原
            lemmatized_tokens = self.lemmatize(filtered_tokens)
            
            return lemmatized_tokens
            
        except Exception as e:
            print(f"Error preprocessing text: {e}")
            return []

# 使用示例
def main():
    # 創建示例文本
    text = "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language."
    
    try:
        # 創建文本預處理器
        preprocessor = TextPreprocessor()
        
        # 清理文本
        cleaned_text = preprocessor.clean_text(text)
        print(f"Cleaned text: {cleaned_text}")
        
        # 分詞
        tokens = preprocessor.tokenize_words(cleaned_text)
        print(f"Tokens: {tokens}")
        
        # 移除停用詞
        filtered_tokens = preprocessor.remove_stopwords(tokens)
        print(f"Filtered tokens: {filtered_tokens}")
        
        # 詞形還原
        lemmatized_tokens = preprocessor.lemmatize(filtered_tokens)
        print(f"Lemmatized tokens: {lemmatized_tokens}")
        
        # 完整的預處理流程
        processed_tokens = preprocessor.preprocess_text(text)
        print(f"Processed tokens: {processed_tokens}")
    
    except Exception as e:
        print(f"Error in main: {e}")

if __name__ == '__main__':
    main()
```

### 2. 文本特徵提取

```python
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from typing import List, Dict, Set, Tuple
import time
import sys
import os

class TextFeatureExtractor:
    def __init__(self):
        """初始化文本特徵提取器"""
        try:
            # 初始化向量化器
            self.count_vectorizer = CountVectorizer()
            self.tfidf_vectorizer = TfidfVectorizer()
            
            print("Text feature extractor initialized")
            
        except Exception as e:
            print(f"Error initializing text feature extractor: {e}")
    
    def extract_bow_features(self, texts: List[str]) -> Tuple[np.ndarray, List[str]]:
        """提取詞袋特徵"""
        try:
            # 擬合並轉換文本
            bow_features = self.count_vectorizer.fit_transform(texts)
            
            # 獲取特徵名稱
            feature_names = self.count_vectorizer.get_feature_names_out()
            
            return bow_features.toarray(), feature_names.tolist()
            
        except Exception as e:
            print(f"Error extracting BOW features: {e}")
            return np.array([]), []
    
    def extract_tfidf_features(self, texts: List[str]) -> Tuple[np.ndarray, List[str]]:
        """提取TF-IDF特徵"""
        try:
            # 擬合並轉換文本
            tfidf_features = self.tfidf_vectorizer.fit_transform(texts)
            
            # 獲取特徵名稱
            feature_names = self.tfidf_vectorizer.get_feature_names_out()
            
            return tfidf_features.toarray(), feature_names.tolist()
            
        except Exception as e:
            print(f"Error extracting TF-IDF features: {e}")
            return np.array([]), []
    
    def get_feature_importance(self, features: np.ndarray, feature_names: List[str], top_n: int = 10) -> List[Tuple[str, float]]:
        """獲取特徵重要性"""
        try:
            # 計算特徵重要性
            importance = np.mean(features, axis=0)
            
            # 獲取最重要的特徵
            top_indices = np.argsort(importance)[-top_n:][::-1]
            top_features = [(feature_names[i], importance[i]) for i in top_indices]
            
            return top_features
            
        except Exception as e:
            print(f"Error getting feature importance: {e}")
            return []
    
    def extract_ngram_features(self, texts: List[str], ngram_range: Tuple[int, int] = (1, 2)) -> Tuple[np.ndarray, List[str]]:
        """提取N-gram特徵"""
        try:
            # 創建N-gram向量化器
            ngram_vectorizer = CountVectorizer(ngram_range=ngram_range)
            
            # 擬合並轉換文本
            ngram_features = ngram_vectorizer.fit_transform(texts)
            
            # 獲取特徵名稱
            feature_names = ngram_vectorizer.get_feature_names_out()
            
            return ngram_features.toarray(), feature_names.tolist()
            
        except Exception as e:
            print(f"Error extracting N-gram features: {e}")
            return np.array([]), []

# 使用示例
def main():
    # 創建示例文本
    texts = [
        "Natural Language Processing is a field of artificial intelligence.",
        "Machine Learning is a subset of artificial intelligence.",
        "Deep Learning is a subset of machine learning."
    ]
    
    try:
        # 創建特徵提取器
        extractor = TextFeatureExtractor()
        
        # 提取詞袋特徵
        bow_features, bow_names = extractor.extract_bow_features(texts)
        print(f"BOW features shape: {bow_features.shape}")
        print(f"Number of BOW features: {len(bow_names)}")
        
        # 提取TF-IDF特徵
        tfidf_features, tfidf_names = extractor.extract_tfidf_features(texts)
        print(f"TF-IDF features shape: {tfidf_features.shape}")
        print(f"Number of TF-IDF features: {len(tfidf_names)}")
        
        # 獲取特徵重要性
        top_features = extractor.get_feature_importance(tfidf_features, tfidf_names)
        print(f"Top features: {top_features}")
        
        # 提取N-gram特徵
        ngram_features, ngram_names = extractor.extract_ngram_features(texts)
        print(f"N-gram features shape: {ngram_features.shape}")
        print(f"Number of N-gram features: {len(ngram_names)}")
    
    except Exception as e:
        print(f"Error in main: {e}")

if __name__ == '__main__':
    main()
```

## 練習題

1. **文本預處理**
   開發文本預處理：
   - 清理文本
   - 分詞分句
   - 移除停用詞
   - 優化性能

2. **特徵提取**
   創建特徵提取：
   - 詞袋特徵
   - TF-IDF特徵
   - N-gram特徵
   - 優化性能

3. **自然語言處理**
   實現自然語言處理：
   - 處理文本
   - 提取特徵
   - 優化性能
   - 處理異常

## 小提醒 💡

1. 文本預處理
   - 選擇合適方法
   - 優化參數
   - 處理異常
   - 提供監控

2. 特徵提取
   - 選擇合適方法
   - 優化性能
   - 處理異常
   - 提供結果

3. 自然語言處理
   - 選擇合適算法
   - 優化性能
   - 處理異常
   - 提供監控

4. 調試技巧
   - 使用開發工具
   - 分析性能
   - 優化關鍵路徑
   - 監控處理狀態

[上一章：深度學習進階](099_深度學習進階.md) | [下一章：自然語言處理進階](101_自然語言處理進階.md) 