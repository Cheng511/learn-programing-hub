[ä¸Šä¸€ç« ï¼šæ·±åº¦å­¸ç¿’é€²éš](099_æ·±åº¦å­¸ç¿’é€²éš.md) | [ä¸‹ä¸€ç« ï¼šè‡ªç„¶èªè¨€è™•ç†é€²éš](101_è‡ªç„¶èªè¨€è™•ç†é€²éš.md)

# Python è‡ªç„¶èªè¨€è™•ç†åŸºç¤ ğŸ“

## æ–‡æœ¬è™•ç†åŸºç¤

### 1. æ–‡æœ¬é è™•ç†

```python
import re
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer
from typing import List, Dict, Set
import time
import sys
import os

class TextPreprocessor:
    def __init__(self):
        """åˆå§‹åŒ–æ–‡æœ¬é è™•ç†å™¨"""
        try:
            # ä¸‹è¼‰å¿…è¦çš„NLTKæ•¸æ“š
            nltk.download('punkt')
            nltk.download('stopwords')
            nltk.download('wordnet')
            
            # åˆå§‹åŒ–å·¥å…·
            self.lemmatizer = WordNetLemmatizer()
            self.stemmer = PorterStemmer()
            self.stop_words = set(stopwords.words('english'))
            
            print("Text preprocessor initialized")
            
        except Exception as e:
            print(f"Error initializing text preprocessor: {e}")
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        try:
            # è½‰æ›ç‚ºå°å¯«
            text = text.lower()
            
            # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
            text = re.sub(r'[^a-zA-Z\s]', '', text)
            
            # ç§»é™¤å¤šé¤˜ç©ºæ ¼
            text = re.sub(r'\s+', ' ', text)
            
            # å»é™¤é¦–å°¾ç©ºæ ¼
            text = text.strip()
            
            return text
            
        except Exception as e:
            print(f"Error cleaning text: {e}")
            return ""
    
    def tokenize_words(self, text: str) -> List[str]:
        """åˆ†è©"""
        try:
            # ä½¿ç”¨NLTKé€²è¡Œåˆ†è©
            tokens = word_tokenize(text)
            return tokens
            
        except Exception as e:
            print(f"Error tokenizing words: {e}")
            return []
    
    def tokenize_sentences(self, text: str) -> List[str]:
        """åˆ†å¥"""
        try:
            # ä½¿ç”¨NLTKé€²è¡Œåˆ†å¥
            sentences = sent_tokenize(text)
            return sentences
            
        except Exception as e:
            print(f"Error tokenizing sentences: {e}")
            return []
    
    def remove_stopwords(self, tokens: List[str]) -> List[str]:
        """ç§»é™¤åœç”¨è©"""
        try:
            # ç§»é™¤åœç”¨è©
            filtered_tokens = [token for token in tokens if token not in self.stop_words]
            return filtered_tokens
            
        except Exception as e:
            print(f"Error removing stopwords: {e}")
            return []
    
    def lemmatize(self, tokens: List[str]) -> List[str]:
        """è©å½¢é‚„åŸ"""
        try:
            # é€²è¡Œè©å½¢é‚„åŸ
            lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in tokens]
            return lemmatized_tokens
            
        except Exception as e:
            print(f"Error lemmatizing tokens: {e}")
            return []
    
    def stem(self, tokens: List[str]) -> List[str]:
        """è©å¹¹æå–"""
        try:
            # é€²è¡Œè©å¹¹æå–
            stemmed_tokens = [self.stemmer.stem(token) for token in tokens]
            return stemmed_tokens
            
        except Exception as e:
            print(f"Error stemming tokens: {e}")
            return []
    
    def preprocess_text(self, text: str) -> List[str]:
        """å®Œæ•´çš„æ–‡æœ¬é è™•ç†æµç¨‹"""
        try:
            # æ¸…ç†æ–‡æœ¬
            cleaned_text = self.clean_text(text)
            
            # åˆ†è©
            tokens = self.tokenize_words(cleaned_text)
            
            # ç§»é™¤åœç”¨è©
            filtered_tokens = self.remove_stopwords(tokens)
            
            # è©å½¢é‚„åŸ
            lemmatized_tokens = self.lemmatize(filtered_tokens)
            
            return lemmatized_tokens
            
        except Exception as e:
            print(f"Error preprocessing text: {e}")
            return []

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # å‰µå»ºç¤ºä¾‹æ–‡æœ¬
    text = "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language."
    
    try:
        # å‰µå»ºæ–‡æœ¬é è™•ç†å™¨
        preprocessor = TextPreprocessor()
        
        # æ¸…ç†æ–‡æœ¬
        cleaned_text = preprocessor.clean_text(text)
        print(f"Cleaned text: {cleaned_text}")
        
        # åˆ†è©
        tokens = preprocessor.tokenize_words(cleaned_text)
        print(f"Tokens: {tokens}")
        
        # ç§»é™¤åœç”¨è©
        filtered_tokens = preprocessor.remove_stopwords(tokens)
        print(f"Filtered tokens: {filtered_tokens}")
        
        # è©å½¢é‚„åŸ
        lemmatized_tokens = preprocessor.lemmatize(filtered_tokens)
        print(f"Lemmatized tokens: {lemmatized_tokens}")
        
        # å®Œæ•´çš„é è™•ç†æµç¨‹
        processed_tokens = preprocessor.preprocess_text(text)
        print(f"Processed tokens: {processed_tokens}")
    
    except Exception as e:
        print(f"Error in main: {e}")

if __name__ == '__main__':
    main()
```

### 2. æ–‡æœ¬ç‰¹å¾µæå–

```python
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from typing import List, Dict, Set, Tuple
import time
import sys
import os

class TextFeatureExtractor:
    def __init__(self):
        """åˆå§‹åŒ–æ–‡æœ¬ç‰¹å¾µæå–å™¨"""
        try:
            # åˆå§‹åŒ–å‘é‡åŒ–å™¨
            self.count_vectorizer = CountVectorizer()
            self.tfidf_vectorizer = TfidfVectorizer()
            
            print("Text feature extractor initialized")
            
        except Exception as e:
            print(f"Error initializing text feature extractor: {e}")
    
    def extract_bow_features(self, texts: List[str]) -> Tuple[np.ndarray, List[str]]:
        """æå–è©è¢‹ç‰¹å¾µ"""
        try:
            # æ“¬åˆä¸¦è½‰æ›æ–‡æœ¬
            bow_features = self.count_vectorizer.fit_transform(texts)
            
            # ç²å–ç‰¹å¾µåç¨±
            feature_names = self.count_vectorizer.get_feature_names_out()
            
            return bow_features.toarray(), feature_names.tolist()
            
        except Exception as e:
            print(f"Error extracting BOW features: {e}")
            return np.array([]), []
    
    def extract_tfidf_features(self, texts: List[str]) -> Tuple[np.ndarray, List[str]]:
        """æå–TF-IDFç‰¹å¾µ"""
        try:
            # æ“¬åˆä¸¦è½‰æ›æ–‡æœ¬
            tfidf_features = self.tfidf_vectorizer.fit_transform(texts)
            
            # ç²å–ç‰¹å¾µåç¨±
            feature_names = self.tfidf_vectorizer.get_feature_names_out()
            
            return tfidf_features.toarray(), feature_names.tolist()
            
        except Exception as e:
            print(f"Error extracting TF-IDF features: {e}")
            return np.array([]), []
    
    def get_feature_importance(self, features: np.ndarray, feature_names: List[str], top_n: int = 10) -> List[Tuple[str, float]]:
        """ç²å–ç‰¹å¾µé‡è¦æ€§"""
        try:
            # è¨ˆç®—ç‰¹å¾µé‡è¦æ€§
            importance = np.mean(features, axis=0)
            
            # ç²å–æœ€é‡è¦çš„ç‰¹å¾µ
            top_indices = np.argsort(importance)[-top_n:][::-1]
            top_features = [(feature_names[i], importance[i]) for i in top_indices]
            
            return top_features
            
        except Exception as e:
            print(f"Error getting feature importance: {e}")
            return []
    
    def extract_ngram_features(self, texts: List[str], ngram_range: Tuple[int, int] = (1, 2)) -> Tuple[np.ndarray, List[str]]:
        """æå–N-gramç‰¹å¾µ"""
        try:
            # å‰µå»ºN-gramå‘é‡åŒ–å™¨
            ngram_vectorizer = CountVectorizer(ngram_range=ngram_range)
            
            # æ“¬åˆä¸¦è½‰æ›æ–‡æœ¬
            ngram_features = ngram_vectorizer.fit_transform(texts)
            
            # ç²å–ç‰¹å¾µåç¨±
            feature_names = ngram_vectorizer.get_feature_names_out()
            
            return ngram_features.toarray(), feature_names.tolist()
            
        except Exception as e:
            print(f"Error extracting N-gram features: {e}")
            return np.array([]), []

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # å‰µå»ºç¤ºä¾‹æ–‡æœ¬
    texts = [
        "Natural Language Processing is a field of artificial intelligence.",
        "Machine Learning is a subset of artificial intelligence.",
        "Deep Learning is a subset of machine learning."
    ]
    
    try:
        # å‰µå»ºç‰¹å¾µæå–å™¨
        extractor = TextFeatureExtractor()
        
        # æå–è©è¢‹ç‰¹å¾µ
        bow_features, bow_names = extractor.extract_bow_features(texts)
        print(f"BOW features shape: {bow_features.shape}")
        print(f"Number of BOW features: {len(bow_names)}")
        
        # æå–TF-IDFç‰¹å¾µ
        tfidf_features, tfidf_names = extractor.extract_tfidf_features(texts)
        print(f"TF-IDF features shape: {tfidf_features.shape}")
        print(f"Number of TF-IDF features: {len(tfidf_names)}")
        
        # ç²å–ç‰¹å¾µé‡è¦æ€§
        top_features = extractor.get_feature_importance(tfidf_features, tfidf_names)
        print(f"Top features: {top_features}")
        
        # æå–N-gramç‰¹å¾µ
        ngram_features, ngram_names = extractor.extract_ngram_features(texts)
        print(f"N-gram features shape: {ngram_features.shape}")
        print(f"Number of N-gram features: {len(ngram_names)}")
    
    except Exception as e:
        print(f"Error in main: {e}")

if __name__ == '__main__':
    main()
```

## ç·´ç¿’é¡Œ

1. **æ–‡æœ¬é è™•ç†**
   é–‹ç™¼æ–‡æœ¬é è™•ç†ï¼š
   - æ¸…ç†æ–‡æœ¬
   - åˆ†è©åˆ†å¥
   - ç§»é™¤åœç”¨è©
   - å„ªåŒ–æ€§èƒ½

2. **ç‰¹å¾µæå–**
   å‰µå»ºç‰¹å¾µæå–ï¼š
   - è©è¢‹ç‰¹å¾µ
   - TF-IDFç‰¹å¾µ
   - N-gramç‰¹å¾µ
   - å„ªåŒ–æ€§èƒ½

3. **è‡ªç„¶èªè¨€è™•ç†**
   å¯¦ç¾è‡ªç„¶èªè¨€è™•ç†ï¼š
   - è™•ç†æ–‡æœ¬
   - æå–ç‰¹å¾µ
   - å„ªåŒ–æ€§èƒ½
   - è™•ç†ç•°å¸¸

## å°æé†’ ğŸ’¡

1. æ–‡æœ¬é è™•ç†
   - é¸æ“‡åˆé©æ–¹æ³•
   - å„ªåŒ–åƒæ•¸
   - è™•ç†ç•°å¸¸
   - æä¾›ç›£æ§

2. ç‰¹å¾µæå–
   - é¸æ“‡åˆé©æ–¹æ³•
   - å„ªåŒ–æ€§èƒ½
   - è™•ç†ç•°å¸¸
   - æä¾›çµæœ

3. è‡ªç„¶èªè¨€è™•ç†
   - é¸æ“‡åˆé©ç®—æ³•
   - å„ªåŒ–æ€§èƒ½
   - è™•ç†ç•°å¸¸
   - æä¾›ç›£æ§

4. èª¿è©¦æŠ€å·§
   - ä½¿ç”¨é–‹ç™¼å·¥å…·
   - åˆ†ææ€§èƒ½
   - å„ªåŒ–é—œéµè·¯å¾‘
   - ç›£æ§è™•ç†ç‹€æ…‹

[ä¸Šä¸€ç« ï¼šæ·±åº¦å­¸ç¿’é€²éš](099_æ·±åº¦å­¸ç¿’é€²éš.md) | [ä¸‹ä¸€ç« ï¼šè‡ªç„¶èªè¨€è™•ç†é€²éš](101_è‡ªç„¶èªè¨€è™•ç†é€²éš.md) 