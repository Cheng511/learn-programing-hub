[上一章：深度學習基礎](098_深度學習基礎.md) | [下一章：自然語言處理基礎](100_自然語言處理基礎.md)

# Python 深度學習進階 🧠

## 高級神經網絡

### 1. 循環神經網絡

```python
import numpy as np
from typing import List, Tuple, Optional
import time
import sys
import os

class RecurrentNeuralNetwork:
    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        """初始化循環神經網絡"""
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.initialize_parameters()
    
    def initialize_parameters(self):
        """初始化參數"""
        try:
            # 初始化權重
            self.Wxh = np.random.randn(self.hidden_size, self.input_size) * 0.01
            self.Whh = np.random.randn(self.hidden_size, self.hidden_size) * 0.01
            self.Why = np.random.randn(self.output_size, self.hidden_size) * 0.01
            
            # 初始化偏置
            self.bh = np.zeros((self.hidden_size, 1))
            self.by = np.zeros((self.output_size, 1))
            
            print("Parameters initialized")
            
        except Exception as e:
            print(f"Error initializing parameters: {e}")
    
    def tanh(self, x: np.ndarray) -> np.ndarray:
        """tanh激活函數"""
        return np.tanh(x)
    
    def tanh_derivative(self, x: np.ndarray) -> np.ndarray:
        """tanh導數"""
        return 1 - np.tanh(x) ** 2
    
    def softmax(self, x: np.ndarray) -> np.ndarray:
        """softmax激活函數"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)
    
    def forward_propagation(self, inputs: List[np.ndarray], h_prev: np.ndarray) -> Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]:
        """前向傳播"""
        try:
            h_states = [h_prev]
            y_states = []
            cache = []
            
            for x in inputs:
                # 計算隱藏狀態
                h = self.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h_states[-1]) + self.bh)
                h_states.append(h)
                
                # 計算輸出
                y = self.softmax(np.dot(self.Why, h) + self.by)
                y_states.append(y)
                
                # 保存中間結果
                cache.append((x, h_states[-2], h, y))
            
            return h_states, y_states, cache
            
        except Exception as e:
            print(f"Error in forward propagation: {e}")
            return [], [], []
    
    def backward_propagation(self, y_states: List[np.ndarray], targets: List[np.ndarray], cache: List[Tuple], h_states: List[np.ndarray]):
        """反向傳播"""
        try:
            # 初始化梯度
            dWxh = np.zeros_like(self.Wxh)
            dWhh = np.zeros_like(self.Whh)
            dWhy = np.zeros_like(self.Why)
            dbh = np.zeros_like(self.bh)
            dby = np.zeros_like(self.by)
            
            # 計算輸出層梯度
            dh_next = np.zeros((self.hidden_size, 1))
            
            for t in reversed(range(len(y_states))):
                # 計算輸出層誤差
                dy = y_states[t] - targets[t]
                dWhy += np.dot(dy, h_states[t + 1].T)
                dby += dy
                
                # 計算隱藏層誤差
                dh = np.dot(self.Why.T, dy) + dh_next
                dh_raw = self.tanh_derivative(cache[t][2]) * dh
                
                # 更新權重梯度
                dWxh += np.dot(dh_raw, cache[t][0].T)
                dWhh += np.dot(dh_raw, cache[t][1].T)
                dbh += dh_raw
                
                # 保存下一時刻的隱藏層梯度
                dh_next = np.dot(self.Whh.T, dh_raw)
            
            # 裁剪梯度
            for dparam in [dWxh, dWhh, dWhy, dbh, dby]:
                np.clip(dparam, -5, 5, out=dparam)
            
            return dWxh, dWhh, dWhy, dbh, dby
            
        except Exception as e:
            print(f"Error in backward propagation: {e}")
            return None, None, None, None, None
    
    def update_parameters(self, dWxh: np.ndarray, dWhh: np.ndarray, dWhy: np.ndarray, dbh: np.ndarray, dby: np.ndarray, learning_rate: float):
        """更新參數"""
        try:
            self.Wxh -= learning_rate * dWxh
            self.Whh -= learning_rate * dWhh
            self.Why -= learning_rate * dWhy
            self.bh -= learning_rate * dbh
            self.by -= learning_rate * dby
            
            print("Parameters updated")
            
        except Exception as e:
            print(f"Error updating parameters: {e}")
    
    def train(self, inputs: List[np.ndarray], targets: List[np.ndarray], epochs: int, learning_rate: float):
        """訓練模型"""
        try:
            for epoch in range(epochs):
                # 初始化隱藏狀態
                h_prev = np.zeros((self.hidden_size, 1))
                
                # 前向傳播
                h_states, y_states, cache = self.forward_propagation(inputs, h_prev)
                
                # 反向傳播
                dWxh, dWhh, dWhy, dbh, dby = self.backward_propagation(y_states, targets, cache, h_states)
                
                # 更新參數
                self.update_parameters(dWxh, dWhh, dWhy, dbh, dby, learning_rate)
                
                if (epoch + 1) % 100 == 0:
                    loss = -np.sum([np.sum(target * np.log(y + 1e-15)) for target, y in zip(targets, y_states)])
                    print(f"Epoch {epoch + 1}, Loss: {loss:.4f}")
            
            print("Training completed")
            
        except Exception as e:
            print(f"Error in training: {e}")
    
    def predict(self, inputs: List[np.ndarray], h_prev: np.ndarray) -> List[np.ndarray]:
        """預測"""
        try:
            h_states, y_states, _ = self.forward_propagation(inputs, h_prev)
            return y_states
            
        except Exception as e:
            print(f"Error in prediction: {e}")
            return []

# 使用示例
def main():
    # 創建示例數據
    input_size = 2
    hidden_size = 4
    output_size = 2
    seq_length = 3
    
    # 創建輸入序列
    inputs = [np.random.randn(input_size, 1) for _ in range(seq_length)]
    targets = [np.random.randint(0, 2, (output_size, 1)) for _ in range(seq_length)]
    
    try:
        # 創建RNN
        rnn = RecurrentNeuralNetwork(input_size, hidden_size, output_size)
        
        # 初始化隱藏狀態
        h_prev = np.zeros((hidden_size, 1))
        
        # 訓練模型
        rnn.train(inputs, targets, epochs=1000, learning_rate=0.01)
        
        # 預測
        predictions = rnn.predict(inputs, h_prev)
        print(f"Number of predictions: {len(predictions)}")
    
    except Exception as e:
        print(f"Error in main: {e}")

if __name__ == '__main__':
    main()
```

### 2. 長短期記憶網絡

```python
import numpy as np
from typing import List, Tuple, Optional
import time
import sys
import os

class LongShortTermMemory:
    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        """初始化長短期記憶網絡"""
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.initialize_parameters()
    
    def initialize_parameters(self):
        """初始化參數"""
        try:
            # 初始化權重
            self.Wf = np.random.randn(self.hidden_size, self.input_size + self.hidden_size) * 0.01
            self.Wi = np.random.randn(self.hidden_size, self.input_size + self.hidden_size) * 0.01
            self.Wc = np.random.randn(self.hidden_size, self.input_size + self.hidden_size) * 0.01
            self.Wo = np.random.randn(self.hidden_size, self.input_size + self.hidden_size) * 0.01
            self.Wy = np.random.randn(self.output_size, self.hidden_size) * 0.01
            
            # 初始化偏置
            self.bf = np.zeros((self.hidden_size, 1))
            self.bi = np.zeros((self.hidden_size, 1))
            self.bc = np.zeros((self.hidden_size, 1))
            self.bo = np.zeros((self.hidden_size, 1))
            self.by = np.zeros((self.output_size, 1))
            
            print("Parameters initialized")
            
        except Exception as e:
            print(f"Error initializing parameters: {e}")
    
    def sigmoid(self, x: np.ndarray) -> np.ndarray:
        """sigmoid激活函數"""
        return 1 / (1 + np.exp(-x))
    
    def sigmoid_derivative(self, x: np.ndarray) -> np.ndarray:
        """sigmoid導數"""
        return x * (1 - x)
    
    def tanh(self, x: np.ndarray) -> np.ndarray:
        """tanh激活函數"""
        return np.tanh(x)
    
    def tanh_derivative(self, x: np.ndarray) -> np.ndarray:
        """tanh導數"""
        return 1 - np.tanh(x) ** 2
    
    def softmax(self, x: np.ndarray) -> np.ndarray:
        """softmax激活函數"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)
    
    def forward_propagation(self, inputs: List[np.ndarray], h_prev: np.ndarray, c_prev: np.ndarray) -> Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[Tuple]]:
        """前向傳播"""
        try:
            h_states = [h_prev]
            c_states = [c_prev]
            y_states = []
            cache = []
            
            for x in inputs:
                # 連接輸入和隱藏狀態
                concat = np.vstack((x, h_states[-1]))
                
                # 遺忘門
                ft = self.sigmoid(np.dot(self.Wf, concat) + self.bf)
                
                # 輸入門
                it = self.sigmoid(np.dot(self.Wi, concat) + self.bi)
                
                # 單元狀態候選值
                c_tilde = self.tanh(np.dot(self.Wc, concat) + self.bc)
                
                # 更新單元狀態
                c = ft * c_states[-1] + it * c_tilde
                c_states.append(c)
                
                # 輸出門
                ot = self.sigmoid(np.dot(self.Wo, concat) + self.bo)
                
                # 更新隱藏狀態
                h = ot * self.tanh(c)
                h_states.append(h)
                
                # 計算輸出
                y = self.softmax(np.dot(self.Wy, h) + self.by)
                y_states.append(y)
                
                # 保存中間結果
                cache.append((x, h_states[-2], c_states[-2], ft, it, c_tilde, c, ot, h, y))
            
            return h_states, c_states, y_states, cache
            
        except Exception as e:
            print(f"Error in forward propagation: {e}")
            return [], [], [], []
    
    def backward_propagation(self, y_states: List[np.ndarray], targets: List[np.ndarray], cache: List[Tuple], h_states: List[np.ndarray], c_states: List[np.ndarray]):
        """反向傳播"""
        try:
            # 初始化梯度
            dWf = np.zeros_like(self.Wf)
            dWi = np.zeros_like(self.Wi)
            dWc = np.zeros_like(self.Wc)
            dWo = np.zeros_like(self.Wo)
            dWy = np.zeros_like(self.Wy)
            dbf = np.zeros_like(self.bf)
            dbi = np.zeros_like(self.bi)
            dbc = np.zeros_like(self.bc)
            dbo = np.zeros_like(self.bo)
            dby = np.zeros_like(self.by)
            
            # 初始化下一時刻的梯度
            dh_next = np.zeros((self.hidden_size, 1))
            dc_next = np.zeros((self.hidden_size, 1))
            
            for t in reversed(range(len(y_states))):
                # 計算輸出層誤差
                dy = y_states[t] - targets[t]
                dWy += np.dot(dy, h_states[t + 1].T)
                dby += dy
                
                # 計算隱藏層誤差
                dh = np.dot(self.Wy.T, dy) + dh_next
                
                # 計算單元狀態誤差
                dc = dc_next + ot * self.tanh_derivative(self.tanh(c_states[t + 1])) * dh
                
                # 計算門控單元誤差
                dot = self.sigmoid_derivative(ot) * self.tanh(c_states[t + 1]) * dh
                dit = self.sigmoid_derivative(it) * c_tilde * dc
                dft = self.sigmoid_derivative(ft) * c_states[t] * dc
                dc_tilde = self.tanh_derivative(c_tilde) * it * dc
                
                # 更新權重梯度
                dWf += np.dot(dft, concat.T)
                dWi += np.dot(dit, concat.T)
                dWc += np.dot(dc_tilde, concat.T)
                dWo += np.dot(dot, concat.T)
                
                # 更新偏置梯度
                dbf += dft
                dbi += dit
                dbc += dc_tilde
                dbo += dot
                
                # 保存下一時刻的梯度
                dh_next = np.dot(self.Wf.T, dft) + np.dot(self.Wi.T, dit) + np.dot(self.Wc.T, dc_tilde) + np.dot(self.Wo.T, dot)
                dc_next = ft * dc
            
            # 裁剪梯度
            for dparam in [dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby]:
                np.clip(dparam, -5, 5, out=dparam)
            
            return dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby
            
        except Exception as e:
            print(f"Error in backward propagation: {e}")
            return None, None, None, None, None, None, None, None, None, None
    
    def update_parameters(self, dWf: np.ndarray, dWi: np.ndarray, dWc: np.ndarray, dWo: np.ndarray, dWy: np.ndarray, dbf: np.ndarray, dbi: np.ndarray, dbc: np.ndarray, dbo: np.ndarray, dby: np.ndarray, learning_rate: float):
        """更新參數"""
        try:
            self.Wf -= learning_rate * dWf
            self.Wi -= learning_rate * dWi
            self.Wc -= learning_rate * dWc
            self.Wo -= learning_rate * dWo
            self.Wy -= learning_rate * dWy
            self.bf -= learning_rate * dbf
            self.bi -= learning_rate * dbi
            self.bc -= learning_rate * dbc
            self.bo -= learning_rate * dbo
            self.by -= learning_rate * dby
            
            print("Parameters updated")
            
        except Exception as e:
            print(f"Error updating parameters: {e}")
    
    def train(self, inputs: List[np.ndarray], targets: List[np.ndarray], epochs: int, learning_rate: float):
        """訓練模型"""
        try:
            for epoch in range(epochs):
                # 初始化隱藏狀態和單元狀態
                h_prev = np.zeros((self.hidden_size, 1))
                c_prev = np.zeros((self.hidden_size, 1))
                
                # 前向傳播
                h_states, c_states, y_states, cache = self.forward_propagation(inputs, h_prev, c_prev)
                
                # 反向傳播
                dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby = self.backward_propagation(y_states, targets, cache, h_states, c_states)
                
                # 更新參數
                self.update_parameters(dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby, learning_rate)
                
                if (epoch + 1) % 100 == 0:
                    loss = -np.sum([np.sum(target * np.log(y + 1e-15)) for target, y in zip(targets, y_states)])
                    print(f"Epoch {epoch + 1}, Loss: {loss:.4f}")
            
            print("Training completed")
            
        except Exception as e:
            print(f"Error in training: {e}")
    
    def predict(self, inputs: List[np.ndarray], h_prev: np.ndarray, c_prev: np.ndarray) -> List[np.ndarray]:
        """預測"""
        try:
            h_states, c_states, y_states, _ = self.forward_propagation(inputs, h_prev, c_prev)
            return y_states
            
        except Exception as e:
            print(f"Error in prediction: {e}")
            return []

# 使用示例
def main():
    # 創建示例數據
    input_size = 2
    hidden_size = 4
    output_size = 2
    seq_length = 3
    
    # 創建輸入序列
    inputs = [np.random.randn(input_size, 1) for _ in range(seq_length)]
    targets = [np.random.randint(0, 2, (output_size, 1)) for _ in range(seq_length)]
    
    try:
        # 創建LSTM
        lstm = LongShortTermMemory(input_size, hidden_size, output_size)
        
        # 初始化隱藏狀態和單元狀態
        h_prev = np.zeros((hidden_size, 1))
        c_prev = np.zeros((hidden_size, 1))
        
        # 訓練模型
        lstm.train(inputs, targets, epochs=1000, learning_rate=0.01)
        
        # 預測
        predictions = lstm.predict(inputs, h_prev, c_prev)
        print(f"Number of predictions: {len(predictions)}")
    
    except Exception as e:
        print(f"Error in main: {e}")

if __name__ == '__main__':
    main()
```

## 練習題

1. **循環神經網絡**
   開發循環神經網絡：
   - 初始化參數
   - 前向傳播
   - 反向傳播
   - 優化性能

2. **長短期記憶網絡**
   創建長短期記憶網絡：
   - 初始化參數
   - 前向傳播
   - 反向傳播
   - 優化性能

3. **深度學習**
   實現深度學習：
   - 處理數據
   - 訓練模型
   - 優化性能
   - 處理異常

## 小提醒 💡

1. 循環神經網絡
   - 選擇合適架構
   - 優化參數
   - 處理異常
   - 提供監控

2. 長短期記憶網絡
   - 選擇合適架構
   - 優化性能
   - 處理異常
   - 提供結果

3. 深度學習
   - 選擇合適算法
   - 優化性能
   - 處理異常
   - 提供監控

4. 調試技巧
   - 使用開發工具
   - 分析性能
   - 優化關鍵路徑
   - 監控訓練狀態

[上一章：深度學習基礎](098_深度學習基礎.md) | [下一章：自然語言處理基礎](100_自然語言處理基礎.md) 