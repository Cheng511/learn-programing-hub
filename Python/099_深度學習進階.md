[ä¸Šä¸€ç« ï¼šæ·±åº¦å­¸ç¿’åŸºç¤](098_æ·±åº¦å­¸ç¿’åŸºç¤.md) | [ä¸‹ä¸€ç« ï¼šè‡ªç„¶èªè¨€è™•ç†åŸºç¤](100_è‡ªç„¶èªè¨€è™•ç†åŸºç¤.md)

# Python æ·±åº¦å­¸ç¿’é€²éš ğŸ§ 

## é«˜ç´šç¥ç¶“ç¶²çµ¡

### 1. å¾ªç’°ç¥ç¶“ç¶²çµ¡

```python
import numpy as np
from typing import List, Tuple, Optional
import time
import sys
import os

class RecurrentNeuralNetwork:
    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        """åˆå§‹åŒ–å¾ªç’°ç¥ç¶“ç¶²çµ¡"""
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.initialize_parameters()
    
    def initialize_parameters(self):
        """åˆå§‹åŒ–åƒæ•¸"""
        try:
            # åˆå§‹åŒ–æ¬Šé‡
            self.Wxh = np.random.randn(self.hidden_size, self.input_size) * 0.01
            self.Whh = np.random.randn(self.hidden_size, self.hidden_size) * 0.01
            self.Why = np.random.randn(self.output_size, self.hidden_size) * 0.01
            
            # åˆå§‹åŒ–åç½®
            self.bh = np.zeros((self.hidden_size, 1))
            self.by = np.zeros((self.output_size, 1))
            
            print("Parameters initialized")
            
        except Exception as e:
            print(f"Error initializing parameters: {e}")
    
    def tanh(self, x: np.ndarray) -> np.ndarray:
        """tanhæ¿€æ´»å‡½æ•¸"""
        return np.tanh(x)
    
    def tanh_derivative(self, x: np.ndarray) -> np.ndarray:
        """tanhå°æ•¸"""
        return 1 - np.tanh(x) ** 2
    
    def softmax(self, x: np.ndarray) -> np.ndarray:
        """softmaxæ¿€æ´»å‡½æ•¸"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)
    
    def forward_propagation(self, inputs: List[np.ndarray], h_prev: np.ndarray) -> Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray]]:
        """å‰å‘å‚³æ’­"""
        try:
            h_states = [h_prev]
            y_states = []
            cache = []
            
            for x in inputs:
                # è¨ˆç®—éš±è—ç‹€æ…‹
                h = self.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h_states[-1]) + self.bh)
                h_states.append(h)
                
                # è¨ˆç®—è¼¸å‡º
                y = self.softmax(np.dot(self.Why, h) + self.by)
                y_states.append(y)
                
                # ä¿å­˜ä¸­é–“çµæœ
                cache.append((x, h_states[-2], h, y))
            
            return h_states, y_states, cache
            
        except Exception as e:
            print(f"Error in forward propagation: {e}")
            return [], [], []
    
    def backward_propagation(self, y_states: List[np.ndarray], targets: List[np.ndarray], cache: List[Tuple], h_states: List[np.ndarray]):
        """åå‘å‚³æ’­"""
        try:
            # åˆå§‹åŒ–æ¢¯åº¦
            dWxh = np.zeros_like(self.Wxh)
            dWhh = np.zeros_like(self.Whh)
            dWhy = np.zeros_like(self.Why)
            dbh = np.zeros_like(self.bh)
            dby = np.zeros_like(self.by)
            
            # è¨ˆç®—è¼¸å‡ºå±¤æ¢¯åº¦
            dh_next = np.zeros((self.hidden_size, 1))
            
            for t in reversed(range(len(y_states))):
                # è¨ˆç®—è¼¸å‡ºå±¤èª¤å·®
                dy = y_states[t] - targets[t]
                dWhy += np.dot(dy, h_states[t + 1].T)
                dby += dy
                
                # è¨ˆç®—éš±è—å±¤èª¤å·®
                dh = np.dot(self.Why.T, dy) + dh_next
                dh_raw = self.tanh_derivative(cache[t][2]) * dh
                
                # æ›´æ–°æ¬Šé‡æ¢¯åº¦
                dWxh += np.dot(dh_raw, cache[t][0].T)
                dWhh += np.dot(dh_raw, cache[t][1].T)
                dbh += dh_raw
                
                # ä¿å­˜ä¸‹ä¸€æ™‚åˆ»çš„éš±è—å±¤æ¢¯åº¦
                dh_next = np.dot(self.Whh.T, dh_raw)
            
            # è£å‰ªæ¢¯åº¦
            for dparam in [dWxh, dWhh, dWhy, dbh, dby]:
                np.clip(dparam, -5, 5, out=dparam)
            
            return dWxh, dWhh, dWhy, dbh, dby
            
        except Exception as e:
            print(f"Error in backward propagation: {e}")
            return None, None, None, None, None
    
    def update_parameters(self, dWxh: np.ndarray, dWhh: np.ndarray, dWhy: np.ndarray, dbh: np.ndarray, dby: np.ndarray, learning_rate: float):
        """æ›´æ–°åƒæ•¸"""
        try:
            self.Wxh -= learning_rate * dWxh
            self.Whh -= learning_rate * dWhh
            self.Why -= learning_rate * dWhy
            self.bh -= learning_rate * dbh
            self.by -= learning_rate * dby
            
            print("Parameters updated")
            
        except Exception as e:
            print(f"Error updating parameters: {e}")
    
    def train(self, inputs: List[np.ndarray], targets: List[np.ndarray], epochs: int, learning_rate: float):
        """è¨“ç·´æ¨¡å‹"""
        try:
            for epoch in range(epochs):
                # åˆå§‹åŒ–éš±è—ç‹€æ…‹
                h_prev = np.zeros((self.hidden_size, 1))
                
                # å‰å‘å‚³æ’­
                h_states, y_states, cache = self.forward_propagation(inputs, h_prev)
                
                # åå‘å‚³æ’­
                dWxh, dWhh, dWhy, dbh, dby = self.backward_propagation(y_states, targets, cache, h_states)
                
                # æ›´æ–°åƒæ•¸
                self.update_parameters(dWxh, dWhh, dWhy, dbh, dby, learning_rate)
                
                if (epoch + 1) % 100 == 0:
                    loss = -np.sum([np.sum(target * np.log(y + 1e-15)) for target, y in zip(targets, y_states)])
                    print(f"Epoch {epoch + 1}, Loss: {loss:.4f}")
            
            print("Training completed")
            
        except Exception as e:
            print(f"Error in training: {e}")
    
    def predict(self, inputs: List[np.ndarray], h_prev: np.ndarray) -> List[np.ndarray]:
        """é æ¸¬"""
        try:
            h_states, y_states, _ = self.forward_propagation(inputs, h_prev)
            return y_states
            
        except Exception as e:
            print(f"Error in prediction: {e}")
            return []

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # å‰µå»ºç¤ºä¾‹æ•¸æ“š
    input_size = 2
    hidden_size = 4
    output_size = 2
    seq_length = 3
    
    # å‰µå»ºè¼¸å…¥åºåˆ—
    inputs = [np.random.randn(input_size, 1) for _ in range(seq_length)]
    targets = [np.random.randint(0, 2, (output_size, 1)) for _ in range(seq_length)]
    
    try:
        # å‰µå»ºRNN
        rnn = RecurrentNeuralNetwork(input_size, hidden_size, output_size)
        
        # åˆå§‹åŒ–éš±è—ç‹€æ…‹
        h_prev = np.zeros((hidden_size, 1))
        
        # è¨“ç·´æ¨¡å‹
        rnn.train(inputs, targets, epochs=1000, learning_rate=0.01)
        
        # é æ¸¬
        predictions = rnn.predict(inputs, h_prev)
        print(f"Number of predictions: {len(predictions)}")
    
    except Exception as e:
        print(f"Error in main: {e}")

if __name__ == '__main__':
    main()
```

### 2. é•·çŸ­æœŸè¨˜æ†¶ç¶²çµ¡

```python
import numpy as np
from typing import List, Tuple, Optional
import time
import sys
import os

class LongShortTermMemory:
    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        """åˆå§‹åŒ–é•·çŸ­æœŸè¨˜æ†¶ç¶²çµ¡"""
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.initialize_parameters()
    
    def initialize_parameters(self):
        """åˆå§‹åŒ–åƒæ•¸"""
        try:
            # åˆå§‹åŒ–æ¬Šé‡
            self.Wf = np.random.randn(self.hidden_size, self.input_size + self.hidden_size) * 0.01
            self.Wi = np.random.randn(self.hidden_size, self.input_size + self.hidden_size) * 0.01
            self.Wc = np.random.randn(self.hidden_size, self.input_size + self.hidden_size) * 0.01
            self.Wo = np.random.randn(self.hidden_size, self.input_size + self.hidden_size) * 0.01
            self.Wy = np.random.randn(self.output_size, self.hidden_size) * 0.01
            
            # åˆå§‹åŒ–åç½®
            self.bf = np.zeros((self.hidden_size, 1))
            self.bi = np.zeros((self.hidden_size, 1))
            self.bc = np.zeros((self.hidden_size, 1))
            self.bo = np.zeros((self.hidden_size, 1))
            self.by = np.zeros((self.output_size, 1))
            
            print("Parameters initialized")
            
        except Exception as e:
            print(f"Error initializing parameters: {e}")
    
    def sigmoid(self, x: np.ndarray) -> np.ndarray:
        """sigmoidæ¿€æ´»å‡½æ•¸"""
        return 1 / (1 + np.exp(-x))
    
    def sigmoid_derivative(self, x: np.ndarray) -> np.ndarray:
        """sigmoidå°æ•¸"""
        return x * (1 - x)
    
    def tanh(self, x: np.ndarray) -> np.ndarray:
        """tanhæ¿€æ´»å‡½æ•¸"""
        return np.tanh(x)
    
    def tanh_derivative(self, x: np.ndarray) -> np.ndarray:
        """tanhå°æ•¸"""
        return 1 - np.tanh(x) ** 2
    
    def softmax(self, x: np.ndarray) -> np.ndarray:
        """softmaxæ¿€æ´»å‡½æ•¸"""
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)
    
    def forward_propagation(self, inputs: List[np.ndarray], h_prev: np.ndarray, c_prev: np.ndarray) -> Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[Tuple]]:
        """å‰å‘å‚³æ’­"""
        try:
            h_states = [h_prev]
            c_states = [c_prev]
            y_states = []
            cache = []
            
            for x in inputs:
                # é€£æ¥è¼¸å…¥å’Œéš±è—ç‹€æ…‹
                concat = np.vstack((x, h_states[-1]))
                
                # éºå¿˜é–€
                ft = self.sigmoid(np.dot(self.Wf, concat) + self.bf)
                
                # è¼¸å…¥é–€
                it = self.sigmoid(np.dot(self.Wi, concat) + self.bi)
                
                # å–®å…ƒç‹€æ…‹å€™é¸å€¼
                c_tilde = self.tanh(np.dot(self.Wc, concat) + self.bc)
                
                # æ›´æ–°å–®å…ƒç‹€æ…‹
                c = ft * c_states[-1] + it * c_tilde
                c_states.append(c)
                
                # è¼¸å‡ºé–€
                ot = self.sigmoid(np.dot(self.Wo, concat) + self.bo)
                
                # æ›´æ–°éš±è—ç‹€æ…‹
                h = ot * self.tanh(c)
                h_states.append(h)
                
                # è¨ˆç®—è¼¸å‡º
                y = self.softmax(np.dot(self.Wy, h) + self.by)
                y_states.append(y)
                
                # ä¿å­˜ä¸­é–“çµæœ
                cache.append((x, h_states[-2], c_states[-2], ft, it, c_tilde, c, ot, h, y))
            
            return h_states, c_states, y_states, cache
            
        except Exception as e:
            print(f"Error in forward propagation: {e}")
            return [], [], [], []
    
    def backward_propagation(self, y_states: List[np.ndarray], targets: List[np.ndarray], cache: List[Tuple], h_states: List[np.ndarray], c_states: List[np.ndarray]):
        """åå‘å‚³æ’­"""
        try:
            # åˆå§‹åŒ–æ¢¯åº¦
            dWf = np.zeros_like(self.Wf)
            dWi = np.zeros_like(self.Wi)
            dWc = np.zeros_like(self.Wc)
            dWo = np.zeros_like(self.Wo)
            dWy = np.zeros_like(self.Wy)
            dbf = np.zeros_like(self.bf)
            dbi = np.zeros_like(self.bi)
            dbc = np.zeros_like(self.bc)
            dbo = np.zeros_like(self.bo)
            dby = np.zeros_like(self.by)
            
            # åˆå§‹åŒ–ä¸‹ä¸€æ™‚åˆ»çš„æ¢¯åº¦
            dh_next = np.zeros((self.hidden_size, 1))
            dc_next = np.zeros((self.hidden_size, 1))
            
            for t in reversed(range(len(y_states))):
                # è¨ˆç®—è¼¸å‡ºå±¤èª¤å·®
                dy = y_states[t] - targets[t]
                dWy += np.dot(dy, h_states[t + 1].T)
                dby += dy
                
                # è¨ˆç®—éš±è—å±¤èª¤å·®
                dh = np.dot(self.Wy.T, dy) + dh_next
                
                # è¨ˆç®—å–®å…ƒç‹€æ…‹èª¤å·®
                dc = dc_next + ot * self.tanh_derivative(self.tanh(c_states[t + 1])) * dh
                
                # è¨ˆç®—é–€æ§å–®å…ƒèª¤å·®
                dot = self.sigmoid_derivative(ot) * self.tanh(c_states[t + 1]) * dh
                dit = self.sigmoid_derivative(it) * c_tilde * dc
                dft = self.sigmoid_derivative(ft) * c_states[t] * dc
                dc_tilde = self.tanh_derivative(c_tilde) * it * dc
                
                # æ›´æ–°æ¬Šé‡æ¢¯åº¦
                dWf += np.dot(dft, concat.T)
                dWi += np.dot(dit, concat.T)
                dWc += np.dot(dc_tilde, concat.T)
                dWo += np.dot(dot, concat.T)
                
                # æ›´æ–°åç½®æ¢¯åº¦
                dbf += dft
                dbi += dit
                dbc += dc_tilde
                dbo += dot
                
                # ä¿å­˜ä¸‹ä¸€æ™‚åˆ»çš„æ¢¯åº¦
                dh_next = np.dot(self.Wf.T, dft) + np.dot(self.Wi.T, dit) + np.dot(self.Wc.T, dc_tilde) + np.dot(self.Wo.T, dot)
                dc_next = ft * dc
            
            # è£å‰ªæ¢¯åº¦
            for dparam in [dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby]:
                np.clip(dparam, -5, 5, out=dparam)
            
            return dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby
            
        except Exception as e:
            print(f"Error in backward propagation: {e}")
            return None, None, None, None, None, None, None, None, None, None
    
    def update_parameters(self, dWf: np.ndarray, dWi: np.ndarray, dWc: np.ndarray, dWo: np.ndarray, dWy: np.ndarray, dbf: np.ndarray, dbi: np.ndarray, dbc: np.ndarray, dbo: np.ndarray, dby: np.ndarray, learning_rate: float):
        """æ›´æ–°åƒæ•¸"""
        try:
            self.Wf -= learning_rate * dWf
            self.Wi -= learning_rate * dWi
            self.Wc -= learning_rate * dWc
            self.Wo -= learning_rate * dWo
            self.Wy -= learning_rate * dWy
            self.bf -= learning_rate * dbf
            self.bi -= learning_rate * dbi
            self.bc -= learning_rate * dbc
            self.bo -= learning_rate * dbo
            self.by -= learning_rate * dby
            
            print("Parameters updated")
            
        except Exception as e:
            print(f"Error updating parameters: {e}")
    
    def train(self, inputs: List[np.ndarray], targets: List[np.ndarray], epochs: int, learning_rate: float):
        """è¨“ç·´æ¨¡å‹"""
        try:
            for epoch in range(epochs):
                # åˆå§‹åŒ–éš±è—ç‹€æ…‹å’Œå–®å…ƒç‹€æ…‹
                h_prev = np.zeros((self.hidden_size, 1))
                c_prev = np.zeros((self.hidden_size, 1))
                
                # å‰å‘å‚³æ’­
                h_states, c_states, y_states, cache = self.forward_propagation(inputs, h_prev, c_prev)
                
                # åå‘å‚³æ’­
                dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby = self.backward_propagation(y_states, targets, cache, h_states, c_states)
                
                # æ›´æ–°åƒæ•¸
                self.update_parameters(dWf, dWi, dWc, dWo, dWy, dbf, dbi, dbc, dbo, dby, learning_rate)
                
                if (epoch + 1) % 100 == 0:
                    loss = -np.sum([np.sum(target * np.log(y + 1e-15)) for target, y in zip(targets, y_states)])
                    print(f"Epoch {epoch + 1}, Loss: {loss:.4f}")
            
            print("Training completed")
            
        except Exception as e:
            print(f"Error in training: {e}")
    
    def predict(self, inputs: List[np.ndarray], h_prev: np.ndarray, c_prev: np.ndarray) -> List[np.ndarray]:
        """é æ¸¬"""
        try:
            h_states, c_states, y_states, _ = self.forward_propagation(inputs, h_prev, c_prev)
            return y_states
            
        except Exception as e:
            print(f"Error in prediction: {e}")
            return []

# ä½¿ç”¨ç¤ºä¾‹
def main():
    # å‰µå»ºç¤ºä¾‹æ•¸æ“š
    input_size = 2
    hidden_size = 4
    output_size = 2
    seq_length = 3
    
    # å‰µå»ºè¼¸å…¥åºåˆ—
    inputs = [np.random.randn(input_size, 1) for _ in range(seq_length)]
    targets = [np.random.randint(0, 2, (output_size, 1)) for _ in range(seq_length)]
    
    try:
        # å‰µå»ºLSTM
        lstm = LongShortTermMemory(input_size, hidden_size, output_size)
        
        # åˆå§‹åŒ–éš±è—ç‹€æ…‹å’Œå–®å…ƒç‹€æ…‹
        h_prev = np.zeros((hidden_size, 1))
        c_prev = np.zeros((hidden_size, 1))
        
        # è¨“ç·´æ¨¡å‹
        lstm.train(inputs, targets, epochs=1000, learning_rate=0.01)
        
        # é æ¸¬
        predictions = lstm.predict(inputs, h_prev, c_prev)
        print(f"Number of predictions: {len(predictions)}")
    
    except Exception as e:
        print(f"Error in main: {e}")

if __name__ == '__main__':
    main()
```

## ç·´ç¿’é¡Œ

1. **å¾ªç’°ç¥ç¶“ç¶²çµ¡**
   é–‹ç™¼å¾ªç’°ç¥ç¶“ç¶²çµ¡ï¼š
   - åˆå§‹åŒ–åƒæ•¸
   - å‰å‘å‚³æ’­
   - åå‘å‚³æ’­
   - å„ªåŒ–æ€§èƒ½

2. **é•·çŸ­æœŸè¨˜æ†¶ç¶²çµ¡**
   å‰µå»ºé•·çŸ­æœŸè¨˜æ†¶ç¶²çµ¡ï¼š
   - åˆå§‹åŒ–åƒæ•¸
   - å‰å‘å‚³æ’­
   - åå‘å‚³æ’­
   - å„ªåŒ–æ€§èƒ½

3. **æ·±åº¦å­¸ç¿’**
   å¯¦ç¾æ·±åº¦å­¸ç¿’ï¼š
   - è™•ç†æ•¸æ“š
   - è¨“ç·´æ¨¡å‹
   - å„ªåŒ–æ€§èƒ½
   - è™•ç†ç•°å¸¸

## å°æé†’ ğŸ’¡

1. å¾ªç’°ç¥ç¶“ç¶²çµ¡
   - é¸æ“‡åˆé©æ¶æ§‹
   - å„ªåŒ–åƒæ•¸
   - è™•ç†ç•°å¸¸
   - æä¾›ç›£æ§

2. é•·çŸ­æœŸè¨˜æ†¶ç¶²çµ¡
   - é¸æ“‡åˆé©æ¶æ§‹
   - å„ªåŒ–æ€§èƒ½
   - è™•ç†ç•°å¸¸
   - æä¾›çµæœ

3. æ·±åº¦å­¸ç¿’
   - é¸æ“‡åˆé©ç®—æ³•
   - å„ªåŒ–æ€§èƒ½
   - è™•ç†ç•°å¸¸
   - æä¾›ç›£æ§

4. èª¿è©¦æŠ€å·§
   - ä½¿ç”¨é–‹ç™¼å·¥å…·
   - åˆ†ææ€§èƒ½
   - å„ªåŒ–é—œéµè·¯å¾‘
   - ç›£æ§è¨“ç·´ç‹€æ…‹

[ä¸Šä¸€ç« ï¼šæ·±åº¦å­¸ç¿’åŸºç¤](098_æ·±åº¦å­¸ç¿’åŸºç¤.md) | [ä¸‹ä¸€ç« ï¼šè‡ªç„¶èªè¨€è™•ç†åŸºç¤](100_è‡ªç„¶èªè¨€è™•ç†åŸºç¤.md) 