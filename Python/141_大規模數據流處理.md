[上一章：事件驅動架構](140_事件驅動架構.md) | [下一章：系統可觀測性](142_系統可觀測性.md)

# Python 大規模數據流處理 🌊

## 1. 流處理框架

### 1.1 Apache Flink 集成

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.common.typeinfo import Types
from pyflink.datastream.connectors import FlinkKafkaConsumer, FlinkKafkaProducer
from typing import Dict, List
import json

class FlinkProcessor:
    """Flink 處理器"""
    def __init__(self):
        self.env = StreamExecutionEnvironment.get_execution_environment()
    
    def create_kafka_source(self,
                          topic: str,
                          bootstrap_servers: str,
                          group_id: str):
        """創建 Kafka 數據源"""
        properties = {
            'bootstrap.servers': bootstrap_servers,
            'group.id': group_id,
            'auto.offset.reset': 'latest'
        }
        
        return FlinkKafkaConsumer(
            topics=topic,
            deserialization_schema=Types.STRING(),
            properties=properties
        )
    
    def create_kafka_sink(self,
                         topic: str,
                         bootstrap_servers: str):
        """創建 Kafka 輸出"""
        properties = {
            'bootstrap.servers': bootstrap_servers,
            'transaction.timeout.ms': '5000'
        }
        
        return FlinkKafkaProducer(
            topic=topic,
            serialization_schema=Types.STRING(),
            producer_config=properties
        )
    
    def process_stream(self,
                      input_topic: str,
                      output_topic: str,
                      bootstrap_servers: str,
                      group_id: str):
        """處理數據流"""
        # 創建數據源
        source = self.create_kafka_source(
            input_topic,
            bootstrap_servers,
            group_id
        )
        
        # 創建輸出
        sink = self.create_kafka_sink(
            output_topic,
            bootstrap_servers
        )
        
        # 數據處理
        self.env.add_source(source) \
            .map(lambda x: self._process_record(json.loads(x))) \
            .map(lambda x: json.dumps(x)) \
            .add_sink(sink)
        
        # 執行任務
        self.env.execute("Stream Processing Job")
    
    def _process_record(self, record: Dict) -> Dict:
        """處理單條記錄"""
        # 實現具體的處理邏輯
        return record
```

### 1.2 Apache Spark Streaming

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from typing import Dict, List

class SparkStreamProcessor:
    """Spark 流處理器"""
    def __init__(self,
                 app_name: str,
                 master: str = "local[*]"):
        self.spark = SparkSession.builder \
            .appName(app_name) \
            .master(master) \
            .getOrCreate()
    
    def create_kafka_stream(self,
                          topic: str,
                          bootstrap_servers: str):
        """創建 Kafka 流"""
        return self.spark.readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", bootstrap_servers) \
            .option("subscribe", topic) \
            .load()
    
    def process_stream(self,
                      input_topic: str,
                      output_topic: str,
                      bootstrap_servers: str,
                      schema: StructType):
        """處理數據流"""
        # 讀取數據流
        df = self.create_kafka_stream(
            input_topic,
            bootstrap_servers
        )
        
        # 解析 JSON 數據
        parsed_df = df.select(
            from_json(
                col("value").cast("string"),
                schema
            ).alias("data")
        ).select("data.*")
        
        # 處理數據
        processed_df = self._process_data(parsed_df)
        
        # 寫入結果
        query = processed_df \
            .selectExpr("to_json(struct(*)) as value") \
            .writeStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", bootstrap_servers) \
            .option("topic", output_topic) \
            .option("checkpointLocation", "/tmp/checkpoint") \
            .start()
        
        query.awaitTermination()
    
    def _process_data(self, df):
        """處理數據"""
        # 實現具體的處理邏輯
        return df
```

## 2. 實時分析

### 2.1 時間窗口處理

```python
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import pandas as pd

class WindowProcessor:
    """窗口處理器"""
    def __init__(self,
                 window_size: timedelta,
                 slide_size: Optional[timedelta] = None):
        self.window_size = window_size
        self.slide_size = slide_size or window_size
        self.buffer: List[Dict] = []
    
    def process(self,
                event: Dict) -> Optional[Dict]:
        """處理事件"""
        # 添加事件到緩衝區
        self.buffer.append(event)
        
        # 獲取當前時間
        current_time = datetime.utcnow()
        
        # 清理過期數據
        self.buffer = [
            e for e in self.buffer
            if current_time - datetime.fromisoformat(e['timestamp']) <= self.window_size
        ]
        
        # 檢查是否需要觸發計算
        if self._should_trigger(current_time):
            return self._compute_window()
        
        return None
    
    def _should_trigger(self,
                       current_time: datetime) -> bool:
        """檢查是否需要觸發計算"""
        if not self.buffer:
            return False
        
        oldest_time = datetime.fromisoformat(self.buffer[0]['timestamp'])
        return current_time - oldest_time >= self.slide_size
    
    def _compute_window(self) -> Dict:
        """計算窗口結果"""
        df = pd.DataFrame(self.buffer)
        
        return {
            'start_time': df['timestamp'].min(),
            'end_time': df['timestamp'].max(),
            'count': len(df),
            'metrics': self._compute_metrics(df)
        }
    
    def _compute_metrics(self, df: pd.DataFrame) -> Dict:
        """計算指標"""
        # 實現具體的指標計算邏輯
        return {}
```

### 2.2 實時聚合

```python
from collections import defaultdict
from typing import Dict, List, Tuple
import numpy as np

class StreamAggregator:
    """流式聚合器"""
    def __init__(self):
        self.aggregates = defaultdict(lambda: {
            'count': 0,
            'sum': 0,
            'min': float('inf'),
            'max': float('-inf'),
            'values': []
        })
    
    def update(self,
              key: str,
              value: float) -> Dict:
        """更新聚合值"""
        agg = self.aggregates[key]
        
        # 更新基本統計
        agg['count'] += 1
        agg['sum'] += value
        agg['min'] = min(agg['min'], value)
        agg['max'] = max(agg['max'], value)
        
        # 更新值列表（用於計算分位數）
        agg['values'].append(value)
        
        # 限制存儲的值數量
        if len(agg['values']) > 1000:
            agg['values'] = self._reservoir_sampling(agg['values'], 1000)
        
        return self.get_metrics(key)
    
    def get_metrics(self, key: str) -> Dict:
        """獲取聚合指標"""
        agg = self.aggregates[key]
        
        if agg['count'] == 0:
            return {}
        
        return {
            'count': agg['count'],
            'avg': agg['sum'] / agg['count'],
            'min': agg['min'],
            'max': agg['max'],
            'percentiles': self._calculate_percentiles(agg['values'])
        }
    
    def _reservoir_sampling(self,
                          values: List[float],
                          k: int) -> List[float]:
        """水塘抽樣"""
        result = values[:k]
        for i in range(k, len(values)):
            j = np.random.randint(0, i + 1)
            if j < k:
                result[j] = values[i]
        return result
    
    def _calculate_percentiles(self,
                             values: List[float]) -> Dict[str, float]:
        """計算分位數"""
        if not values:
            return {}
        
        return {
            'p50': np.percentile(values, 50),
            'p90': np.percentile(values, 90),
            'p95': np.percentile(values, 95),
            'p99': np.percentile(values, 99)
        }
```

## 3. 性能優化

### 3.1 並行處理

```python
import asyncio
from typing import Callable, Dict, List
from concurrent.futures import ProcessPoolExecutor

class ParallelProcessor:
    """並行處理器"""
    def __init__(self,
                 num_workers: int = None):
        self.num_workers = num_workers or asyncio.get_event_loop()._default_executor._max_workers
        self.executor = ProcessPoolExecutor(max_workers=self.num_workers)
    
    async def process_batch(self,
                          items: List[Dict],
                          processor: Callable) -> List[Dict]:
        """並行處理批次數據"""
        loop = asyncio.get_event_loop()
        
        # 將數據分片
        chunk_size = max(1, len(items) // self.num_workers)
        chunks = [
            items[i:i + chunk_size]
            for i in range(0, len(items), chunk_size)
        ]
        
        # 並行處理
        tasks = [
            loop.run_in_executor(self.executor, processor, chunk)
            for chunk in chunks
        ]
        
        # 等待所有任務完成
        results = await asyncio.gather(*tasks)
        
        # 合併結果
        return [
            item
            for chunk_result in results
            for item in chunk_result
        ]
    
    def shutdown(self):
        """關閉處理器"""
        self.executor.shutdown()
```

### 3.2 背壓控制

```python
from asyncio import Queue
from typing import Callable, Dict, Optional
import time

class BackpressureController:
    """背壓控制器"""
    def __init__(self,
                 max_queue_size: int,
                 processing_rate: float):
        self.queue = Queue(maxsize=max_queue_size)
        self.processing_rate = processing_rate
        self.last_process_time = time.time()
    
    async def submit(self,
                    item: Dict) -> bool:
        """提交項目"""
        try:
            await self.queue.put(item)
            return True
        except asyncio.QueueFull:
            return False
    
    async def process(self,
                     processor: Callable,
                     batch_size: int = 100) -> List[Dict]:
        """處理項目"""
        # 檢查處理間隔
        current_time = time.time()
        time_diff = current_time - self.last_process_time
        
        if time_diff < 1.0 / self.processing_rate:
            await asyncio.sleep(1.0 / self.processing_rate - time_diff)
        
        # 收集批次數據
        items = []
        for _ in range(min(batch_size, self.queue.qsize())):
            if not self.queue.empty():
                items.append(await self.queue.get())
        
        if not items:
            return []
        
        # 處理數據
        results = await processor(items)
        
        # 更新處理時間
        self.last_process_time = time.time()
        
        return results
```

## 練習題 🏃‍♂️

1. 實現流處理系統：
   - 數據接入
   - 流式處理
   - 窗口計算
   - 狀態管理
   - 容錯處理

2. 開發實時分析應用：
   - 時間序列分析
   - 異常檢測
   - 趨勢預測
   - 模式識別
   - 實時報表

3. 實現性能優化：
   - 並行處理
   - 數據分片
   - 內存管理
   - 背壓控制
   - 資源調優

4. 創建監控系統：
   - 吞吐量監控
   - 延遲監控
   - 資源使用
   - 錯誤追踪
   - 性能分析

5. 開發測試框架：
   - 單元測試
   - 性能測試
   - 容錯測試
   - 壓力測試
   - 集成測試

## 小結 📝

- 了解了流處理框架的使用方法
- 掌握了實時分析的核心技術
- 學會了時間窗口的處理方式
- 理解了流式聚合的實現原理
- 掌握了性能優化的關鍵策略

## 延伸閱讀 📚

1. 流處理系統設計
2. 實時分析最佳實踐
3. 分布式流處理
4. 時間序列數據處理
5. 大規模數據系統

[上一章：事件驅動架構](140_事件驅動架構.md) | [下一章：系統可觀測性](142_系統可觀測性.md) 