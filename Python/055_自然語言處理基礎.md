[上一章：深度學習入門](054_深度學習入門.md) | [下一章：網頁爬蟲基礎](056_網頁爬蟲基礎.md)

# Python 自然語言處理基礎 📝

## 自然語言處理概述

自然語言處理（NLP）是人工智能的一個重要分支，致力於讓計算機理解和處理人類語言。本章將介紹使用Python進行自然語言處理的基礎知識。

## 文本預處理

### 1. NLTK基礎操作

```python
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

# 下載必要的資源
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# 示例文本
text = """Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence 
concerned with the interactions between computers and human language, in particular how to program computers to 
process and analyze large amounts of natural language data."""

# 句子分割
sentences = sent_tokenize(text)
print('句子分割:', sentences)

# 詞語分割
words = word_tokenize(text)
print('詞語分割:', words)

# 停用詞過濾
stop_words = set(stopwords.words('english'))
filtered_words = [word for word in words if word.lower() not in stop_words]
print('過濾停用詞:', filtered_words)

# 詞幹提取
stemmer = PorterStemmer()
stemmed_words = [stemmer.stem(word) for word in filtered_words]
print('詞幹提取:', stemmed_words)

# 詞形還原
lemmatizer = WordNetLemmatizer()
lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]
print('詞形還原:', lemmatized_words)
```

### 2. spaCy基礎操作

```python
import spacy

# 加載英語模型
nlp = spacy.load('en_core_web_sm')

# 處理文本
doc = nlp(text)

# 詞性標註
pos_tags = [(token.text, token.pos_) for token in doc]
print('詞性標註:', pos_tags)

# 命名實體識別
entities = [(ent.text, ent.label_) for ent in doc.ents]
print('命名實體:', entities)

# 依存句法分析
dependencies = [(token.text, token.dep_, token.head.text) for token in doc]
print('依存關係:', dependencies)
```

## 文本特徵提取

### 1. 詞袋模型

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# 示例文本
corpus = [
    'This is the first document.',
    'This document is the second document.',
    'And this is the third document.',
    'Is this the first document?'
]

# 詞袋模型
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
print('詞彙表:', vectorizer.get_feature_names_out())
print('文檔-詞頻矩陣:\n', X.toarray())

# TF-IDF
tfidf = TfidfVectorizer()
X_tfidf = tfidf.fit_transform(corpus)
print('TF-IDF矩陣:\n', X_tfidf.toarray())
```

### 2. Word2Vec詞嵌入

```python
from gensim.models import Word2Vec
import numpy as np

# 準備訓練數據
sentences = [word_tokenize(sent.lower()) for sent in corpus]

# 訓練Word2Vec模型
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)

# 獲取詞向量
word_vectors = model.wv
print('詞向量示例:', word_vectors['document'])

# 查找相似詞
similar_words = word_vectors.most_similar('document')
print('與"document"相似的詞:', similar_words)
```

## 文本分類

### 1. 情感分析

```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# 示例數據
texts = [
    "I love this product, it's amazing!",
    "This is a terrible product, very disappointed.",
    "Great service and fast delivery!",
    "Poor quality and bad customer service.",
    "Excellent value for money!"
]
labels = [1, 0, 1, 0, 1]  # 1表示正面，0表示負面

# 特徵提取
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)

# 分割數據
X_train, X_test, y_train, y_test = train_test_split(
    X, labels, test_size=0.2, random_state=42
)

# 訓練模型
model = LogisticRegression()
model.fit(X_train, y_train)

# 預測和評估
y_pred = model.predict(X_test)
print('分類報告:\n', classification_report(y_test, y_pred))
```

### 2. 文本主題分類

```python
from sklearn.decomposition import LatentDirichletAllocation
import numpy as np

# 示例文本
documents = [
    "Machine learning is a subset of artificial intelligence",
    "Deep learning models require large amounts of data",
    "Natural language processing helps computers understand text",
    "Python is a popular programming language for AI",
    "Neural networks are inspired by biological brains"
]

# 特徵提取
vectorizer = CountVectorizer(max_features=1000, stop_words='english')
X = vectorizer.fit_transform(documents)

# 主題建模
n_topics = 2
lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
lda.fit(X)

# 顯示主題詞
def print_topics(model, feature_names, n_top_words):
    for topic_idx, topic in enumerate(model.components_):
        top_words = [feature_names[i] 
                    for i in topic.argsort()[:-n_top_words-1:-1]]
        print(f"Topic {topic_idx + 1}: {', '.join(top_words)}")

print_topics(lda, vectorizer.get_feature_names_out(), 5)
```

## 序列標註

### 1. 命名實體識別

```python
import spacy

# 加載模型
nlp = spacy.load('en_core_web_sm')

# 示例文本
text = """Apple Inc. is planning to open a new store in New York City. 
The company's CEO Tim Cook announced this during a press conference 
held on Monday."""

# 處理文本
doc = nlp(text)

# 識別命名實體
for ent in doc.ents:
    print(f'實體: {ent.text}, 類型: {ent.label_}')

# 可視化實體
from spacy import displacy
displacy.render(doc, style='ent', jupyter=True)
```

### 2. 詞性標註

```python
# 示例文本
text = "The quick brown fox jumps over the lazy dog."
doc = nlp(text)

# 詞性標註
for token in doc:
    print(f'詞語: {token.text}, 詞性: {token.pos_}, 詳細: {token.tag_}')

# 依存句法分析
for token in doc:
    print(f'{token.text} --> {token.dep_} --> {token.head.text}')
```

## 實戰項目：新聞文本分類器

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report

def load_news_data():
    # 示例數據
    news_data = {
        'text': [
            "New technology breakthrough in artificial intelligence",
            "Stock market reaches record high",
            "Scientists discover new species in Amazon",
            "Tech company releases latest smartphone",
            "Global climate change conference begins"
        ],
        'category': ['Technology', 'Business', 'Science', 
                    'Technology', 'Environment']
    }
    return pd.DataFrame(news_data)

def preprocess_text(text):
    # 文本預處理
    doc = nlp(text.lower())
    tokens = [token.text for token in doc 
             if not token.is_stop and not token.is_punct]
    return ' '.join(tokens)

def main():
    # 加載數據
    df = load_news_data()
    
    # 預處理
    df['processed_text'] = df['text'].apply(preprocess_text)
    
    # 特徵提取
    vectorizer = TfidfVectorizer(max_features=1000)
    X = vectorizer.fit_transform(df['processed_text'])
    y = df['category']
    
    # 分割數據
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # 訓練模型
    model = MultinomialNB()
    model.fit(X_train, y_train)
    
    # 評估
    y_pred = model.predict(X_test)
    print(classification_report(y_test, y_pred))
    
    # 預測新聞
    new_text = "New AI model achieves breakthrough in language understanding"
    processed_new = preprocess_text(new_text)
    X_new = vectorizer.transform([processed_new])
    prediction = model.predict(X_new)
    print(f'預測類別: {prediction[0]}')

if __name__ == '__main__':
    main()
```

## 練習題

1. **文本分類器**
   實現一個多類別文本分類器：
   - 數據收集和預處理
   - 特徵工程
   - 模型訓練和評估
   - 部署和應用

2. **情感分析系統**
   開發一個社交媒體情感分析系統：
   - 文本清理
   - 情感詞典建立
   - 情感分類
   - 結果可視化

3. **命名實體識別**
   實現一個自定義的NER系統：
   - 數據標註
   - 模型訓練
   - 實體提取
   - 性能評估

## 小提醒 💡

1. 文本預處理
   - 注意文本清理
   - 選擇合適的分詞方法
   - 處理特殊字符
   - 考慮語言特點

2. 特徵工程
   - 選擇合適的特徵表示
   - 處理稀疏特徵
   - 考慮詞序信息
   - 使用領域知識

3. 模型選擇
   - 根據任務選擇模型
   - 注意數據規模
   - 平衡效果和效率
   - 考慮可解釋性

4. 實踐建議
   - 建立評估基準
   - 進行錯誤分析
   - 持續優化模型
   - 注意實際應用

[上一章：深度學習入門](054_深度學習入門.md) | [下一章：網頁爬蟲基礎](056_網頁爬蟲基礎.md) 