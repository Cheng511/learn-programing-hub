[ä¸Šä¸€ç« ï¼šæ·±åº¦å­¸ç¿’å…¥é–€](054_æ·±åº¦å­¸ç¿’å…¥é–€.md) | [ä¸‹ä¸€ç« ï¼šç¶²é çˆ¬èŸ²åŸºç¤](056_ç¶²é çˆ¬èŸ²åŸºç¤.md)

# Python è‡ªç„¶èªè¨€è™•ç†åŸºç¤ ğŸ“

## è‡ªç„¶èªè¨€è™•ç†æ¦‚è¿°

è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€å€‹é‡è¦åˆ†æ”¯ï¼Œè‡´åŠ›æ–¼è®“è¨ˆç®—æ©Ÿç†è§£å’Œè™•ç†äººé¡èªè¨€ã€‚æœ¬ç« å°‡ä»‹ç´¹ä½¿ç”¨Pythoné€²è¡Œè‡ªç„¶èªè¨€è™•ç†çš„åŸºç¤çŸ¥è­˜ã€‚

## æ–‡æœ¬é è™•ç†

### 1. NLTKåŸºç¤æ“ä½œ

```python
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

# ä¸‹è¼‰å¿…è¦çš„è³‡æº
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# ç¤ºä¾‹æ–‡æœ¬
text = """Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence 
concerned with the interactions between computers and human language, in particular how to program computers to 
process and analyze large amounts of natural language data."""

# å¥å­åˆ†å‰²
sentences = sent_tokenize(text)
print('å¥å­åˆ†å‰²:', sentences)

# è©èªåˆ†å‰²
words = word_tokenize(text)
print('è©èªåˆ†å‰²:', words)

# åœç”¨è©éæ¿¾
stop_words = set(stopwords.words('english'))
filtered_words = [word for word in words if word.lower() not in stop_words]
print('éæ¿¾åœç”¨è©:', filtered_words)

# è©å¹¹æå–
stemmer = PorterStemmer()
stemmed_words = [stemmer.stem(word) for word in filtered_words]
print('è©å¹¹æå–:', stemmed_words)

# è©å½¢é‚„åŸ
lemmatizer = WordNetLemmatizer()
lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]
print('è©å½¢é‚„åŸ:', lemmatized_words)
```

### 2. spaCyåŸºç¤æ“ä½œ

```python
import spacy

# åŠ è¼‰è‹±èªæ¨¡å‹
nlp = spacy.load('en_core_web_sm')

# è™•ç†æ–‡æœ¬
doc = nlp(text)

# è©æ€§æ¨™è¨»
pos_tags = [(token.text, token.pos_) for token in doc]
print('è©æ€§æ¨™è¨»:', pos_tags)

# å‘½åå¯¦é«”è­˜åˆ¥
entities = [(ent.text, ent.label_) for ent in doc.ents]
print('å‘½åå¯¦é«”:', entities)

# ä¾å­˜å¥æ³•åˆ†æ
dependencies = [(token.text, token.dep_, token.head.text) for token in doc]
print('ä¾å­˜é—œä¿‚:', dependencies)
```

## æ–‡æœ¬ç‰¹å¾µæå–

### 1. è©è¢‹æ¨¡å‹

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# ç¤ºä¾‹æ–‡æœ¬
corpus = [
    'This is the first document.',
    'This document is the second document.',
    'And this is the third document.',
    'Is this the first document?'
]

# è©è¢‹æ¨¡å‹
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
print('è©å½™è¡¨:', vectorizer.get_feature_names_out())
print('æ–‡æª”-è©é »çŸ©é™£:\n', X.toarray())

# TF-IDF
tfidf = TfidfVectorizer()
X_tfidf = tfidf.fit_transform(corpus)
print('TF-IDFçŸ©é™£:\n', X_tfidf.toarray())
```

### 2. Word2Vecè©åµŒå…¥

```python
from gensim.models import Word2Vec
import numpy as np

# æº–å‚™è¨“ç·´æ•¸æ“š
sentences = [word_tokenize(sent.lower()) for sent in corpus]

# è¨“ç·´Word2Vecæ¨¡å‹
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)

# ç²å–è©å‘é‡
word_vectors = model.wv
print('è©å‘é‡ç¤ºä¾‹:', word_vectors['document'])

# æŸ¥æ‰¾ç›¸ä¼¼è©
similar_words = word_vectors.most_similar('document')
print('èˆ‡"document"ç›¸ä¼¼çš„è©:', similar_words)
```

## æ–‡æœ¬åˆ†é¡

### 1. æƒ…æ„Ÿåˆ†æ

```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# ç¤ºä¾‹æ•¸æ“š
texts = [
    "I love this product, it's amazing!",
    "This is a terrible product, very disappointed.",
    "Great service and fast delivery!",
    "Poor quality and bad customer service.",
    "Excellent value for money!"
]
labels = [1, 0, 1, 0, 1]  # 1è¡¨ç¤ºæ­£é¢ï¼Œ0è¡¨ç¤ºè² é¢

# ç‰¹å¾µæå–
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)

# åˆ†å‰²æ•¸æ“š
X_train, X_test, y_train, y_test = train_test_split(
    X, labels, test_size=0.2, random_state=42
)

# è¨“ç·´æ¨¡å‹
model = LogisticRegression()
model.fit(X_train, y_train)

# é æ¸¬å’Œè©•ä¼°
y_pred = model.predict(X_test)
print('åˆ†é¡å ±å‘Š:\n', classification_report(y_test, y_pred))
```

### 2. æ–‡æœ¬ä¸»é¡Œåˆ†é¡

```python
from sklearn.decomposition import LatentDirichletAllocation
import numpy as np

# ç¤ºä¾‹æ–‡æœ¬
documents = [
    "Machine learning is a subset of artificial intelligence",
    "Deep learning models require large amounts of data",
    "Natural language processing helps computers understand text",
    "Python is a popular programming language for AI",
    "Neural networks are inspired by biological brains"
]

# ç‰¹å¾µæå–
vectorizer = CountVectorizer(max_features=1000, stop_words='english')
X = vectorizer.fit_transform(documents)

# ä¸»é¡Œå»ºæ¨¡
n_topics = 2
lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
lda.fit(X)

# é¡¯ç¤ºä¸»é¡Œè©
def print_topics(model, feature_names, n_top_words):
    for topic_idx, topic in enumerate(model.components_):
        top_words = [feature_names[i] 
                    for i in topic.argsort()[:-n_top_words-1:-1]]
        print(f"Topic {topic_idx + 1}: {', '.join(top_words)}")

print_topics(lda, vectorizer.get_feature_names_out(), 5)
```

## åºåˆ—æ¨™è¨»

### 1. å‘½åå¯¦é«”è­˜åˆ¥

```python
import spacy

# åŠ è¼‰æ¨¡å‹
nlp = spacy.load('en_core_web_sm')

# ç¤ºä¾‹æ–‡æœ¬
text = """Apple Inc. is planning to open a new store in New York City. 
The company's CEO Tim Cook announced this during a press conference 
held on Monday."""

# è™•ç†æ–‡æœ¬
doc = nlp(text)

# è­˜åˆ¥å‘½åå¯¦é«”
for ent in doc.ents:
    print(f'å¯¦é«”: {ent.text}, é¡å‹: {ent.label_}')

# å¯è¦–åŒ–å¯¦é«”
from spacy import displacy
displacy.render(doc, style='ent', jupyter=True)
```

### 2. è©æ€§æ¨™è¨»

```python
# ç¤ºä¾‹æ–‡æœ¬
text = "The quick brown fox jumps over the lazy dog."
doc = nlp(text)

# è©æ€§æ¨™è¨»
for token in doc:
    print(f'è©èª: {token.text}, è©æ€§: {token.pos_}, è©³ç´°: {token.tag_}')

# ä¾å­˜å¥æ³•åˆ†æ
for token in doc:
    print(f'{token.text} --> {token.dep_} --> {token.head.text}')
```

## å¯¦æˆ°é …ç›®ï¼šæ–°èæ–‡æœ¬åˆ†é¡å™¨

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report

def load_news_data():
    # ç¤ºä¾‹æ•¸æ“š
    news_data = {
        'text': [
            "New technology breakthrough in artificial intelligence",
            "Stock market reaches record high",
            "Scientists discover new species in Amazon",
            "Tech company releases latest smartphone",
            "Global climate change conference begins"
        ],
        'category': ['Technology', 'Business', 'Science', 
                    'Technology', 'Environment']
    }
    return pd.DataFrame(news_data)

def preprocess_text(text):
    # æ–‡æœ¬é è™•ç†
    doc = nlp(text.lower())
    tokens = [token.text for token in doc 
             if not token.is_stop and not token.is_punct]
    return ' '.join(tokens)

def main():
    # åŠ è¼‰æ•¸æ“š
    df = load_news_data()
    
    # é è™•ç†
    df['processed_text'] = df['text'].apply(preprocess_text)
    
    # ç‰¹å¾µæå–
    vectorizer = TfidfVectorizer(max_features=1000)
    X = vectorizer.fit_transform(df['processed_text'])
    y = df['category']
    
    # åˆ†å‰²æ•¸æ“š
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # è¨“ç·´æ¨¡å‹
    model = MultinomialNB()
    model.fit(X_train, y_train)
    
    # è©•ä¼°
    y_pred = model.predict(X_test)
    print(classification_report(y_test, y_pred))
    
    # é æ¸¬æ–°è
    new_text = "New AI model achieves breakthrough in language understanding"
    processed_new = preprocess_text(new_text)
    X_new = vectorizer.transform([processed_new])
    prediction = model.predict(X_new)
    print(f'é æ¸¬é¡åˆ¥: {prediction[0]}')

if __name__ == '__main__':
    main()
```

## ç·´ç¿’é¡Œ

1. **æ–‡æœ¬åˆ†é¡å™¨**
   å¯¦ç¾ä¸€å€‹å¤šé¡åˆ¥æ–‡æœ¬åˆ†é¡å™¨ï¼š
   - æ•¸æ“šæ”¶é›†å’Œé è™•ç†
   - ç‰¹å¾µå·¥ç¨‹
   - æ¨¡å‹è¨“ç·´å’Œè©•ä¼°
   - éƒ¨ç½²å’Œæ‡‰ç”¨

2. **æƒ…æ„Ÿåˆ†æç³»çµ±**
   é–‹ç™¼ä¸€å€‹ç¤¾äº¤åª’é«”æƒ…æ„Ÿåˆ†æç³»çµ±ï¼š
   - æ–‡æœ¬æ¸…ç†
   - æƒ…æ„Ÿè©å…¸å»ºç«‹
   - æƒ…æ„Ÿåˆ†é¡
   - çµæœå¯è¦–åŒ–

3. **å‘½åå¯¦é«”è­˜åˆ¥**
   å¯¦ç¾ä¸€å€‹è‡ªå®šç¾©çš„NERç³»çµ±ï¼š
   - æ•¸æ“šæ¨™è¨»
   - æ¨¡å‹è¨“ç·´
   - å¯¦é«”æå–
   - æ€§èƒ½è©•ä¼°

## å°æé†’ ğŸ’¡

1. æ–‡æœ¬é è™•ç†
   - æ³¨æ„æ–‡æœ¬æ¸…ç†
   - é¸æ“‡åˆé©çš„åˆ†è©æ–¹æ³•
   - è™•ç†ç‰¹æ®Šå­—ç¬¦
   - è€ƒæ…®èªè¨€ç‰¹é»

2. ç‰¹å¾µå·¥ç¨‹
   - é¸æ“‡åˆé©çš„ç‰¹å¾µè¡¨ç¤º
   - è™•ç†ç¨€ç–ç‰¹å¾µ
   - è€ƒæ…®è©åºä¿¡æ¯
   - ä½¿ç”¨é ˜åŸŸçŸ¥è­˜

3. æ¨¡å‹é¸æ“‡
   - æ ¹æ“šä»»å‹™é¸æ“‡æ¨¡å‹
   - æ³¨æ„æ•¸æ“šè¦æ¨¡
   - å¹³è¡¡æ•ˆæœå’Œæ•ˆç‡
   - è€ƒæ…®å¯è§£é‡‹æ€§

4. å¯¦è¸å»ºè­°
   - å»ºç«‹è©•ä¼°åŸºæº–
   - é€²è¡ŒéŒ¯èª¤åˆ†æ
   - æŒçºŒå„ªåŒ–æ¨¡å‹
   - æ³¨æ„å¯¦éš›æ‡‰ç”¨

[ä¸Šä¸€ç« ï¼šæ·±åº¦å­¸ç¿’å…¥é–€](054_æ·±åº¦å­¸ç¿’å…¥é–€.md) | [ä¸‹ä¸€ç« ï¼šç¶²é çˆ¬èŸ²åŸºç¤](056_ç¶²é çˆ¬èŸ²åŸºç¤.md) 